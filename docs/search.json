[
  {
    "objectID": "src/01-RStudio_Intro.html",
    "href": "src/01-RStudio_Intro.html",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "",
    "text": "Download and install the necessary tools (R, RStudio)\nDevelop comfort in navigating the tools in RStudio\nDevelop comfort in writing and knitting a R Markdown (or a new Quarto) file\nIdentify the characteristics of tidy data\nUse R code: as a calculator and to explore tidy data",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/01-RStudio_Intro.html#learning-goals",
    "href": "src/01-RStudio_Intro.html#learning-goals",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "",
    "text": "Download and install the necessary tools (R, RStudio)\nDevelop comfort in navigating the tools in RStudio\nDevelop comfort in writing and knitting a R Markdown (or a new Quarto) file\nIdentify the characteristics of tidy data\nUse R code: as a calculator and to explore tidy data",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/01-RStudio_Intro.html#getting-started-in-rstudio",
    "href": "src/01-RStudio_Intro.html#getting-started-in-rstudio",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "Getting Started in RStudio",
    "text": "Getting Started in RStudio\nAs you might guess from the name, “Data Science” requires data. Working with modern (large, messy) data sets requires statistical software. We’ll exclusively use RStudio. Why?\n\nit’s free\n\nit’s open source (the code is free & anybody can contribute to it)\nit has a huge online community (which is helpful for when you get stuck)\n\nit’s one of the industry standards\n\nit can be used to create reproducible and lovely documents (In fact, the course materials that you’re currently reading were constructed entirely within RStudio!)\n\n\nDownload R & RStudio\nTo get started, take the following two steps in the given order. Even if you already have R/RStudio, make sure to update to the most recent versions.\n\nDownload and install the R statistical software at https://mirror.las.iastate.edu/CRAN/\n\n\n\nMac: Check to see if you have an Intel or Apple Silicon Processor Chip (Apple logo &gt; About this Mac). This will impact the version you download.\n\n\nDownload and install the FREE version of RStudio at https://posit.co/download/rstudio-desktop/\n\n\nMac: Once you download the dmg file and click on it, drag the RStudio icon to applications and then open Finder and click the eject icon next to the RStudio temporary drive under Locations.\n\nIf you are having issues with downloading, log on to https://rstudio.macalester.edu/ (use Mac credentials) to use the RStudio server.\nWhat’s the difference between R and RStudio? Mainly, RStudio requires R – thus it does everything R does and more. We will be using RStudio exclusively.\n\n\nA quick tour of RStudio\nOpen RStudio! You should see four panes, each serving a different purpose:\n\n\n\n\n\n\n\n\nFigure 1: RStudio Interface\n\n\n\n\n\nThis short video tour of RStudio summarizes some basic features of the console.\nUse RStudio as a simple calculator to do the following:\n  \n  1) Perform a simple calculation: calculate `90/3`.\n  2) RStudio has built-in *functions* to which we supply the necessary *arguments*:  `function(arguments)`.  Use the built-in function `sqrt` to calculate the square root of 25.\n  3) Use the built-in function `rep` to repeat the number \"5\" eight times. (Type `?rep` in the console and press Return. Check out the Help documentation for examples at the bottom.)\n  4) Use the `seq` function to create the vector `(0, 3, 6, 9, 12)`.  (Type `?seq` in the console and press Return.)\n  5) Create a new vector by concatenating three repetitions of the vector from the previous part. (Type `?c` in the console and press Return.)\nSolution\n\n90/3 \n## [1] 30\n\nsqrt(25)\n## [1] 5\n\nrep(5, times = 8)\n## [1] 5 5 5 5 5 5 5 5\n\nseq(0, 12, by = 3)\n## [1]  0  3  6  9 12\n\nrep(seq(0, 12, by = 3), times =  3)\n##  [1]  0  3  6  9 12  0  3  6  9 12  0  3  6  9 12\n\nrep(seq(0, 12, by = 3), each = 3) #notice the difference between the named input of times and each\n##  [1]  0  0  0  3  3  3  6  6  6  9  9  9 12 12 12\n\n```{name=“Assigning Values to Variables”, label=“assignment”} We often want to store our output for later use (why?). The basic idea in R:\n`name &lt;- output`\nCopy and paste the following code into the console, line by line. NOTE: RStudio ignores any content after the #. Thus we use this to make ‘comments’ and organize our code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#type square_3\nsquare_3\n    \n#calculate 3 squared\n3^2    \n    \n#store this as \"square_3\"\nsquare_3 &lt;- 3^2    \n    \n#type square_3 again!\nsquare_3\n    \n#do some math with square_3\nsquare_3 + 2\n:::\n\n\nData\nNot only does “Data Science” require statistical software, it requires DATA! Consider the Google definition:\n\n\n\n\n\nA datum.\n\n\n\n\nWith this definition in mind, which of the following are examples of data?\n\ntables\n\n\n\n  family father mother sex height nkids\n1      1   78.5   67.0   M   73.2     4\n2      1   78.5   67.0   F   69.2     4\n3      1   78.5   67.0   F   69.0     4\n4      1   78.5   67.0   F   69.0     4\n5      2   75.5   66.5   M   73.5     4\n6      2   75.5   66.5   M   72.5     4\n\n\n\nphoto\nvideo\ntext / tweets\n\nWe’ll mostly work with data that look like this:\n\n\n  family father mother sex height nkids\n1      1   78.5   67.0   M   73.2     4\n2      1   78.5   67.0   F   69.2     4\n3      1   78.5   67.0   F   69.0     4\n4      1   78.5   67.0   F   69.0     4\n5      2   75.5   66.5   M   73.5     4\n6      2   75.5   66.5   M   72.5     4\n\n\nThis isn’t as restrictive as it seems. We can convert the above signals: photos, videos, and text to a data table format!\n\n\nTidy Data\nExample: After a scandal among FIFA officials, fivethirtyeight.com posted an analysis of FIFA viewership, “How to Break FIFA”. Here’s a snapshot of the data used in this article:\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nconfederation\npopulation_share\ntv_audience_share\ngdp_weighted_share\n\n\n\n\nUnited States\nCONCACAF\n4.5\n4.3\n11.3\n\n\nJapan\nAFC\n1.9\n4.9\n9.1\n\n\nChina\nAFC\n19.5\n14.8\n7.3\n\n\nGermany\nUEFA\n1.2\n2.9\n6.3\n\n\nBrazil\nCONMEBOL\n2.8\n7.1\n5.4\n\n\nUnited Kingdom\nUEFA\n0.9\n2.1\n4.2\n\n\nItaly\nUEFA\n0.9\n2.1\n4.0\n\n\nFrance\nUEFA\n0.9\n2.0\n4.0\n\n\nRussia\nUEFA\n2.1\n3.1\n3.5\n\n\nSpain\nUEFA\n0.7\n1.8\n3.1\n\n\n\n\n\nThe data table above is in tidy format. Tidy data tables have three key features:\n\nEach row represents a unit of observation (also referred to as a case).\n\nEach column represents a variable (ie. an attribute of the cases that can vary from case to case). Each variable is one of two types:\n\n\n\nquantitative = numerical/numbers with units\n\ncategorical = discrete possibilities/categories\n\n\n\nEach entry contains a single data value; no analysis, summaries, footnotes, comments, etc., and only one value per cell\n\n\n\n\nTidy Data: Art by Allison Horst\n\n\nConsider the following in a group:   \n\nWhat are the units of observation in the FIFA data?\n\nWhat are the variables? Which are quantitative? Which are categorical?\n\nAre these tidy data?\n\nSolution\n\nA FIFA member country\ncountry name, soccer or football confederation, country’s share of global population (percentage), country’s share of global world cup TV Audience (percentage), country’s GDP-weighted audience share (percentage)\nYes\n\n\n\nCheck out the following data.  Explain to each other why they are untidy and how we can tidy them.    \n  \n  a. Data 1: FIFA    \n    \n              country  confederation  population share    tv_share\n        ------------- -------------- ----------------- ----------- ------------------\n        United States       CONCACAF     i don't know*       4.3%  *look up later      \n                 Japan           AFC               1.9       4.9%\n                 China           AFC              19.5      14.8%    \n                                                        total=24%           \n  \n  b. Data 2: Gapminder life expectancies by country    \n        \n                          country  1952  1957  1962\n        ------------ ------------ ----- ----- -----\n                Asia  Afghanistan  28.8  30.3  32.0\n                          Bahrain  50.9  53.8  56.9    \n              Africa      Algeria  43.0  45.7  48.3    \n\nSolution\n\nThere are notes such as “I don’t know” and “look up later” in columns with numeric values; the last row with the total is a summary. We could remove the text notes, replace it with the value if known, and remove the last row with the total summary.\nThe first column does not have a row name. It should be continent. Additionally, Bahrain needs a value for the continent.The column names ‘1952’, ‘1957’ and ‘1962’ are values not variables. The table should be ‘pivoted’ (more information on this coming soon!), so that there is an additional column named ‘year’ and each country has three observations (rows) associated with it (one for each year).\n\n\n\nData Basics in RStudio\nFor now, we’ll focus on tidy data. In a couple of weeks, you’ll learn how to “clean data” and turn untidy data into tidy data.\nThe first step to working with data in RStudio is getting it in there!  How we do this depends on its format (eg: Excel spreadsheet, csv file, txt file) and storage locations (eg: online, within Wiki, desktop).  \n\nLuckily for us, the `fifa_audience` data are stored in the `fivethirtyeight` RStudio package. Copy and paste the following code into the Console and press Enter.\n\n#download the data and information in the fivethirtyeight package (we only need to do this once)\ninstall.packages('fivethirtyeight')\n\n#load the fivethirtyeight package\nlibrary(fivethirtyeight)\n    \n#load the fifa data\ndata(\"fifa_audience\")\n    \n#store this under a shorter, easier name\nfifa &lt;- fifa_audience\n\nBefore we can analyze our data, we must understand its structure.  Try out the following functions (copy and paste into the Console).  For each, make a note that describes its action.  \n\n#(what does View do?)\nView(fifa)  \n\n#(what does head do?)\nhead(fifa)  \n\n#(what does dim do?)\ndim(fifa)           \n\n#(what does names do?)\nnames(fifa)         \n\nSolution\n\n#View() opens up a new tab with a spreadsheet preview of the data to visually explore the data. It is commented out in the Rmarkdown/Quarto file because this is an interactive feature\n#View(fifa)  \n\n#head() gives the first 6 (default number) rows of a data set\nhead(fifa)  \n## # A tibble: 6 × 5\n##   country    confederation population_share tv_audience_share gdp_weighted_share\n##   &lt;chr&gt;      &lt;chr&gt;                    &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;\n## 1 United St… CONCACAF                   4.5               4.3               11.3\n## 2 Japan      AFC                        1.9               4.9                9.1\n## 3 China      AFC                       19.5              14.8                7.3\n## 4 Germany    UEFA                       1.2               2.9                6.3\n## 5 Brazil     CONMEBOL                   2.8               7.1                5.4\n## 6 United Ki… UEFA                       0.9               2.1                4.2\n\n#dim() gives the number of rows and number of columns\ndim(fifa)           \n## [1] 191   5\n\n#names() gives the names of the columns/variables\nnames(fifa)   \n## [1] \"country\"            \"confederation\"      \"population_share\"  \n## [4] \"tv_audience_share\"  \"gdp_weighted_share\"\n\nData are also only useful if we know what they measure!  The `fifa` data table is *tidy*; it doesn't have any helpful notes in the data itself.\nRather, information about the data is stored in a separate codebook. Codebooks can be stored in many ways (eg: Google docs, word docs, etc). Here the authors have made their codebook available in RStudio (under the original fifa_audience name). Check it out (run the following code in the console):\n\n?fifa_audience\n\n\nWhat does population_share measure?\nWhat are the units of population_share?\n\nSolution\n\nCountry’s share of global population\nPercentage between 0 and 100\n\nConsider the following:\n\nWe might want to access and focus on a single variable. To this end, we can use the $ notation (see below). What are the values of tv_audience_share? Of confederation? Is it easy to figure out?\n\n\nfifa$tv_audience_share\nfifa$confederation\n\nSolution\n\nThe values of tv_audience_share are numerical values between 0 and 7.1. By scanning through the long list, it looks like the values of confederation are words (a string of alphabetical characters): OFC, CAF, AFC, UEFA, CONCACAF, CONMEBOL.\n\nIt’s important to understand the format/class of each variable (quantitative, categorical, date, etc) in both its meaning and its structure within RStudio:\n\nclass(fifa$tv_audience_share)\nclass(fifa$confederation)\n\n\nIf a variable is categorical (in factor format), we can determine its levels / category labels. What are the value of confederation?\n\n\nlevels(fifa$confederation) #it is in character format\nlevels(factor(fifa$confederation)) #we can convert to factor format\n\nSolution\n\nThe values of confederation are words, also known as strings of characters in computing languages (indicated by the quotes around them): “AFC”, “CAF”, “CONCACAF”, “CONMEBOL”, “OFC”, “UEFA”.",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/01-RStudio_Intro.html#r-markdownquarto-and-reproducible-research",
    "href": "src/01-RStudio_Intro.html#r-markdownquarto-and-reproducible-research",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "R Markdown/Quarto and Reproducible Research",
    "text": "R Markdown/Quarto and Reproducible Research\n\nReproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. - Reproducible Research, Coursera\n\nUseful Resources:\n\nR Markdown Quick Tour\n\nR Markdown Cheatsheet\nR Markdown Reference Guide\nQuarto Get Started Guide\n\nResearch often makes claims that are difficult to verify. A recent study of published psychology articles found that less than half of published claims could be reproduced. One of the most common reasons claims cannot be reproduced is confusion about data analysis. It may be unclear exactly how data was prepared and analyzed, or there may be a mistake in the analysis.\nIn this course we will use an innovative format called R Markdown that dramatically increases the transparency of data analysis. R Markdown interleaves data, R code, graphs, tables, and text, packaging them into an easily publishable format. Quarto is the update version of R Markdown that allows you to incorporate code from many programming languages and is more general than RStudio. You can choose to work in either format.\nTo use R Markdown, you will write an R Markdown formatted file in RStudio and then ask RStudio to knit it into an HTML document (or occasionally a PDF or MS Word document). If you work in a Quarto document, you press the render button to turn it into an HTML document.\nLook at this [Sample RMarkdown](http://www.statpower.net/Content/310/R%20Stuff/SampleMarkdown.Rmd) and the [HTML webpage](http://www.statpower.net/Content/310/R%20Stuff/SampleMarkdown.html) it creates. Consider the following and discuss:\n    \na) How are bullets, italics, and section headers represented in the R Markdown file?\nb) How does R code appear in the R Markdown file?\nc) In the HTML webpage, do you see the R code, the output of the R code, or both?\nSolution\nBullets are represented with * and +\nItalics are represented with * before and after a word or phrase\nSection headers are represented with #\n\nR code chunks are between 3 tick marks at the beginning and end; it is R code if there is an r in curly braces\n  \nIf echo=FALSE in curly braces, the code is not shown. Otherwise, both code and output are shown by default.\nNow take a look at the R Markdown cheatsheet. Look up the R Markdown features from the previous question on the cheatsheet. There’s a great deal more information there.",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/01-RStudio_Intro.html#practice",
    "href": "src/01-RStudio_Intro.html#practice",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "Practice",
    "text": "Practice\nComplete the following. If you get stuck along the way, refer to the R Markdown cheatsheet linked above, search the web for answers, and/or ask for help!\nCreate a new R Markdown (or a Quarto document) about your favorite food.    \n\na. Create a new file in RStudio (File -&gt; New File -&gt; R Markdown or Quarto Document) with a Title of `First_Markdown` [unclick the visual editor button]. Save it to a new folder on your Desktop called `COMP_STAT_112`; within that new folder, create another new folder called `Assignment_01`.  \nb. Make sure you can compile/render (Knit/Render) the Markdown/Quarto into a webpage (html file).  \nc. Add a new line between `title` and `output` that reads: `author: Your Name`.\nd. Delete everything from `## RMarkdown` or `## Quarto` and below. Create a new section by typing `## Favorite Food`.\ne. Write a very brief essay about your favorite food. Make sure to include:    \n  * A picture from the web    \n  * A bullet list    \n  * A numbered list  \nf. Compile (Knit/Render) the document into an html file [which appears in the folder `Assignment_01` you created] and make sure it looks like you want it to.\nThere's a data set named `comic_characters` in the `fivethirtyeightdata` package.\nInstall the package by running the following in the console:\ninstall.packages('fivethirtyeightdata', repos = 'https://fivethirtyeightdata.github.io/drat/', type = 'source')\nCheck out the codebook (hint: use ?) to understand what these data measure.\nAdd a second section to your RMarkdown file that you’ve created (with ##), and then use code chunks and R commands to perform/answer the following tasks/questions:\n\nLoad the comic_characters data.\n\nWhat are the units of observation? How many observations are there?\n\nIn a new code chunk, print out the first 12 rows of the data set.\nGet a list of all variable names.\n\nWhat’s the class of the date variable?\n\nList all of the unique entries in the gsm variable (no need to include NA).\nCompile the document into an html file.",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/01-RStudio_Intro.html#appendix-r-functions",
    "href": "src/01-RStudio_Intro.html#appendix-r-functions",
    "title": "P1: Intro to R, RStudio, and R Markdown",
    "section": "Appendix: R Functions",
    "text": "Appendix: R Functions\n\nR as a calculator\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\n/\nDivision\n90/30\n\n\n*\nMultiplication\n2*5\n\n\n+\nAddition\n1+1\n\n\n-\nSubtraction\n1-1\n\n\n^\nExponent/Power to\n3^2\n\n\nsqrt(x)\nSquare root\nsqrt(25)\n\n\n\n\n\nR Basics\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\ninstall.packages('packagename')\nDownload a R package (function, data, etc.) from repository\ninstall.packages('fivethirtyeight')\n\n\nlibrary(packagename)\nAccess a downloaded R package\nlibrary(fivethirtyeight)\n\n\n?function_object_name\nOpens the help/documentation for the function or object\n?seq\n\n\nrep(x, times, each)\nRepeat x a # times\nrep(5,8)\n\n\nseq(from, to, by)\nSequence generation\nseq(0, 12, by = 2)\n\n\nname &lt;- value_output\nAssign value or output to a name\nsquared_3 &lt;- 3^2\n\n\nView(x)\nOpen spreadsheet viewer of dataset\nView(fifa_audience)\n\n\nhead(x)\nPrint the first 6 rows of a dataset\nhead(fifa_audience)\n\n\ndim(x)\nPrint the dimensions (number of rows and columns) of a dataset\ndim(fifa_audience)\n\n\nnames(x)\nPrint the names of the variables in a dataset\nnames(fifa_audience)\n\n\n$\nUsed to access one variable in a data set based on its name\nfifa_audience$confederation\n\n\nclass(x)\nPrint the class types argument or input\nclass(fifa_audience$confederation)\n\n\nfactor(x)\nConverts the argument or input to a factor class type (categorical variable)\nfactor(fifa_audience$confederation)\n\n\nlevels(x)\nPrints the unique categories of a factor\nlevels(factor(fifa_audience$confederation))",
    "crumbs": [
      "Src",
      "P1: Intro to R, RStudio, and R Markdown"
    ]
  },
  {
    "objectID": "src/21-Prez_Tools.html",
    "href": "src/21-Prez_Tools.html",
    "title": "Presentation Tools",
    "section": "",
    "text": "R Markdown can be used to output a wide range of formats. We have already seen a bunch of these including documents (html, pdf, word, handout), html files with built-in interactive widgets, Shiny apps, and dashboards. There is a nice gallery to showcase all of these formats, as well as a list.\nNote: The newest presentation tool available (as of 2022) is Quarto, which is a generalization of R Markdown format. It allows you to weave R, Python, Julia, and other code and output with marked up text. This is incredibly important as most organizations use a combination of computational tools. While we will still refer to R Markdown below, many of the same formats listed below are available in Quarto, which is probably the soon-to-be future standard. See https://quarto.org/ for more information.\n\n\nThere are many different ways to write slide packages in RStudio. Advantages of all of these methods are that (i) the output is an html or pdf, so it is easy to put on a website and access from anywhere, and (ii) you have already done all of your wrangling, graphics, etc. in R. So you do not need to cut and paste graphics or code clippings or anything else from RStudio into another software package like PowerPoint or Google Docs. The main disadvantage is that there is not a drag and drop way to arrange slides for these types of presentations. So if you want an image to be of a certain size at a certain location of the slide, you’ll have to input those specifications, as opposed to stretching and moving with a cursor.\nIf you are interested in trying to write your presentation in R Markdown (or Quarto), my suggestion is to pick one of the formats below, download a template (I’ve linked to a bunch), make sure you can compile it from R Markdown, and then start editing.\n\n\n\nioslides_presentation, an HTML presentation. Just need to include output: ioslides_presentation in the header of your Rmd file and then an option will appear to Knit to HTML (ioslides). Here is the documentation, an example, and the Rmd for the example. These files are from one of Garret Grolemund’s repositories here.\nslidy_presentation, an HTML presentation. Same thing as above. Just include output: slidy_presentation and an option will appear to Knit to HTML (Slidy). Here is the documentation.\nbeamer_presentation, a PDF presentation with LaTeX Beamer. Same as above again. Just include output: beamer_presentation and then you can Knit to PDF (Beamer).\n\n\n\n\n\nrevealjs::revealjs_presentation, an HTML presentation that requires the revealjs package. Here is the documentation, an example, and the Rmd for the example. These files are from one of Garret Grolemund’s repositories here.\nrmdshower, an HTML presentation that requires the rmdshower package. Here is an example and the code to generate it can be found in this repository.\nSlidify is a wrapper package that enables you to use many different types of presentations. Here is a review and a pretty good tutorial, the code for which you can find here.\n\nNote: Within RStudio, you can also create a Quarto Presentation, which is very similar to Rmd revealjs slides. Brianna has been using Quarto Presentations this semester and finds that it works a bit easier than Rmd revealjs.\n\n\n\n\nYou have all used the standard output:html_document to knit your Rmd file into an html file. For your technical blog posts, you can stick to that, or try tweaking some options. Here are some examples:\n\nYou can change the appearance and style of the output of the html_document by changing the theme and highlight options. I’ve changed these in one knitting option above with the lumen theme and espresso highlight. You can find the full list of options here.\nYou can load the prettydoc package, and use one of its themes. You can find the full list in the documentation or this tutorial.\nSome alternative Markdown templates\nThe bookdown package was designed to write books in R Markdown, but you can also use it write single html files or pdf handouts. I’ve been using it all semester. Here is the documentation, a tutorial video, and a bunch of books that were written with the package. Check out the list of contributors.\n\nHere a couple others you probably will not need for this project:\n\nThe blogdown package helps you set up an actual blogs with multiple posts. Here is a tutorial (written in bookdown ofcourse) and an example blog.\nThe rticles package helps you use R Markdown to write an article that conforms to a specific journal’s template.\n\n\n\nBelow is some sample content to test out some of these output options. I’ve set everything up in the header of this Rmd file. All you have to do is knit it with different options and check out the output html file.\n\n\n\n\n\n\nTrips &lt;- readRDS(\"data/2014-Q4-Trips-History-Data.rds\")\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\nStations &lt;- read.csv(\"data/DC-Stations.csv\")\n\nWarning in file(file, \"rt\"): cannot open file 'data/DC-Stations.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'singleBike' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'singleTidy' not found\n\n\nError in get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\"): Stamen map tiles are now hosted by Stadia Maps, use `get_stadiamap()`.\n\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'num_daily_departures' not found\n\n\nError: object 'NetTraffic' not found\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectWide' not found\n\n\n\n\n\n\n\nWarning in dygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four\nSelect Stations\"): restarting interrupted promise evaluation\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectXTS' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\n\nError in eval(expr, envir, enclos): object 'p' not found",
    "crumbs": [
      "Src",
      "Presentation Tools"
    ]
  },
  {
    "objectID": "src/21-Prez_Tools.html#output-formats",
    "href": "src/21-Prez_Tools.html#output-formats",
    "title": "Presentation Tools",
    "section": "",
    "text": "R Markdown can be used to output a wide range of formats. We have already seen a bunch of these including documents (html, pdf, word, handout), html files with built-in interactive widgets, Shiny apps, and dashboards. There is a nice gallery to showcase all of these formats, as well as a list.\nNote: The newest presentation tool available (as of 2022) is Quarto, which is a generalization of R Markdown format. It allows you to weave R, Python, Julia, and other code and output with marked up text. This is incredibly important as most organizations use a combination of computational tools. While we will still refer to R Markdown below, many of the same formats listed below are available in Quarto, which is probably the soon-to-be future standard. See https://quarto.org/ for more information.\n\n\nThere are many different ways to write slide packages in RStudio. Advantages of all of these methods are that (i) the output is an html or pdf, so it is easy to put on a website and access from anywhere, and (ii) you have already done all of your wrangling, graphics, etc. in R. So you do not need to cut and paste graphics or code clippings or anything else from RStudio into another software package like PowerPoint or Google Docs. The main disadvantage is that there is not a drag and drop way to arrange slides for these types of presentations. So if you want an image to be of a certain size at a certain location of the slide, you’ll have to input those specifications, as opposed to stretching and moving with a cursor.\nIf you are interested in trying to write your presentation in R Markdown (or Quarto), my suggestion is to pick one of the formats below, download a template (I’ve linked to a bunch), make sure you can compile it from R Markdown, and then start editing.\n\n\n\nioslides_presentation, an HTML presentation. Just need to include output: ioslides_presentation in the header of your Rmd file and then an option will appear to Knit to HTML (ioslides). Here is the documentation, an example, and the Rmd for the example. These files are from one of Garret Grolemund’s repositories here.\nslidy_presentation, an HTML presentation. Same thing as above. Just include output: slidy_presentation and an option will appear to Knit to HTML (Slidy). Here is the documentation.\nbeamer_presentation, a PDF presentation with LaTeX Beamer. Same as above again. Just include output: beamer_presentation and then you can Knit to PDF (Beamer).\n\n\n\n\n\nrevealjs::revealjs_presentation, an HTML presentation that requires the revealjs package. Here is the documentation, an example, and the Rmd for the example. These files are from one of Garret Grolemund’s repositories here.\nrmdshower, an HTML presentation that requires the rmdshower package. Here is an example and the code to generate it can be found in this repository.\nSlidify is a wrapper package that enables you to use many different types of presentations. Here is a review and a pretty good tutorial, the code for which you can find here.\n\nNote: Within RStudio, you can also create a Quarto Presentation, which is very similar to Rmd revealjs slides. Brianna has been using Quarto Presentations this semester and finds that it works a bit easier than Rmd revealjs.\n\n\n\n\nYou have all used the standard output:html_document to knit your Rmd file into an html file. For your technical blog posts, you can stick to that, or try tweaking some options. Here are some examples:\n\nYou can change the appearance and style of the output of the html_document by changing the theme and highlight options. I’ve changed these in one knitting option above with the lumen theme and espresso highlight. You can find the full list of options here.\nYou can load the prettydoc package, and use one of its themes. You can find the full list in the documentation or this tutorial.\nSome alternative Markdown templates\nThe bookdown package was designed to write books in R Markdown, but you can also use it write single html files or pdf handouts. I’ve been using it all semester. Here is the documentation, a tutorial video, and a bunch of books that were written with the package. Check out the list of contributors.\n\nHere a couple others you probably will not need for this project:\n\nThe blogdown package helps you set up an actual blogs with multiple posts. Here is a tutorial (written in bookdown ofcourse) and an example blog.\nThe rticles package helps you use R Markdown to write an article that conforms to a specific journal’s template.\n\n\n\nBelow is some sample content to test out some of these output options. I’ve set everything up in the header of this Rmd file. All you have to do is knit it with different options and check out the output html file.\n\n\n\n\n\n\nTrips &lt;- readRDS(\"data/2014-Q4-Trips-History-Data.rds\")\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\nStations &lt;- read.csv(\"data/DC-Stations.csv\")\n\nWarning in file(file, \"rt\"): cannot open file 'data/DC-Stations.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'singleBike' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'singleTidy' not found\n\n\nError in get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\"): Stamen map tiles are now hosted by Stadia Maps, use `get_stadiamap()`.\n\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nError in eval(expr, envir, enclos): object 'num_daily_departures' not found\n\n\nError: object 'NetTraffic' not found\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectWide' not found\n\n\n\n\n\n\n\nWarning in dygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four\nSelect Stations\"): restarting interrupted promise evaluation\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectXTS' not found\n\n\n\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\n\nError in eval(expr, envir, enclos): object 'p' not found",
    "crumbs": [
      "Src",
      "Presentation Tools"
    ]
  },
  {
    "objectID": "src/21-Prez_Tools.html#footnotes",
    "href": "src/21-Prez_Tools.html#footnotes",
    "title": "Presentation Tools",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a footnote.↩︎",
    "crumbs": [
      "Src",
      "Presentation Tools"
    ]
  },
  {
    "objectID": "src/18-Text_Analysis.html",
    "href": "src/18-Text_Analysis.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Understand the analysis process of decomposing text into tokens and considering word/token frequency\nDevelop comfort in comparing the text across multiple documents in terms of tf-idf and log odds ratio\nDevelop comfort in using lexicons to perform sentiment analysis on a document of text\n\n\n\n\nWe have seen how to manipulate strings with regular expressions. Here, we examine how to analyze longer text documents. Text refers to information composed primarily of words: song lyrics, Tweets, news articles, novels, Wikipedia articles, online forums, and countless other resources.\nIn R and most other programming languages, text is stored in strings of characters.\nThere are a variety of common ways to get strings containing the text we want to analyze.\n\n\nString Literals\nIt may be natural to start by declaring an R variable that holds a string. Let’s consider the U.S. Declaration of Independence. Here’s an R variable that contains one of the most memorable sentences in the Declaration of Independence:\n\nus_dec_sentence &lt;- \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n# Show the number of characters in the sentence.\nnchar(us_dec_sentence)\n\n[1] 209\n\n# Show the sentence itself.\nus_dec_sentence\n\n[1] \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n\nUnfortunately, creating literal string variables like this can become unwieldy for larger texts, or collections of multiple texts.\nUsing this technique, your R program would be narrowly written to analyze hard-coded string variables, and defining those string variables may take up the vast majority of our program’s source code, making it difficult to read.\nWe will discuss two more flexible ways of getting textual data: reading a .TXT file and accessing a web API.\n\n\nThe simplest file format for text is a .TXT (or .txt) file. A .txt file contains raw textual data. You can find .txt files by using Google’s filetype: search filter. Go to http://google.com and type filetype:txt declaration of independence in the search box.\nIn the results you should see many .txt files containing the U.S. Declaration of Independence.\nFor example, https://infamous.net/documents/declaration-of-independence.txt. We can read this .txt file into R as a string using the readr package.1\nBecause the text is so large, we use the strtrim function to only show the first 500 characters of the text.\n\nlibrary(readr)\nus_dec &lt;- read_file(\"https://infamous.net/documents/declaration-of-independence.txt\")\nnchar(us_dec)\n\n[1] 9841\n\nstrtrim(us_dec, 500)\n\n[1] \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Congress, July 4, 1776,\\nTHE UNANIMOUS DECLARATION OF THE THIRTEEN UNITED STATES OF AMERICA\\n\\nWhen in the Course of human events, it becomes necessary for one \\npeople to dissolve the political bands which have connected them \\nwith another, and to assume among the Powers of the earth, the \\nseparate and equal station to which the Laws of Nature and of \\nNature's God entitle them, a decent respect to the opinions of \\nmankind requires that they should declare the causes which\"\n\n\nNotice all those \\n sequences that appear in the string.\nThese are newline characters that denote the end of a line.\nThere are a few other special characters that you may see. For example, '\\t' is a tab.\n\n\n\nWhen we want to analyze textual data created on websites or mobile apps such as Facebook and Twitter, we can use web APIs to gather the text. Here is one example from the largest single collection of written knowledge in human history: Wikipedia!\nThe function below retrieves the text content of a Wikipedia article with a particular title. It uses Wikipedia’s Public API to do so, which enables any computer program to interact with Wikipedia. Wikipedia’s API is convenient for us because it is vast, open, and free. Don’t worry if you don’t follow the details of the code below.\n\nGetArticleText &lt;- function(langCode, titles) {\n  # Returns the text of the specified article in the specified language\n\n  # An accumulator variable that will hold the text of each article\n\n  # Create\n  texts &lt;- sapply(titles, function(t) {\n    print(t)\n    article_info &lt;- getForm(\n      paste(\"https://\", langCode, \".wikipedia.org/w/api.php\", sep = \"\"),\n      action  = \"query\",\n      prop = \"extracts\",\n      format  = \"json\",\n      explaintext = \"\",\n      titles  = t\n    )\n\n    js &lt;- fromJSON(article_info)\n    return(js$query$pages[[1]]$extract)\n  })\n  return(texts)\n}\n\n# Get the text for https://en.wikipedia.org/wiki/Macalester_College,\n# https://en.wikipedia.org/wiki/Carleton_College, and https://en.wikipedia.org/wiki/University_of_Minnesota in English (\"en\").\n# We could also get the text for the Spanish article (\"es\"), or German article (\"de\")\n\nschool_wiki_titles &lt;- c(\"Macalester College\", \"Carleton College\", \"University of Minnesota\")\nschool_wiki_text &lt;- GetArticleText(\"en\", school_wiki_titles)\n\n[1] \"Macalester College\"\n\n\nError in function (type, msg, asError = TRUE) : schannel: CertGetCertificateChain trust error CERT_TRUST_IS_NOT_TIME_VALID\n\n# Print out the first 500 characters of the text\nstrtrim(school_wiki_text, 500)\n\nError in eval(expr, envir, enclos): object 'school_wiki_text' not found\n\n\nWe’ll analyze these documents further below.\n\n\n\n\nIf we tried to make a data frame directly out of the text, it would look odd. It contains the text as a single row in a column named “text.” This doesn’t seem any more useful than the original string itself.\n\nus_dec_df &lt;- tibble(title = \"Declaration of Independence\", text = us_dec)\nus_dec_df\n\n# A tibble: 1 × 2\n  title                       text                                              \n  &lt;chr&gt;                       &lt;chr&gt;                                             \n1 Declaration of Independence \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Con…\n\n\n\n\nWe need to restructure the text into components that can be easily analyzed.\nWe will use two units of data:\n\nA token is the smallest textual information unit we wish to measure, typically a word.\nA document is a collection of tokens.\n\nFor our example here, the Declaration of Independence is the document, and a word is the token. However, a document could be a tweet, a novel chapter, a Wikipedia article, or anything else that seems interesting. Other possibilities for tokens include sentences, lines, paragraphs, characters, ngrams, and more.2\nLater on, we will also give an example of how to perform textual analyses comparing two or more documents.\nWe will be using the tidy text format, which has one row for each unit of analysis. Our work will focus on word-level analysis within each document, so each row will contain a document and word.\nTidyText’s unnest_tokens function takes a data frame containing one row per document and breaks it into a data frame containing one row per token.\n\ntidy_us_dec &lt;- us_dec_df %&gt;%\n  unnest_tokens(word, text)\n\ntidy_us_dec\n\n# A tibble: 1,539 × 2\n   title                       word        \n   &lt;chr&gt;                       &lt;chr&gt;       \n 1 Declaration of Independence the         \n 2 Declaration of Independence declaration \n 3 Declaration of Independence of          \n 4 Declaration of Independence independence\n 5 Declaration of Independence in          \n 6 Declaration of Independence congress    \n 7 Declaration of Independence july        \n 8 Declaration of Independence 4           \n 9 Declaration of Independence 1776        \n10 Declaration of Independence the         \n# ℹ 1,529 more rows\n\n\nNote that because we only have one document, the initial data frame (us_dec_df) is just one row and the tidy text data frame (tidy_us_dec) has the same title for each row.\nLater on we will analyze more than one document and these columns can change.\nWe can now analyze this tidy text data frame. For example, we can determine the total number of words.\n\nnrow(tidy_us_dec)\n\n[1] 1539\n\n\nWe can also find the most frequently used words by using dplyr’s count function, which creates a frequency table for (in our case) words:\n\n# Create and display frequency count table\nall_us_dec_counts &lt;- tidy_us_dec %&gt;%\n  count(word, sort = TRUE)\nall_us_dec_counts\n\n# A tibble: 674 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the      84\n 2 of       83\n 3 to       67\n 4 and      58\n 5 for      28\n 6 our      26\n 7 has      20\n 8 in       20\n 9 their    20\n10 he       19\n# ℹ 664 more rows\n\n\nWe can count the rows in this data frame to determine how many different unique words appear in the document.\n\nnrow(all_us_dec_counts)\n\n[1] 674\n\n\n\n\n\nNotice that the most frequent words are common words that are present in any document and not particularly descriptive of the topic of the document. These common words are called stop words, and they are typically removed from textual analysis.\nTidyText provides a built in set of 1,149 different stop words. We can load the dataset and use anti_join to remove rows associated with words in the dataset.\n\n# Load stop words dataset and display it\ndata(stop_words)\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n# Create and display frequency count table after removing stop words from the dataset\nus_dec_counts &lt;- tidy_us_dec %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\nus_dec_counts\n\n# A tibble: 525 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        10\n 2 laws           9\n 3 government     6\n 4 john           6\n 5 thomas         6\n 6 william        6\n 7 free           5\n 8 george         5\n 9 powers         5\n10 time           5\n# ℹ 515 more rows\n\n\nWord Clouds\nA word cloud is a visualization of the most frequent words in the dataset:\n\nlibrary(wordcloud)\n\n# Show a word cloud with some customized options\n\nwordcloud(us_dec_counts$word, # column of words\n  us_dec_counts$n, # column of frequencies\n  scale = c(5, 0.2), # range of font sizes of words\n  min.freq = 2, # minimum word frequency to show\n  max.words = 200, # show the 200 most frequent words\n  random.order = FALSE, # position the most popular words first\n  colors = brewer.pal(8, \"Dark2\") # color palette\n) \n\nWarning in wordcloud(us_dec_counts$word, us_dec_counts$n, scale = c(5, 0.2), :\ntelecomputing could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s now create a TidyText data frame with the three Wikipedia documents we collected above via the API. Remember that the TidyText data frame has one row for each word.\n\n# Create the three-row original data frame\ntext_df &lt;- tibble(article = school_wiki_titles, text = school_wiki_text)\n\nError: object 'school_wiki_text' not found\n\ntext_df\n\nError in eval(expr, envir, enclos): object 'text_df' not found\n\n\n\n# Unnest the data frame so each row corresponds to a single word in a single document.\ntidy_df &lt;- text_df %&gt;%\n  unnest_tokens(word, text)\n\nError in eval(expr, envir, enclos): object 'text_df' not found\n\ntidy_df\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\n\n\n\n\nmacalester_counts &lt;- tidy_df %&gt;%\n  filter(article == \"Macalester College\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\nmacalester_counts\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\numn_counts &lt;- tidy_df %&gt;%\n  filter(article == \"University of Minnesota\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\numn_counts\n\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n\ncarleton_counts &lt;- tidy_df %&gt;%\n  filter(article == \"Carleton College\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\ncarleton_counts\n\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n\n\n\nwordcloud(macalester_counts$word, macalester_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\nwordcloud(umn_counts$word, umn_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n\nwordcloud(carleton_counts$word, carleton_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n\n\nBrainstorm\nHow do we compare multiple documents quantitatively?\n\nBrainstorm a metric for comparing the relative frequency/importance of different words in two or more documents. What factors might you account for?\n\n\n\n\n\nTo compare the prevalence of certain words in one document relative to another document, we could just count the occurrences. However, the documents may be different lengths, meaning that many more words might occur more often in the longer document. There are different ways to account for this, but one of the most common is term frequency - inverse document frequency.\n\nThe term frequency aims to capture how frequently a word appears in each document. There are different ways to measure this, including a raw count, logarithmically scaled (1 + log of the raw count), or Boolean (either 1 or 0 depending on whether the word occurs).\nThe inverse document frequency aims to capture how common the word is across documents. It is \\[\\log\\left(\\frac{N}{|\\{doc: word \\in doc\\}|}\\right),\\] where \\(N\\) is the number of documents, and the denominator of the fraction is the number of documents in which the selected word appears. Thus, if the word appears in all documents under consideration, the idf score is equal to log(1)=0.\nThe td-idf score is then the product of the term frequency and the inverse document frequency.\n\nWe’ll use the bind_tf_idf command from the tidytext library. Its default measure for term frequency is the raw count of a given word divided by the total number of words in the document. Let’s start by computing the thirty-five document-word pairs with the highest tf-idf scores:\n\ntfidf_analysis &lt;- tidy_df %&gt;%\n  count(article, word) %&gt;%\n  bind_tf_idf(word, article, n) %&gt;%\n  arrange(desc(tf_idf))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\nHere is a graphic with the same data:\n\ntfidf_analysis %&gt;%\n  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;%\n  top_n(35) %&gt;%\n  ggplot(aes(word, tf_idf, fill = article)) +\n  geom_col() +\n  labs(x = NULL, y = \"tf-idf\") +\n  coord_flip()\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\nNext, let’s say we want to determine which school is the most relevant to the query “internationalism, multiculturalism, and service to society.”\n\ntarget_words &lt;- c(\"internationalism\", \"multiculturalism\", \"service\", \"society\")\nmission &lt;- tfidf_analysis %&gt;%\n  filter(word %in% target_words)\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'mission' not found\n\n\n\n\n\nAnother metric for comparing the frequency of different words in two documents is the log odds ratio:\n\\[\\log\\left(\\frac{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc1}}}{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc2}}} \\right),\\] where \\(n\\) is the number of times the word appears and \\(total\\) is the total number of words in the document.\n\ntotal.mac &lt;- nrow(filter(tidy_df, article == \"Macalester College\"))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\ntotal.carleton &lt;- nrow(filter(tidy_df, article == \"Carleton College\"))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\nlogratios &lt;- macalester_counts %&gt;%\n  full_join(carleton_counts, by = \"word\", suffix = c(\".mac\", \".carleton\")) %&gt;%\n  replace_na(list(n.mac = 0, n.carleton = 0)) %&gt;%\n  mutate(n.total = n.mac + n.carleton) %&gt;%\n  filter(n.total &gt;= 5) %&gt;%\n  mutate(logodds.mac = log(((n.mac + 1) / (total.mac + 1)) / ((n.carleton + 1) / (total.carleton + 1))))\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\n\nWhich words appear at roughly equal frequencies?\n\nlogratios %&gt;%\n  arrange(abs(logodds.mac)) %&gt;%\n  head(n = 20)\n\nError in eval(expr, envir, enclos): object 'logratios' not found\n\n\nWhat are the most distinctive words?\n\nlogratios %&gt;%\n  group_by(logodds.mac &lt; 0) %&gt;%\n  top_n(15, abs(logodds.mac)) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, logodds.mac)) %&gt;%\n  ggplot(aes(word, logodds.mac, fill = logodds.mac &lt; 0)) +\n  geom_col() +\n  coord_flip() +\n  ylab(\"log odds ratio (Mac/Carleton)\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Macalester\", \"Carleton\"))\n\nError in eval(expr, envir, enclos): object 'logratios' not found\n\n\n\n\n\n\nWe often want to understand whether text conveys certain characteristics. For example, is Macalester’s mission statement more happy, sad, or angry than that of the University of Minnesota?\nA common way of doing this is by using a word dictionary that contains a list of words with the characteristics we are seeking (e.g., a list of words that are happy, sad, or angry). We can then measure how often words with each characteristic appear in the text. These word dictionaries are also called lexicons, and dictionaries related to emotive feelings are often called sentiment lexicons.\nTidy Text’s sentiments dataset contains built-in sentiment lexicons. We can look at the structure of some of these:\n\nafinn &lt;- get_sentiments(\"afinn\")\n\nError: The textdata package is required to download the AFINN lexicon.\nInstall the textdata package to access this dataset.\n\nnrc &lt;- get_sentiments(\"nrc\")\n\nError: The textdata package is required to download the NRC word-emotion association lexicon.\nInstall the textdata package to access this dataset.\n\nbing &lt;- get_sentiments(\"bing\")\n\n\n\nError in eval(expr, envir, enclos): object 'afinn' not found\n\n\nError in eval(expr, envir, enclos): object 'nrc' not found\n\n\n\n\n\nword\nsentiment\n\n\n\n\n2-faces\nnegative\n\n\nabnormal\nnegative\n\n\nabolish\nnegative\n\n\nabominable\nnegative\n\n\nabominably\nnegative\n\n\nabominate\nnegative\n\n\n\n\n\nLet’s take a look at the sentiments described within each lexicon:\n\n# Show the number of words and unique sentiments in each lexicon\nafinn %&gt;%\n  summarize(num_words = n(), values = paste(sort(unique(value)), collapse = \", \"))\n\nError in eval(expr, envir, enclos): object 'afinn' not found\n\nnrc %&gt;%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n\nError in eval(expr, envir, enclos): object 'nrc' not found\n\nbing %&gt;%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n\n# A tibble: 1 × 2\n  num_words sentiments        \n      &lt;int&gt; &lt;chr&gt;             \n1      6786 negative, positive\n\n\nThe Tidy Text book has some great background on these data sets:\n\nThe three general-purpose lexicons are\n\nAFINN from Finn Årup Nielsen,\nbing from Bing Liu and collaborators, and\nnrc from Saif Mohammad and Peter Turney.\n\nAll three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.\n\nTo apply these dictionaries, we need to create a Tidy Text data frame with words for each row and join it to the dictionary scores. Let’s give this a try using the Macalester Statement of Purpose and Belief. We start by creating the Tidy Text data frame:\n\n# Declare a string containing the Macalester Statement of Purpose & Belief\nstatement &lt;- \"At Macalester College we believe that education is a fundamentally transforming experience. As a community of learners, the possibilities for this personal, social, and intellectual transformation extend to us all. We affirm the importance of the intellectual growth of the students, staff and faculty through individual and collaborative endeavor. We believe that this can best be achieved through an environment that values the diverse cultures of our world and recognizes our responsibility to provide a supportive and respectful environment for students, staff and faculty of all cultures and backgrounds.\n\nWe expect students to develop a broad understanding of the liberal arts while they are at Macalester. Students should follow a primary course of study in order to acquire an understanding of disciplinary theory and methodology; they should be able to apply their understanding of theories to address problems in the larger community. Students should develop the ability to use information and communication resources effectively, be adept at critical, analytical and logical thinking, and express themselves well in both oral and written forms. Finally, students should be prepared to take responsibility for their personal, social and intellectual choices.\n\nWe believe that the benefit of the educational experience at Macalester is the development of individuals who make informed judgments and interpretations of the broader world around them and choose actions or beliefs for which they are willing to be held accountable. We expect them to develop the ability to seek and use knowledge and experience in contexts that challenge and inform their suppositions about the world. We are committed to helping students grow intellectually and personally within an environment that models and promotes academic excellence and ethical behavior. The education a student begins at Macalester provides the basis for continuous transformation through learning and service.\"\n\n# Expand this into a tidy data frame, with one row per word\ntidy_df &lt;- tibble(college = c(\"Macalester College\"), text = statement) %&gt;%\n  unnest_tokens(word, text)\n\n# Display the data frame and the most popular words\ntidy_df\n\n# A tibble: 293 × 2\n   college            word         \n   &lt;chr&gt;              &lt;chr&gt;        \n 1 Macalester College at           \n 2 Macalester College macalester   \n 3 Macalester College college      \n 4 Macalester College we           \n 5 Macalester College believe      \n 6 Macalester College that         \n 7 Macalester College education    \n 8 Macalester College is           \n 9 Macalester College a            \n10 Macalester College fundamentally\n# ℹ 283 more rows\n\ntidy_df %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word)\n\n# A tibble: 105 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ability         2\n 2 academic        1\n 3 accountable     1\n 4 achieved        1\n 5 acquire         1\n 6 actions         1\n 7 address         1\n 8 adept           1\n 9 affirm          1\n10 analytical      1\n# ℹ 95 more rows\n\n\nNext, we join this data frame with the lexicon. Let’s use nrc. Since we don’t care about words not in the lexicon, we will use an inner join.\n\ntidy_df %&gt;%\n  inner_join(nrc) %&gt;%\n  count(sentiment)\n\nError: object 'nrc' not found\n\n\nThere are some odd sentiments for a mission statement (anger, disgust, fear, and negative). Let’s take a look at what words are contributing to them.\n\ntidy_df %&gt;%\n  inner_join(nrc) %&gt;%\n  filter(sentiment %in% c(\"anger\", \"disgust\", \"fear\", \"negative\")) %&gt;%\n  select(word, sentiment)\n\nError: object 'nrc' not found\n\n\nAs you can see, word dictionaries are not perfect tools. When using them, make sure you look at the individual words contributing to the overall patterns to ensure they make sense.\n\n\n\nThere are all sorts of other interesting questions we can ask when analyzing texts. These include:\n\nHow do word frequencies change over time (e.g., Twitter) or over the course of a text?\nWhat is the correlation between different words (or names of characters in novels)? For example, how frequently do they appear in the same section of a text, or within \\(K\\) number of words of each other?3\nHow can we visualize such co-occurrences with networks?\nWhat “topics” are found in different documents? What word collections comprise these topics? This area is called topic modeling.\nCan you guess who wrote a document by analyzing its text?\nHow does the structure of different languages (e.g., sentence structure, sentence length, parts-of-speech) compare? These and many other interesting questions are asked by computational linguists.",
    "crumbs": [
      "Src",
      "Text Analysis"
    ]
  },
  {
    "objectID": "src/18-Text_Analysis.html#learning-goals",
    "href": "src/18-Text_Analysis.html#learning-goals",
    "title": "Text Analysis",
    "section": "",
    "text": "Understand the analysis process of decomposing text into tokens and considering word/token frequency\nDevelop comfort in comparing the text across multiple documents in terms of tf-idf and log odds ratio\nDevelop comfort in using lexicons to perform sentiment analysis on a document of text",
    "crumbs": [
      "Src",
      "Text Analysis"
    ]
  },
  {
    "objectID": "src/18-Text_Analysis.html#introduction-to-text-analysis-in-r",
    "href": "src/18-Text_Analysis.html#introduction-to-text-analysis-in-r",
    "title": "Text Analysis",
    "section": "",
    "text": "We have seen how to manipulate strings with regular expressions. Here, we examine how to analyze longer text documents. Text refers to information composed primarily of words: song lyrics, Tweets, news articles, novels, Wikipedia articles, online forums, and countless other resources.\nIn R and most other programming languages, text is stored in strings of characters.\nThere are a variety of common ways to get strings containing the text we want to analyze.\n\n\nString Literals\nIt may be natural to start by declaring an R variable that holds a string. Let’s consider the U.S. Declaration of Independence. Here’s an R variable that contains one of the most memorable sentences in the Declaration of Independence:\n\nus_dec_sentence &lt;- \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n# Show the number of characters in the sentence.\nnchar(us_dec_sentence)\n\n[1] 209\n\n# Show the sentence itself.\nus_dec_sentence\n\n[1] \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n\nUnfortunately, creating literal string variables like this can become unwieldy for larger texts, or collections of multiple texts.\nUsing this technique, your R program would be narrowly written to analyze hard-coded string variables, and defining those string variables may take up the vast majority of our program’s source code, making it difficult to read.\nWe will discuss two more flexible ways of getting textual data: reading a .TXT file and accessing a web API.\n\n\nThe simplest file format for text is a .TXT (or .txt) file. A .txt file contains raw textual data. You can find .txt files by using Google’s filetype: search filter. Go to http://google.com and type filetype:txt declaration of independence in the search box.\nIn the results you should see many .txt files containing the U.S. Declaration of Independence.\nFor example, https://infamous.net/documents/declaration-of-independence.txt. We can read this .txt file into R as a string using the readr package.1\nBecause the text is so large, we use the strtrim function to only show the first 500 characters of the text.\n\nlibrary(readr)\nus_dec &lt;- read_file(\"https://infamous.net/documents/declaration-of-independence.txt\")\nnchar(us_dec)\n\n[1] 9841\n\nstrtrim(us_dec, 500)\n\n[1] \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Congress, July 4, 1776,\\nTHE UNANIMOUS DECLARATION OF THE THIRTEEN UNITED STATES OF AMERICA\\n\\nWhen in the Course of human events, it becomes necessary for one \\npeople to dissolve the political bands which have connected them \\nwith another, and to assume among the Powers of the earth, the \\nseparate and equal station to which the Laws of Nature and of \\nNature's God entitle them, a decent respect to the opinions of \\nmankind requires that they should declare the causes which\"\n\n\nNotice all those \\n sequences that appear in the string.\nThese are newline characters that denote the end of a line.\nThere are a few other special characters that you may see. For example, '\\t' is a tab.\n\n\n\nWhen we want to analyze textual data created on websites or mobile apps such as Facebook and Twitter, we can use web APIs to gather the text. Here is one example from the largest single collection of written knowledge in human history: Wikipedia!\nThe function below retrieves the text content of a Wikipedia article with a particular title. It uses Wikipedia’s Public API to do so, which enables any computer program to interact with Wikipedia. Wikipedia’s API is convenient for us because it is vast, open, and free. Don’t worry if you don’t follow the details of the code below.\n\nGetArticleText &lt;- function(langCode, titles) {\n  # Returns the text of the specified article in the specified language\n\n  # An accumulator variable that will hold the text of each article\n\n  # Create\n  texts &lt;- sapply(titles, function(t) {\n    print(t)\n    article_info &lt;- getForm(\n      paste(\"https://\", langCode, \".wikipedia.org/w/api.php\", sep = \"\"),\n      action  = \"query\",\n      prop = \"extracts\",\n      format  = \"json\",\n      explaintext = \"\",\n      titles  = t\n    )\n\n    js &lt;- fromJSON(article_info)\n    return(js$query$pages[[1]]$extract)\n  })\n  return(texts)\n}\n\n# Get the text for https://en.wikipedia.org/wiki/Macalester_College,\n# https://en.wikipedia.org/wiki/Carleton_College, and https://en.wikipedia.org/wiki/University_of_Minnesota in English (\"en\").\n# We could also get the text for the Spanish article (\"es\"), or German article (\"de\")\n\nschool_wiki_titles &lt;- c(\"Macalester College\", \"Carleton College\", \"University of Minnesota\")\nschool_wiki_text &lt;- GetArticleText(\"en\", school_wiki_titles)\n\n[1] \"Macalester College\"\n\n\nError in function (type, msg, asError = TRUE) : schannel: CertGetCertificateChain trust error CERT_TRUST_IS_NOT_TIME_VALID\n\n# Print out the first 500 characters of the text\nstrtrim(school_wiki_text, 500)\n\nError in eval(expr, envir, enclos): object 'school_wiki_text' not found\n\n\nWe’ll analyze these documents further below.\n\n\n\n\nIf we tried to make a data frame directly out of the text, it would look odd. It contains the text as a single row in a column named “text.” This doesn’t seem any more useful than the original string itself.\n\nus_dec_df &lt;- tibble(title = \"Declaration of Independence\", text = us_dec)\nus_dec_df\n\n# A tibble: 1 × 2\n  title                       text                                              \n  &lt;chr&gt;                       &lt;chr&gt;                                             \n1 Declaration of Independence \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Con…\n\n\n\n\nWe need to restructure the text into components that can be easily analyzed.\nWe will use two units of data:\n\nA token is the smallest textual information unit we wish to measure, typically a word.\nA document is a collection of tokens.\n\nFor our example here, the Declaration of Independence is the document, and a word is the token. However, a document could be a tweet, a novel chapter, a Wikipedia article, or anything else that seems interesting. Other possibilities for tokens include sentences, lines, paragraphs, characters, ngrams, and more.2\nLater on, we will also give an example of how to perform textual analyses comparing two or more documents.\nWe will be using the tidy text format, which has one row for each unit of analysis. Our work will focus on word-level analysis within each document, so each row will contain a document and word.\nTidyText’s unnest_tokens function takes a data frame containing one row per document and breaks it into a data frame containing one row per token.\n\ntidy_us_dec &lt;- us_dec_df %&gt;%\n  unnest_tokens(word, text)\n\ntidy_us_dec\n\n# A tibble: 1,539 × 2\n   title                       word        \n   &lt;chr&gt;                       &lt;chr&gt;       \n 1 Declaration of Independence the         \n 2 Declaration of Independence declaration \n 3 Declaration of Independence of          \n 4 Declaration of Independence independence\n 5 Declaration of Independence in          \n 6 Declaration of Independence congress    \n 7 Declaration of Independence july        \n 8 Declaration of Independence 4           \n 9 Declaration of Independence 1776        \n10 Declaration of Independence the         \n# ℹ 1,529 more rows\n\n\nNote that because we only have one document, the initial data frame (us_dec_df) is just one row and the tidy text data frame (tidy_us_dec) has the same title for each row.\nLater on we will analyze more than one document and these columns can change.\nWe can now analyze this tidy text data frame. For example, we can determine the total number of words.\n\nnrow(tidy_us_dec)\n\n[1] 1539\n\n\nWe can also find the most frequently used words by using dplyr’s count function, which creates a frequency table for (in our case) words:\n\n# Create and display frequency count table\nall_us_dec_counts &lt;- tidy_us_dec %&gt;%\n  count(word, sort = TRUE)\nall_us_dec_counts\n\n# A tibble: 674 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the      84\n 2 of       83\n 3 to       67\n 4 and      58\n 5 for      28\n 6 our      26\n 7 has      20\n 8 in       20\n 9 their    20\n10 he       19\n# ℹ 664 more rows\n\n\nWe can count the rows in this data frame to determine how many different unique words appear in the document.\n\nnrow(all_us_dec_counts)\n\n[1] 674\n\n\n\n\n\nNotice that the most frequent words are common words that are present in any document and not particularly descriptive of the topic of the document. These common words are called stop words, and they are typically removed from textual analysis.\nTidyText provides a built in set of 1,149 different stop words. We can load the dataset and use anti_join to remove rows associated with words in the dataset.\n\n# Load stop words dataset and display it\ndata(stop_words)\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n# Create and display frequency count table after removing stop words from the dataset\nus_dec_counts &lt;- tidy_us_dec %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\nus_dec_counts\n\n# A tibble: 525 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        10\n 2 laws           9\n 3 government     6\n 4 john           6\n 5 thomas         6\n 6 william        6\n 7 free           5\n 8 george         5\n 9 powers         5\n10 time           5\n# ℹ 515 more rows\n\n\nWord Clouds\nA word cloud is a visualization of the most frequent words in the dataset:\n\nlibrary(wordcloud)\n\n# Show a word cloud with some customized options\n\nwordcloud(us_dec_counts$word, # column of words\n  us_dec_counts$n, # column of frequencies\n  scale = c(5, 0.2), # range of font sizes of words\n  min.freq = 2, # minimum word frequency to show\n  max.words = 200, # show the 200 most frequent words\n  random.order = FALSE, # position the most popular words first\n  colors = brewer.pal(8, \"Dark2\") # color palette\n) \n\nWarning in wordcloud(us_dec_counts$word, us_dec_counts$n, scale = c(5, 0.2), :\ntelecomputing could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s now create a TidyText data frame with the three Wikipedia documents we collected above via the API. Remember that the TidyText data frame has one row for each word.\n\n# Create the three-row original data frame\ntext_df &lt;- tibble(article = school_wiki_titles, text = school_wiki_text)\n\nError: object 'school_wiki_text' not found\n\ntext_df\n\nError in eval(expr, envir, enclos): object 'text_df' not found\n\n\n\n# Unnest the data frame so each row corresponds to a single word in a single document.\ntidy_df &lt;- text_df %&gt;%\n  unnest_tokens(word, text)\n\nError in eval(expr, envir, enclos): object 'text_df' not found\n\ntidy_df\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\n\n\n\n\nmacalester_counts &lt;- tidy_df %&gt;%\n  filter(article == \"Macalester College\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\nmacalester_counts\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\numn_counts &lt;- tidy_df %&gt;%\n  filter(article == \"University of Minnesota\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\numn_counts\n\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n\ncarleton_counts &lt;- tidy_df %&gt;%\n  filter(article == \"Carleton College\") %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\ncarleton_counts\n\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n\n\n\nwordcloud(macalester_counts$word, macalester_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\nwordcloud(umn_counts$word, umn_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n\nwordcloud(carleton_counts$word, carleton_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n\n\nBrainstorm\nHow do we compare multiple documents quantitatively?\n\nBrainstorm a metric for comparing the relative frequency/importance of different words in two or more documents. What factors might you account for?\n\n\n\n\n\nTo compare the prevalence of certain words in one document relative to another document, we could just count the occurrences. However, the documents may be different lengths, meaning that many more words might occur more often in the longer document. There are different ways to account for this, but one of the most common is term frequency - inverse document frequency.\n\nThe term frequency aims to capture how frequently a word appears in each document. There are different ways to measure this, including a raw count, logarithmically scaled (1 + log of the raw count), or Boolean (either 1 or 0 depending on whether the word occurs).\nThe inverse document frequency aims to capture how common the word is across documents. It is \\[\\log\\left(\\frac{N}{|\\{doc: word \\in doc\\}|}\\right),\\] where \\(N\\) is the number of documents, and the denominator of the fraction is the number of documents in which the selected word appears. Thus, if the word appears in all documents under consideration, the idf score is equal to log(1)=0.\nThe td-idf score is then the product of the term frequency and the inverse document frequency.\n\nWe’ll use the bind_tf_idf command from the tidytext library. Its default measure for term frequency is the raw count of a given word divided by the total number of words in the document. Let’s start by computing the thirty-five document-word pairs with the highest tf-idf scores:\n\ntfidf_analysis &lt;- tidy_df %&gt;%\n  count(article, word) %&gt;%\n  bind_tf_idf(word, article, n) %&gt;%\n  arrange(desc(tf_idf))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\nHere is a graphic with the same data:\n\ntfidf_analysis %&gt;%\n  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;%\n  top_n(35) %&gt;%\n  ggplot(aes(word, tf_idf, fill = article)) +\n  geom_col() +\n  labs(x = NULL, y = \"tf-idf\") +\n  coord_flip()\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\nNext, let’s say we want to determine which school is the most relevant to the query “internationalism, multiculturalism, and service to society.”\n\ntarget_words &lt;- c(\"internationalism\", \"multiculturalism\", \"service\", \"society\")\nmission &lt;- tfidf_analysis %&gt;%\n  filter(word %in% target_words)\n\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'mission' not found\n\n\n\n\n\nAnother metric for comparing the frequency of different words in two documents is the log odds ratio:\n\\[\\log\\left(\\frac{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc1}}}{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc2}}} \\right),\\] where \\(n\\) is the number of times the word appears and \\(total\\) is the total number of words in the document.\n\ntotal.mac &lt;- nrow(filter(tidy_df, article == \"Macalester College\"))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\ntotal.carleton &lt;- nrow(filter(tidy_df, article == \"Carleton College\"))\n\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n\nlogratios &lt;- macalester_counts %&gt;%\n  full_join(carleton_counts, by = \"word\", suffix = c(\".mac\", \".carleton\")) %&gt;%\n  replace_na(list(n.mac = 0, n.carleton = 0)) %&gt;%\n  mutate(n.total = n.mac + n.carleton) %&gt;%\n  filter(n.total &gt;= 5) %&gt;%\n  mutate(logodds.mac = log(((n.mac + 1) / (total.mac + 1)) / ((n.carleton + 1) / (total.carleton + 1))))\n\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n\n\nWhich words appear at roughly equal frequencies?\n\nlogratios %&gt;%\n  arrange(abs(logodds.mac)) %&gt;%\n  head(n = 20)\n\nError in eval(expr, envir, enclos): object 'logratios' not found\n\n\nWhat are the most distinctive words?\n\nlogratios %&gt;%\n  group_by(logodds.mac &lt; 0) %&gt;%\n  top_n(15, abs(logodds.mac)) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, logodds.mac)) %&gt;%\n  ggplot(aes(word, logodds.mac, fill = logodds.mac &lt; 0)) +\n  geom_col() +\n  coord_flip() +\n  ylab(\"log odds ratio (Mac/Carleton)\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Macalester\", \"Carleton\"))\n\nError in eval(expr, envir, enclos): object 'logratios' not found\n\n\n\n\n\n\nWe often want to understand whether text conveys certain characteristics. For example, is Macalester’s mission statement more happy, sad, or angry than that of the University of Minnesota?\nA common way of doing this is by using a word dictionary that contains a list of words with the characteristics we are seeking (e.g., a list of words that are happy, sad, or angry). We can then measure how often words with each characteristic appear in the text. These word dictionaries are also called lexicons, and dictionaries related to emotive feelings are often called sentiment lexicons.\nTidy Text’s sentiments dataset contains built-in sentiment lexicons. We can look at the structure of some of these:\n\nafinn &lt;- get_sentiments(\"afinn\")\n\nError: The textdata package is required to download the AFINN lexicon.\nInstall the textdata package to access this dataset.\n\nnrc &lt;- get_sentiments(\"nrc\")\n\nError: The textdata package is required to download the NRC word-emotion association lexicon.\nInstall the textdata package to access this dataset.\n\nbing &lt;- get_sentiments(\"bing\")\n\n\n\nError in eval(expr, envir, enclos): object 'afinn' not found\n\n\nError in eval(expr, envir, enclos): object 'nrc' not found\n\n\n\n\n\nword\nsentiment\n\n\n\n\n2-faces\nnegative\n\n\nabnormal\nnegative\n\n\nabolish\nnegative\n\n\nabominable\nnegative\n\n\nabominably\nnegative\n\n\nabominate\nnegative\n\n\n\n\n\nLet’s take a look at the sentiments described within each lexicon:\n\n# Show the number of words and unique sentiments in each lexicon\nafinn %&gt;%\n  summarize(num_words = n(), values = paste(sort(unique(value)), collapse = \", \"))\n\nError in eval(expr, envir, enclos): object 'afinn' not found\n\nnrc %&gt;%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n\nError in eval(expr, envir, enclos): object 'nrc' not found\n\nbing %&gt;%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n\n# A tibble: 1 × 2\n  num_words sentiments        \n      &lt;int&gt; &lt;chr&gt;             \n1      6786 negative, positive\n\n\nThe Tidy Text book has some great background on these data sets:\n\nThe three general-purpose lexicons are\n\nAFINN from Finn Årup Nielsen,\nbing from Bing Liu and collaborators, and\nnrc from Saif Mohammad and Peter Turney.\n\nAll three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.\n\nTo apply these dictionaries, we need to create a Tidy Text data frame with words for each row and join it to the dictionary scores. Let’s give this a try using the Macalester Statement of Purpose and Belief. We start by creating the Tidy Text data frame:\n\n# Declare a string containing the Macalester Statement of Purpose & Belief\nstatement &lt;- \"At Macalester College we believe that education is a fundamentally transforming experience. As a community of learners, the possibilities for this personal, social, and intellectual transformation extend to us all. We affirm the importance of the intellectual growth of the students, staff and faculty through individual and collaborative endeavor. We believe that this can best be achieved through an environment that values the diverse cultures of our world and recognizes our responsibility to provide a supportive and respectful environment for students, staff and faculty of all cultures and backgrounds.\n\nWe expect students to develop a broad understanding of the liberal arts while they are at Macalester. Students should follow a primary course of study in order to acquire an understanding of disciplinary theory and methodology; they should be able to apply their understanding of theories to address problems in the larger community. Students should develop the ability to use information and communication resources effectively, be adept at critical, analytical and logical thinking, and express themselves well in both oral and written forms. Finally, students should be prepared to take responsibility for their personal, social and intellectual choices.\n\nWe believe that the benefit of the educational experience at Macalester is the development of individuals who make informed judgments and interpretations of the broader world around them and choose actions or beliefs for which they are willing to be held accountable. We expect them to develop the ability to seek and use knowledge and experience in contexts that challenge and inform their suppositions about the world. We are committed to helping students grow intellectually and personally within an environment that models and promotes academic excellence and ethical behavior. The education a student begins at Macalester provides the basis for continuous transformation through learning and service.\"\n\n# Expand this into a tidy data frame, with one row per word\ntidy_df &lt;- tibble(college = c(\"Macalester College\"), text = statement) %&gt;%\n  unnest_tokens(word, text)\n\n# Display the data frame and the most popular words\ntidy_df\n\n# A tibble: 293 × 2\n   college            word         \n   &lt;chr&gt;              &lt;chr&gt;        \n 1 Macalester College at           \n 2 Macalester College macalester   \n 3 Macalester College college      \n 4 Macalester College we           \n 5 Macalester College believe      \n 6 Macalester College that         \n 7 Macalester College education    \n 8 Macalester College is           \n 9 Macalester College a            \n10 Macalester College fundamentally\n# ℹ 283 more rows\n\ntidy_df %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word)\n\n# A tibble: 105 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ability         2\n 2 academic        1\n 3 accountable     1\n 4 achieved        1\n 5 acquire         1\n 6 actions         1\n 7 address         1\n 8 adept           1\n 9 affirm          1\n10 analytical      1\n# ℹ 95 more rows\n\n\nNext, we join this data frame with the lexicon. Let’s use nrc. Since we don’t care about words not in the lexicon, we will use an inner join.\n\ntidy_df %&gt;%\n  inner_join(nrc) %&gt;%\n  count(sentiment)\n\nError: object 'nrc' not found\n\n\nThere are some odd sentiments for a mission statement (anger, disgust, fear, and negative). Let’s take a look at what words are contributing to them.\n\ntidy_df %&gt;%\n  inner_join(nrc) %&gt;%\n  filter(sentiment %in% c(\"anger\", \"disgust\", \"fear\", \"negative\")) %&gt;%\n  select(word, sentiment)\n\nError: object 'nrc' not found\n\n\nAs you can see, word dictionaries are not perfect tools. When using them, make sure you look at the individual words contributing to the overall patterns to ensure they make sense.\n\n\n\nThere are all sorts of other interesting questions we can ask when analyzing texts. These include:\n\nHow do word frequencies change over time (e.g., Twitter) or over the course of a text?\nWhat is the correlation between different words (or names of characters in novels)? For example, how frequently do they appear in the same section of a text, or within \\(K\\) number of words of each other?3\nHow can we visualize such co-occurrences with networks?\nWhat “topics” are found in different documents? What word collections comprise these topics? This area is called topic modeling.\nCan you guess who wrote a document by analyzing its text?\nHow does the structure of different languages (e.g., sentence structure, sentence length, parts-of-speech) compare? These and many other interesting questions are asked by computational linguists.",
    "crumbs": [
      "Src",
      "Text Analysis"
    ]
  },
  {
    "objectID": "src/18-Text_Analysis.html#footnotes",
    "href": "src/18-Text_Analysis.html#footnotes",
    "title": "Text Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInstead of reading the file directly from the internet, it is often a good idea to first save it to your working directory through your browser, and then read it locally. The benefits of this include having the data backed-up in case the website changes, and being able to run your code if you are offline. The drawback is that if the website is updated and you actually want those changes to be reflected in your text analysis, you’ll have to download new versions periodically.↩︎\nSee the help for unnest_tokens to learn more about options for the token.↩︎\nCheck out the widyr package and its pairwise_count() function if interested in these and similar questions.↩︎",
    "crumbs": [
      "Src",
      "Text Analysis"
    ]
  },
  {
    "objectID": "src/16-API.html",
    "href": "src/16-API.html",
    "title": "Public Web APIs",
    "section": "",
    "text": "Understand the difference between acquiring data through web scraping vs. a web API\nSet up an API key for a public API\nDevelop comfort in using a wrapper package or url-method of calling a web API\nRecognize the structure in a url for a web API and adjust for your purposes\n\nYou can download a template .Rmd of this activity here.\n\n\n\nIn this lesson you’ll learn how to collect data from websites such as The New York Times, Zillow, and Google. While these sites are primarily known for the information they provide to humans browsing the web, they (along with most large websites) also provide information to computer programs.\nHumans use browsers such as Firefox or Chrome to navigate the web. Behind the scenes, our browsers communicate with web servers using a technology called HTTP or Hypertext Transfer Protocol.\nProgramming languages such as R can also use HTTP to communicate with web servers. We have seen how it is possible for R to “scrape” data from almost any static web page. However, it’s easiest to interact with websites that are specifically designed to communicate with programs. These Web APIs, or Web Application Programming Interfaces, focus on transmitting data, rather than images, colors, or other appearance-related information.\nAn large variety of web APIs provide data accessible to programs written in R (and almost any other programming language!). Almost all reasonably large commercial websites offer APIs. Todd Motto has compiled an excellent list of Public Web APIs on GitHub. Browse the list to see what kind of information is available.\n\n\n\nPossible readings:  1. NY Times API  2. NY Times Blog post announcing the API  3. Working with the NY Times API in R\n4. nytimes pacakge for accessing the NY Times’ APIs from R  5. Video showing how to use the NY Times API  6. rOpenSci has a good collection of wrapper packages\nIn R, it is easiest to use Web APIs through a wrapper package, an R package written specifically for a particular Web API. The R development community has already contributed wrapper packages for most large Web APIs. To find a wrapper package, search the web for “R Package” and the name of the website. For example, a search for “R Reddit Package” returns RedditExtractor and a search for “R Weather.com Package” surfaces weatherData.\nThis activity will build on the New York Times Web API, which provides access to news articles, movie reviews, book reviews, and many other data. Our activity will specifically focus on the Article Search API, which finds information about news articles that contain a particular word or phrase.\nWe will use the nytimes package that provides functions for some (but not all) of the NYTimes APIs. First, install the package by copying the following two lines into your console (you just need to run these once):\ninstall.packages(\"devtools\")\ndevtools::install_github(\"mkearney/nytimes\")\nNext, take a look at the Article Search API example on the package website to get a sense of the syntax.\n\nWhat do you think the nytimes function below does? How does it communicate with the NY Times? Where is the data about articles stored?\n\n\n\nres &lt;- nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\")\n\nTo get started with the NY Times API, you must register and get an authentication key. Signup only takes a few seconds, and it lets the New York Times make sure nobody abuses their API for commercial purposes. It also rate limits their API and ensures programs don’t make too many requests per day. For the NY Times API, this limit is 1000 calls per day. Be aware that most APIs do have rate limits — especially for their free tiers.\nOnce you have signed up, verified your email, log back in to https://developer.nytimes.com. Under your email address, click on Apps and Create a new App (call it First API) and enable Article Search API, then press Save. This creates an authentication key, which is a 32 digit string with numbers and the letters a-e.\nStore this in a variable as follows (this is just an example ID, not an actual one):\n\ntimes_key &lt;- \"c935b213b2dc1218050eec976283dbbd\"\n\n# Tell nytimes what our API key is\nSys.setenv(NYTIMES_KEY = times_key)\n\nNow, let’s use the key to issue our first API call. We’ll adapt the code we see in the vignette to do what we need.\n\nlibrary(nytimes)\n\nError in library(nytimes): there is no package called 'nytimes'\n\n# Issue our first API call\nres &lt;- nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\")\n\nError in nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\"): could not find function \"nyt_search\"\n\n# Convert response object to data frame\nres &lt;- as.data.frame(res)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\n\nSomething magical just happened. Your computer sent a message to the New York Times and asked for information about 20 articles about Gamergate starting at January 1, 2015 and going backwards in time. Thousands of public Web APIs allow your computer to tap into almost any piece of public digital information on the web.\nLet’s take a peek at the structure of the results. You can also look at the data in the “Environment” tab in one of the windows of RStudio:\n\ncolnames(res)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$web_url)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$headline)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$pub_date)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\n\n\n\n\nWrapper packages such as nytimes provide a convenient way to interact with Web APIs. However, many Web APIs have incomplete wrapper packages, or no wrapper package at all. Fortunately, most Web APIs share a common structure that R can access relatively easily. There are two parts to each Web API: the request, which corresponds to a function call, and the response, which corresponds to the function’s return value.1\nAs mentioned earlier, a Web API call differs from a regular function call in that the request is sent over the Internet to a webserver, which performs the computation and calculates the return result, which is sent back over the Internet to the original computer.\nWeb API Requests\nPossible readings:  1. Understanding URLs  2. urltools Vignette\nThe request for a Web API call is usually encoded through the URL, the web address associated with the API’s webserver. Let’s look at the URL associated with the first nytimes nyt_search example we did. Open the following URL in your browser (you should replace MY_KEY with the api key you were given earlier).\nhttp://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate&api-key=MY_KEY\nThe text you see in the browser is the response data. We’ll talk more about that in a bit. Right now, let’s focus on the structure of the URL. You can see that it has a few parts:\n\nhttp:// — The scheme, which tells your browser or program how to communicate with the webserver. This will typically be either http: or https:.\napi.nytimes.com — The hostname, which is a name that identifies the webserver that will process the request.\n/svc/search/v2/articlesearch.json — The path, which tells the webserver what function you would like to call.\n?q=gamergate&api-key=MY_KEY — The query parameters, which provide the parameters for the function you would like to call. Note that the query can be thought of as a table, where each row has a key and a value (known as a key-value pair). In this case, the first row has key q and value gamergate and the second row has value MY_KEY. The query parameters are preceded by a ?. Rows in the key-value table are separated by ‘&’, and individual key-value pairs are separated by an =.\n\nTypically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (http://api.nytimes.com/svc/search/v2/articlesearch.json) will be referred to as the endpoint for the API call.\nWe will use the urltools module to build up a full URL from its parts. We start by creating a string with the endpoint and then add the parameters one by one using param_set and url_encode:\n\nlibrary(urltools)\n\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"marlon james\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\nurl\n\nCopy and paste the resulting URL into your browser to see what the NY Times response looks like!\n\nYou may be wondering why we need to use `param_set` and `url_encode` instead of writing the full url by hand. This exercise will illustrate why we need to be careful. \n\n a) Repeat the above steps, but create a URL that finds articles related to `Ferris Bueller's Day Off` (note the apostrophe). What is interesting about how the title appears in the URL?\n b) Repeat the steps above for the phrase `Nico & Vinz` (make sure you use the punctuation mark `&`). What do you notice?\n c) Take a look at the Wikipedia page describing [percent encoding](https://en.wikipedia.org/wiki/Percent-encoding). Explain how the process works in your own words.\n\n\n\n\n\nPossible readings:  1. A Non-Programmer’s Introduction to JSON  2. Getting Started With JSON and jsonlite  3. Fetching JSON data from REST APIs\nWe now discuss the structure of the web response, the return value of the Web API function. Web APIs generate string responses. If you visited the earlier New York Times API link in your browser, you would be shown the string response from the New York Times webserver:\n{\"status\":\"OK\",\"copyright\":\"Copyright (c) 2021 The New York Times Company. All Rights Reserved.\",\"response\":{\"docs\":[{\"abstract\":\"Here’s what you need to know.\",\"web_url\":\"https://www.nytimes.com/2019/08/16/briefing/rashida-tlaib-gamergate-greenland.html\",\"snippet\":\"Here’s what you need to know.\",\"lead_paragraph\":\"(Want to get this briefing by email? Here’s the sign-up.)\",\"source\":\"The New York Times\",\"multimedia\":[{\"rank\":0,\"subtype\":\"xlarge\",\"caption\":null,\"credit\":null,\"type\":\"image\",\"url\":\"images/2019/08/16/world/16US-AMBRIEFING-TLAIB-amcore/merlin_158003643_c67928bc-e547-4a2e-9344-5f0209ca024d-articleLarge.jpg\",\"height\":400,\"width\":600,\"legacy\":{\"xlarge\":\"images/2019/08/16/world/16US-AMBRIEFING-TLAIB-amcore/merlin_158003643_c67928bc-e547-4a2e-9344-5f0209ca024d-articleLarge.jpg\",\"xlargewidth\":600,\"xlargeheight\":400},\"subType\":\"xlarge\",\"crop_name\":\"articleLarge\"},...\nIf you stared very hard at the above response, you may be able to interpret it. However, it would be much easier to interact with the response in some more structured, programmatic way. The vast majority of Web APIs, including the New York Times, use a standard called JSON (Javascript Object Notation) to take data and encode it as a string. To understand the structure of JSON, take the NY Times web response in your browser, and copy and paste it into an online JSON formatter. The formatter will add newlines and tabs to make the data more human interpretable. You’ll see the following:\n{  \n   \"status\":\"OK\",\n   \"copyright\":\"Copyright (c) 2021 The New York Times Company. All Rights Reserved.\",\n   \"response\":{  \n      \"docs\":[  \n      \n        # A HUGE piece of data, with one object for each of the result articles\n        \n      ],\n      \"meta\":{  \n         \"hits\":128,\n         \"offset\":0,\n         \"time\":93\n      }\n   }\n}     \nYou’ll notice a few things in the JSON above:\n\nStrings are enclosed in double quotes, for example \"status\" and \"OK\".\nNumbers are written plainly, like 2350 or 72.\nSome data is enclosed in square brackets [ and ]. These data containers can be thought of as R lists.\nSome data is enclosed in curly braces { and }. These data containers are called Objects. An object can be thought of as a single observation in a table. The columns or variables for the observation appear as keys on the left (hits, offset, etc.). The values appear after the specific key separated by a colon (2350, and 0, respectively). Thus, we can think of the meta object above as:\n\n\n\n\n\n\n\nhits\noffset\ntime\n\n\n\n\n128\n0\n93\n\n\n\n\n\n\n\n\nLet’s repeat the NY Times search for gamergate, but this time we will peform the Web API call by hand instead of using the nytimes wrapper package. We will use the jsonlite package to retrieve the response from the webserver and turn the string response into an R object. The fromJson function sends our request out over and across the web to the NY Times webserver, retrieves it, and turns it from a JSON-formatted string into R data.\n\nlibrary(jsonlite)\n\n# Rebuild the URL\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"gamergate\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\n\nError in eval(expr, envir, enclos): object 'times_key' not found\n\n# Send the request to the webserver over the Internet and\n# retrieve the JSON response. Turn the JSON response into an\n# R Object.\nresponse_js &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate': HTTP\nstatus was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate'\n\n\nThe jsonlite makes the keys and values of an object available as attributes. For example, we can fetch the status:\n\nresponse_js$status\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\n\nWhile some keys in the object are associated with simple values, such as \"status\", others are associated with more complex data. For example, the key \"response\" is associated with an object that has two keys: \"docs\", and \"meta\". \"meta\" is another object: {  \"hits\":128, \"offset\":0, \"time\":19 }. We can retrieve these nested attributes by sequentially accessing the object keys from the outside in. For example, the inner \"hits\" attribute would be accessed as follows:\n\nresponse_js$response$meta$hits\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\n\n\nRetrieve the data associated with \n\n1) the `copyright` key of the `response_js` object, and \n2) the `time` attribute nested within the `meta` object.\n\n\nThe majority of the data is stored under response, in docs. Notice that docs is a list, where each element of the list is a JSON object that looks like the following:\n {  \n  \"web_url\":\"https://www.nytimes.com/2017/06/27/arts/milkshake-duck-meme.html\",\n  \"snippet\":\"Oxford Dictionaries is keeping a close eye on a term that describes someone who rapidly gains and inevitably loses the internet’s intense love.\",  \n  \"blog\":{  },  \n  \"source\":\"The New York Times\",  \n  \"multimedia\":[    \n      ... A LIST OF OBJECTS ...  \n  ],  \n  \"headline\":{    \n     \"main\":\"How a Joke Becomes a Meme: The Birth of ‘Milkshake Duck’\",  \n     \"print_headline\":\"How a Joke Becomes a Meme: The Birth of ‘Milkshake Duck’\"  \n  },  \n  \"keywords\":[    \n      ... A LIST OF OBJECTS ...  \n  ],  \n  \"pub_date\":\"2017-06-27T12:24:20+0000\",  \n  \"document_type\":\"article\",  \n  \"new_desk\":\"Culture\",  \n  \"byline\":{    \n     \"original\":\"By JONAH ENGEL BROMWICH\"  \n  },  \n  \"type_of_material\":\"News\",  \n  \"_id\":\"59524e7f7c459f257c1ac39f\",  \n  \"word_count\":1033,  \n  \"score\":0.35532707,  \n  \"uri\":\"nyt://article/a3e5bf4a-6216-5dba-9983-73bc45a98e69\"  \n},\njsonlite makes lists of objects available as a data frame, where the columns are the keys in the object (web_url, snippet, etc.)\n\ndocs_df &lt;- response_js$response$docs\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\nclass(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\ncolnames(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\ndim(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\n\n\nConsider the following:\n\n a) Select your own article search query (any topic of interest to you). You may want to play with NY Times online search or the [API web search console](https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get) to find a query that is interesting, but not overly popular. You can change any part of the query you would like. Your query should have at least 30 matches.\n b) Retrieve data for the first three pages of search results from the article search API, and create a data frame that joins together the `docs` data frames for the three pages of results.  Hint: The example in the section below shows how to get different pages of results and use `bind_rows to combine them.   \n c) Visualize the number of search results per day or month in your result set.\n \n\n\n\nHere is some code to generate queries on NY Times articles about the Red Sox. It fetches the first thirty entries in batches of 10.\n\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"Red Sox\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\n\nError in eval(expr, envir, enclos): object 'times_key' not found\n\nurl &lt;- param_set(url, \"page\", 0)\nres1 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=0':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=0'\n\n# This pauses for 1 second.\n# It is required when knitting to prevent R from issuing too many requests to\n# The NY Times API at a time. If you don't have it you will get an error that\n# says \"Too Many Requests (429)\"\nSys.sleep(1)\nurl &lt;- param_set(url, \"page\", 1)\nres2 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=1':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=1'\n\nSys.sleep(1)\nurl &lt;- param_set(url, \"page\", 2)\nres3 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=2':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=2'\n\ndocs1 &lt;- res1$response$docs\n\nError in eval(expr, envir, enclos): object 'res1' not found\n\ndocs2 &lt;- res2$response$docs\n\nError in eval(expr, envir, enclos): object 'res2' not found\n\ndocs3 &lt;- res3$response$docs\n\nError in eval(expr, envir, enclos): object 'res3' not found\n\n\nEach of these docs variables is a table with ten entries (articles) and the same 18 variables:\n\nnames(docs1)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nNow we want to stack the tables on top of each other to get a single table with 30 rows and 18 variables. If you try the following command:\nbind_rows(docs1,docs2,docs3)\nthen you will get an error saying “Error in bind_rows_(x, .id) : Argument 4 can’t be a list containing data frames.”\nWhat is happening???\nLet’s check out the first column of the docs1 table:\n\ndocs1$web_url\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nIt lists the web addresses of the first ten sites returned in the search. It is a vector of ten character strings, which is just fine for one column of data in our table.\nNow let’s check out the headline variable:\n\ndocs1$headline\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nThe headline variable is actually a data frame that contains three variables: main, kicker, and print_headline. That is, we have nested data frames. This is a common problem when scraping data from JSON files, and it is why we are not able to directly bind the rows of our three tables on top of each other.\nWe can check out the type of variable in each column with the class function:\n\nsapply(docs1, class)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nWe see that blog, headline, and byline are the three problem columns that each contain their own data frames.\nThe solution is to flatten these variables, which generates a new column in the outer table for each of the columns in the inner tables.\n\ndocs1_flat &lt;- jsonlite::flatten(docs1)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\nnames(docs1_flat)\n\nError in eval(expr, envir, enclos): object 'docs1_flat' not found\n\nsapply(docs1_flat, class)\n\nError in eval(expr, envir, enclos): object 'docs1_flat' not found\n\n\nThe headline variable is now replaced with seven separate columns for headline.main, headline.kicker, headline.content_kicker, headline.print_headline, headline.name, headline.seo, and headline.sub. The byline variable is replaced with three separae columns. The blog variable contained an empty data frame, so it has been removed. The overall result is a new flat table with 25 columns, and no more nested data frames.\nOnce the data is flattened, we can bind rows:\n\nall_docs &lt;- bind_rows(jsonlite::flatten(docs1), jsonlite::flatten(docs2), jsonlite::flatten(docs3))\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\ndim(all_docs)\n\nError in eval(expr, envir, enclos): object 'all_docs' not found\n\n\n\n\n\n\n\n Browse [toddomotos' list of Public APIS](https://github.com/toddmotto/public-apis#science) and [abhishekbanthia's list of Public APIs](https://github.com/abhishekbanthia/Public-APIs). Select one of the APIs from the list. Here are a few criteria you should consider:\n\n * You must use the JSON approach we illustrated above; not all APIs support JSON.^[If you want to use an API that does not support JSON, you can check if there is an `R` wrapper package.]\n * Stay away from APIs that require OAuth for Authorization unless you are prepared for extra work before you get data! Most of the large social APIs (Facebook, LinkedIn, Twitter, etc.) require OAuth. toddomoto's page lists this explicitly, but you'll need to dig a bit if the API is only on abhishekbanthia's list.\n * You will probably need to explore several different APIs before you find one that works well for your interests and this assignment.\n * Beware of the `rate limits` associated with the API you choose. These determine the maximimum number of API calls you can make per second, hour or day. Though these are not always officially published, you can find them by Google (for example) `GitHub API rate limit`. If you need to slow your program down to meet the API insert calls to `Sys.sleep(1)` as is done in the example below.\n * Sketch out one interesting visualization that relies on the public API you selected earlier. Make sure the exact data you need is available. If it's not, try a new visualization or API.\n * If a wrapper package is available, you may use it, but you should also try to create the request URL and retrieve the JSON data using the techniques we showed earlier, without the wrapper package.\n * Visualize the data you collected and describe the results.",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#learning-goals",
    "href": "src/16-API.html#learning-goals",
    "title": "Public Web APIs",
    "section": "",
    "text": "Understand the difference between acquiring data through web scraping vs. a web API\nSet up an API key for a public API\nDevelop comfort in using a wrapper package or url-method of calling a web API\nRecognize the structure in a url for a web API and adjust for your purposes\n\nYou can download a template .Rmd of this activity here.",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#apis",
    "href": "src/16-API.html#apis",
    "title": "Public Web APIs",
    "section": "",
    "text": "In this lesson you’ll learn how to collect data from websites such as The New York Times, Zillow, and Google. While these sites are primarily known for the information they provide to humans browsing the web, they (along with most large websites) also provide information to computer programs.\nHumans use browsers such as Firefox or Chrome to navigate the web. Behind the scenes, our browsers communicate with web servers using a technology called HTTP or Hypertext Transfer Protocol.\nProgramming languages such as R can also use HTTP to communicate with web servers. We have seen how it is possible for R to “scrape” data from almost any static web page. However, it’s easiest to interact with websites that are specifically designed to communicate with programs. These Web APIs, or Web Application Programming Interfaces, focus on transmitting data, rather than images, colors, or other appearance-related information.\nAn large variety of web APIs provide data accessible to programs written in R (and almost any other programming language!). Almost all reasonably large commercial websites offer APIs. Todd Motto has compiled an excellent list of Public Web APIs on GitHub. Browse the list to see what kind of information is available.",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#wrapper-packages",
    "href": "src/16-API.html#wrapper-packages",
    "title": "Public Web APIs",
    "section": "",
    "text": "Possible readings:  1. NY Times API  2. NY Times Blog post announcing the API  3. Working with the NY Times API in R\n4. nytimes pacakge for accessing the NY Times’ APIs from R  5. Video showing how to use the NY Times API  6. rOpenSci has a good collection of wrapper packages\nIn R, it is easiest to use Web APIs through a wrapper package, an R package written specifically for a particular Web API. The R development community has already contributed wrapper packages for most large Web APIs. To find a wrapper package, search the web for “R Package” and the name of the website. For example, a search for “R Reddit Package” returns RedditExtractor and a search for “R Weather.com Package” surfaces weatherData.\nThis activity will build on the New York Times Web API, which provides access to news articles, movie reviews, book reviews, and many other data. Our activity will specifically focus on the Article Search API, which finds information about news articles that contain a particular word or phrase.\nWe will use the nytimes package that provides functions for some (but not all) of the NYTimes APIs. First, install the package by copying the following two lines into your console (you just need to run these once):\ninstall.packages(\"devtools\")\ndevtools::install_github(\"mkearney/nytimes\")\nNext, take a look at the Article Search API example on the package website to get a sense of the syntax.\n\nWhat do you think the nytimes function below does? How does it communicate with the NY Times? Where is the data about articles stored?\n\n\n\nres &lt;- nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\")\n\nTo get started with the NY Times API, you must register and get an authentication key. Signup only takes a few seconds, and it lets the New York Times make sure nobody abuses their API for commercial purposes. It also rate limits their API and ensures programs don’t make too many requests per day. For the NY Times API, this limit is 1000 calls per day. Be aware that most APIs do have rate limits — especially for their free tiers.\nOnce you have signed up, verified your email, log back in to https://developer.nytimes.com. Under your email address, click on Apps and Create a new App (call it First API) and enable Article Search API, then press Save. This creates an authentication key, which is a 32 digit string with numbers and the letters a-e.\nStore this in a variable as follows (this is just an example ID, not an actual one):\n\ntimes_key &lt;- \"c935b213b2dc1218050eec976283dbbd\"\n\n# Tell nytimes what our API key is\nSys.setenv(NYTIMES_KEY = times_key)\n\nNow, let’s use the key to issue our first API call. We’ll adapt the code we see in the vignette to do what we need.\n\nlibrary(nytimes)\n\nError in library(nytimes): there is no package called 'nytimes'\n\n# Issue our first API call\nres &lt;- nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\")\n\nError in nyt_search(q = \"gamergate\", n = 20, end_date = \"20150101\"): could not find function \"nyt_search\"\n\n# Convert response object to data frame\nres &lt;- as.data.frame(res)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\n\nSomething magical just happened. Your computer sent a message to the New York Times and asked for information about 20 articles about Gamergate starting at January 1, 2015 and going backwards in time. Thousands of public Web APIs allow your computer to tap into almost any piece of public digital information on the web.\nLet’s take a peek at the structure of the results. You can also look at the data in the “Environment” tab in one of the windows of RStudio:\n\ncolnames(res)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$web_url)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$headline)\n\nError in eval(expr, envir, enclos): object 'res' not found\n\nhead(res$pub_date)\n\nError in eval(expr, envir, enclos): object 'res' not found",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#accessing-web-apis",
    "href": "src/16-API.html#accessing-web-apis",
    "title": "Public Web APIs",
    "section": "",
    "text": "Wrapper packages such as nytimes provide a convenient way to interact with Web APIs. However, many Web APIs have incomplete wrapper packages, or no wrapper package at all. Fortunately, most Web APIs share a common structure that R can access relatively easily. There are two parts to each Web API: the request, which corresponds to a function call, and the response, which corresponds to the function’s return value.1\nAs mentioned earlier, a Web API call differs from a regular function call in that the request is sent over the Internet to a webserver, which performs the computation and calculates the return result, which is sent back over the Internet to the original computer.\nWeb API Requests\nPossible readings:  1. Understanding URLs  2. urltools Vignette\nThe request for a Web API call is usually encoded through the URL, the web address associated with the API’s webserver. Let’s look at the URL associated with the first nytimes nyt_search example we did. Open the following URL in your browser (you should replace MY_KEY with the api key you were given earlier).\nhttp://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate&api-key=MY_KEY\nThe text you see in the browser is the response data. We’ll talk more about that in a bit. Right now, let’s focus on the structure of the URL. You can see that it has a few parts:\n\nhttp:// — The scheme, which tells your browser or program how to communicate with the webserver. This will typically be either http: or https:.\napi.nytimes.com — The hostname, which is a name that identifies the webserver that will process the request.\n/svc/search/v2/articlesearch.json — The path, which tells the webserver what function you would like to call.\n?q=gamergate&api-key=MY_KEY — The query parameters, which provide the parameters for the function you would like to call. Note that the query can be thought of as a table, where each row has a key and a value (known as a key-value pair). In this case, the first row has key q and value gamergate and the second row has value MY_KEY. The query parameters are preceded by a ?. Rows in the key-value table are separated by ‘&’, and individual key-value pairs are separated by an =.\n\nTypically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (http://api.nytimes.com/svc/search/v2/articlesearch.json) will be referred to as the endpoint for the API call.\nWe will use the urltools module to build up a full URL from its parts. We start by creating a string with the endpoint and then add the parameters one by one using param_set and url_encode:\n\nlibrary(urltools)\n\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"marlon james\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\nurl\n\nCopy and paste the resulting URL into your browser to see what the NY Times response looks like!\n\nYou may be wondering why we need to use `param_set` and `url_encode` instead of writing the full url by hand. This exercise will illustrate why we need to be careful. \n\n a) Repeat the above steps, but create a URL that finds articles related to `Ferris Bueller's Day Off` (note the apostrophe). What is interesting about how the title appears in the URL?\n b) Repeat the steps above for the phrase `Nico & Vinz` (make sure you use the punctuation mark `&`). What do you notice?\n c) Take a look at the Wikipedia page describing [percent encoding](https://en.wikipedia.org/wiki/Percent-encoding). Explain how the process works in your own words.",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#web-api-responses",
    "href": "src/16-API.html#web-api-responses",
    "title": "Public Web APIs",
    "section": "",
    "text": "Possible readings:  1. A Non-Programmer’s Introduction to JSON  2. Getting Started With JSON and jsonlite  3. Fetching JSON data from REST APIs\nWe now discuss the structure of the web response, the return value of the Web API function. Web APIs generate string responses. If you visited the earlier New York Times API link in your browser, you would be shown the string response from the New York Times webserver:\n{\"status\":\"OK\",\"copyright\":\"Copyright (c) 2021 The New York Times Company. All Rights Reserved.\",\"response\":{\"docs\":[{\"abstract\":\"Here’s what you need to know.\",\"web_url\":\"https://www.nytimes.com/2019/08/16/briefing/rashida-tlaib-gamergate-greenland.html\",\"snippet\":\"Here’s what you need to know.\",\"lead_paragraph\":\"(Want to get this briefing by email? Here’s the sign-up.)\",\"source\":\"The New York Times\",\"multimedia\":[{\"rank\":0,\"subtype\":\"xlarge\",\"caption\":null,\"credit\":null,\"type\":\"image\",\"url\":\"images/2019/08/16/world/16US-AMBRIEFING-TLAIB-amcore/merlin_158003643_c67928bc-e547-4a2e-9344-5f0209ca024d-articleLarge.jpg\",\"height\":400,\"width\":600,\"legacy\":{\"xlarge\":\"images/2019/08/16/world/16US-AMBRIEFING-TLAIB-amcore/merlin_158003643_c67928bc-e547-4a2e-9344-5f0209ca024d-articleLarge.jpg\",\"xlargewidth\":600,\"xlargeheight\":400},\"subType\":\"xlarge\",\"crop_name\":\"articleLarge\"},...\nIf you stared very hard at the above response, you may be able to interpret it. However, it would be much easier to interact with the response in some more structured, programmatic way. The vast majority of Web APIs, including the New York Times, use a standard called JSON (Javascript Object Notation) to take data and encode it as a string. To understand the structure of JSON, take the NY Times web response in your browser, and copy and paste it into an online JSON formatter. The formatter will add newlines and tabs to make the data more human interpretable. You’ll see the following:\n{  \n   \"status\":\"OK\",\n   \"copyright\":\"Copyright (c) 2021 The New York Times Company. All Rights Reserved.\",\n   \"response\":{  \n      \"docs\":[  \n      \n        # A HUGE piece of data, with one object for each of the result articles\n        \n      ],\n      \"meta\":{  \n         \"hits\":128,\n         \"offset\":0,\n         \"time\":93\n      }\n   }\n}     \nYou’ll notice a few things in the JSON above:\n\nStrings are enclosed in double quotes, for example \"status\" and \"OK\".\nNumbers are written plainly, like 2350 or 72.\nSome data is enclosed in square brackets [ and ]. These data containers can be thought of as R lists.\nSome data is enclosed in curly braces { and }. These data containers are called Objects. An object can be thought of as a single observation in a table. The columns or variables for the observation appear as keys on the left (hits, offset, etc.). The values appear after the specific key separated by a colon (2350, and 0, respectively). Thus, we can think of the meta object above as:\n\n\n\n\n\n\n\nhits\noffset\ntime\n\n\n\n\n128\n0\n93\n\n\n\n\n\n\n\n\nLet’s repeat the NY Times search for gamergate, but this time we will peform the Web API call by hand instead of using the nytimes wrapper package. We will use the jsonlite package to retrieve the response from the webserver and turn the string response into an R object. The fromJson function sends our request out over and across the web to the NY Times webserver, retrieves it, and turns it from a JSON-formatted string into R data.\n\nlibrary(jsonlite)\n\n# Rebuild the URL\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"gamergate\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\n\nError in eval(expr, envir, enclos): object 'times_key' not found\n\n# Send the request to the webserver over the Internet and\n# retrieve the JSON response. Turn the JSON response into an\n# R Object.\nresponse_js &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate': HTTP\nstatus was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=gamergate'\n\n\nThe jsonlite makes the keys and values of an object available as attributes. For example, we can fetch the status:\n\nresponse_js$status\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\n\nWhile some keys in the object are associated with simple values, such as \"status\", others are associated with more complex data. For example, the key \"response\" is associated with an object that has two keys: \"docs\", and \"meta\". \"meta\" is another object: {  \"hits\":128, \"offset\":0, \"time\":19 }. We can retrieve these nested attributes by sequentially accessing the object keys from the outside in. For example, the inner \"hits\" attribute would be accessed as follows:\n\nresponse_js$response$meta$hits\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\n\n\nRetrieve the data associated with \n\n1) the `copyright` key of the `response_js` object, and \n2) the `time` attribute nested within the `meta` object.\n\n\nThe majority of the data is stored under response, in docs. Notice that docs is a list, where each element of the list is a JSON object that looks like the following:\n {  \n  \"web_url\":\"https://www.nytimes.com/2017/06/27/arts/milkshake-duck-meme.html\",\n  \"snippet\":\"Oxford Dictionaries is keeping a close eye on a term that describes someone who rapidly gains and inevitably loses the internet’s intense love.\",  \n  \"blog\":{  },  \n  \"source\":\"The New York Times\",  \n  \"multimedia\":[    \n      ... A LIST OF OBJECTS ...  \n  ],  \n  \"headline\":{    \n     \"main\":\"How a Joke Becomes a Meme: The Birth of ‘Milkshake Duck’\",  \n     \"print_headline\":\"How a Joke Becomes a Meme: The Birth of ‘Milkshake Duck’\"  \n  },  \n  \"keywords\":[    \n      ... A LIST OF OBJECTS ...  \n  ],  \n  \"pub_date\":\"2017-06-27T12:24:20+0000\",  \n  \"document_type\":\"article\",  \n  \"new_desk\":\"Culture\",  \n  \"byline\":{    \n     \"original\":\"By JONAH ENGEL BROMWICH\"  \n  },  \n  \"type_of_material\":\"News\",  \n  \"_id\":\"59524e7f7c459f257c1ac39f\",  \n  \"word_count\":1033,  \n  \"score\":0.35532707,  \n  \"uri\":\"nyt://article/a3e5bf4a-6216-5dba-9983-73bc45a98e69\"  \n},\njsonlite makes lists of objects available as a data frame, where the columns are the keys in the object (web_url, snippet, etc.)\n\ndocs_df &lt;- response_js$response$docs\n\nError in eval(expr, envir, enclos): object 'response_js' not found\n\nclass(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\ncolnames(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\ndim(docs_df)\n\nError in eval(expr, envir, enclos): object 'docs_df' not found\n\n\n\nConsider the following:\n\n a) Select your own article search query (any topic of interest to you). You may want to play with NY Times online search or the [API web search console](https://developer.nytimes.com/docs/articlesearch-product/1/routes/articlesearch.json/get) to find a query that is interesting, but not overly popular. You can change any part of the query you would like. Your query should have at least 30 matches.\n b) Retrieve data for the first three pages of search results from the article search API, and create a data frame that joins together the `docs` data frames for the three pages of results.  Hint: The example in the section below shows how to get different pages of results and use `bind_rows to combine them.   \n c) Visualize the number of search results per day or month in your result set.\n \n\n\n\nHere is some code to generate queries on NY Times articles about the Red Sox. It fetches the first thirty entries in batches of 10.\n\nurl &lt;- \"http://api.nytimes.com/svc/search/v2/articlesearch.json\"\nurl &lt;- param_set(url, \"q\", url_encode(\"Red Sox\"))\nurl &lt;- param_set(url, \"api-key\", url_encode(times_key))\n\nError in eval(expr, envir, enclos): object 'times_key' not found\n\nurl &lt;- param_set(url, \"page\", 0)\nres1 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=0':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=0'\n\n# This pauses for 1 second.\n# It is required when knitting to prevent R from issuing too many requests to\n# The NY Times API at a time. If you don't have it you will get an error that\n# says \"Too Many Requests (429)\"\nSys.sleep(1)\nurl &lt;- param_set(url, \"page\", 1)\nres2 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=1':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=1'\n\nSys.sleep(1)\nurl &lt;- param_set(url, \"page\", 2)\nres3 &lt;- fromJSON(url)\n\nWarning in open.connection(con, \"rb\"): cannot open URL\n'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=2':\nHTTP status was '401 Unauthorized'\n\n\nError in open.connection(con, \"rb\"): cannot open the connection to 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Red%20Sox&page=2'\n\ndocs1 &lt;- res1$response$docs\n\nError in eval(expr, envir, enclos): object 'res1' not found\n\ndocs2 &lt;- res2$response$docs\n\nError in eval(expr, envir, enclos): object 'res2' not found\n\ndocs3 &lt;- res3$response$docs\n\nError in eval(expr, envir, enclos): object 'res3' not found\n\n\nEach of these docs variables is a table with ten entries (articles) and the same 18 variables:\n\nnames(docs1)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nNow we want to stack the tables on top of each other to get a single table with 30 rows and 18 variables. If you try the following command:\nbind_rows(docs1,docs2,docs3)\nthen you will get an error saying “Error in bind_rows_(x, .id) : Argument 4 can’t be a list containing data frames.”\nWhat is happening???\nLet’s check out the first column of the docs1 table:\n\ndocs1$web_url\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nIt lists the web addresses of the first ten sites returned in the search. It is a vector of ten character strings, which is just fine for one column of data in our table.\nNow let’s check out the headline variable:\n\ndocs1$headline\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nThe headline variable is actually a data frame that contains three variables: main, kicker, and print_headline. That is, we have nested data frames. This is a common problem when scraping data from JSON files, and it is why we are not able to directly bind the rows of our three tables on top of each other.\nWe can check out the type of variable in each column with the class function:\n\nsapply(docs1, class)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\n\nWe see that blog, headline, and byline are the three problem columns that each contain their own data frames.\nThe solution is to flatten these variables, which generates a new column in the outer table for each of the columns in the inner tables.\n\ndocs1_flat &lt;- jsonlite::flatten(docs1)\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\nnames(docs1_flat)\n\nError in eval(expr, envir, enclos): object 'docs1_flat' not found\n\nsapply(docs1_flat, class)\n\nError in eval(expr, envir, enclos): object 'docs1_flat' not found\n\n\nThe headline variable is now replaced with seven separate columns for headline.main, headline.kicker, headline.content_kicker, headline.print_headline, headline.name, headline.seo, and headline.sub. The byline variable is replaced with three separae columns. The blog variable contained an empty data frame, so it has been removed. The overall result is a new flat table with 25 columns, and no more nested data frames.\nOnce the data is flattened, we can bind rows:\n\nall_docs &lt;- bind_rows(jsonlite::flatten(docs1), jsonlite::flatten(docs2), jsonlite::flatten(docs3))\n\nError in eval(expr, envir, enclos): object 'docs1' not found\n\ndim(all_docs)\n\nError in eval(expr, envir, enclos): object 'all_docs' not found",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#additional-practice",
    "href": "src/16-API.html#additional-practice",
    "title": "Public Web APIs",
    "section": "",
    "text": "Browse [toddomotos' list of Public APIS](https://github.com/toddmotto/public-apis#science) and [abhishekbanthia's list of Public APIs](https://github.com/abhishekbanthia/Public-APIs). Select one of the APIs from the list. Here are a few criteria you should consider:\n\n * You must use the JSON approach we illustrated above; not all APIs support JSON.^[If you want to use an API that does not support JSON, you can check if there is an `R` wrapper package.]\n * Stay away from APIs that require OAuth for Authorization unless you are prepared for extra work before you get data! Most of the large social APIs (Facebook, LinkedIn, Twitter, etc.) require OAuth. toddomoto's page lists this explicitly, but you'll need to dig a bit if the API is only on abhishekbanthia's list.\n * You will probably need to explore several different APIs before you find one that works well for your interests and this assignment.\n * Beware of the `rate limits` associated with the API you choose. These determine the maximimum number of API calls you can make per second, hour or day. Though these are not always officially published, you can find them by Google (for example) `GitHub API rate limit`. If you need to slow your program down to meet the API insert calls to `Sys.sleep(1)` as is done in the example below.\n * Sketch out one interesting visualization that relies on the public API you selected earlier. Make sure the exact data you need is available. If it's not, try a new visualization or API.\n * If a wrapper package is available, you may use it, but you should also try to create the request URL and retrieve the JSON data using the techniques we showed earlier, without the wrapper package.\n * Visualize the data you collected and describe the results.",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/16-API.html#footnotes",
    "href": "src/16-API.html#footnotes",
    "title": "Public Web APIs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough we imply that a Web API call corresponds to a single function on the webserver, this is not necessarily the case. Still, we use this language because the analogy fits well.↩︎",
    "crumbs": [
      "Src",
      "Public Web APIs"
    ]
  },
  {
    "objectID": "src/14-Regex.html",
    "href": "src/14-Regex.html",
    "title": "Regular Expressions",
    "section": "",
    "text": "Develop comfort in working with strings of text data\nUse regular expressions to search and replace, detect patterns, locate patterns, extract patterns, and separate text with the stringr package.\n\nCreate a new Rmd file (save it as 14-Regex.Rmd). Put this file in a folder Assignment_09 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!\n\n\n\n\nRegular expressions allow us to describe character patterns. Regular expressions allow us to:1\n\nSearch for particular items within a large body of text. For example, you may wish to identify and extract all email addresses.\nReplace particular items. For example, you may wish to clean up some poorly formatted HTML by replacing all uppercase tags with lowercase equivalents.\nValidate input. For example, you may want to check that a password meets certain criteria such as, a mix of uppercase and lowercase, digits and punctuation.\nCoordinate actions. For example, you may wish to process certain files in a directory, but only if they meet particular conditions.\nReformat text. For example, you may want to split strings into different parts, each to form new variables.\nand more…\n\nStart by doing this interactive tutorial. Note that neither the tutorial nor regular expressions more generally are specific to R.\n\nSome of the syntax in the tutorial is slightly different from what we’ll use in R, but it will still help you get acclimated to the main ideas of regular expressions.\n\n\n\n\nNow that we have some idea how regular expressions work, let’s examine how to use them to achieve various tasks in R. It will be helpful to have your cheat sheet handy.\nNote: Many of these tasks can either be accomplished with functions from the base (built-in) package in R or from the stringr package, which is part of the Tidyverse. In general, the stringr functions are faster, which will be noticeable when processing a large amount of text data.\n\nexample &lt;- \"The quick brown fox jumps over the lazy dog.\"\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\n\n\n\nTo search for a pattern and replace it, we can use the function str_replace and str_replace_all in the stringr package. Note that str_replace only replaces the first matched pattern, while str_replace_all replaces all. Here are some examples:\n\nstr_replace(example, \"quick\", \"really quick\")\n\n[1] \"The really quick brown fox jumps over the lazy dog.\"\n\nstr_replace_all(example, \"(fox|dog)\", \"****\") # | reads as OR\n\n[1] \"The quick brown **** jumps over the lazy ****.\"\n\nstr_replace_all(example, \"(fox|dog).\", \"****\") # \".\" for any character\n\n[1] \"The quick brown ****jumps over the lazy ****\"\n\nstr_replace_all(example, \"(fox|dog)\\\\.$\", \"****\") # at end of sentence only, \"\\\\.\" only for a period\n\n[1] \"The quick brown fox jumps over the lazy ****\"\n\nstr_replace_all(example, \"the\", \"a\") # case-sensitive only matches one\n\n[1] \"The quick brown fox jumps over a lazy dog.\"\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # # will match either t or T; could also make \"a\" conditional on capitalization of t\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # first match only\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\n\n\n\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\nexamples &lt;- c(example, example2, example3)\n\npat &lt;- \"[^aeiouAEIOU ]{3}\" # Regular expression for three straight consonants. Note that I've excluded spaces as well\n\nstr_detect(examples, pat) # TRUE/FALSE if it detects pattern\n\n[1]  TRUE  TRUE FALSE\n\nstr_subset(examples, pat) # Pulls out those that detects pattern\n\n[1] \"The quick brown fox jumps over the lazy dog.\"                                                                                                        \n[2] \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\n\n\n\n\n\n\nstr_locate(example, pat) # starting position and ending position of first match\n\n     start end\n[1,]    23  25\n\n\nLet’s check the answer:\n\nstr_sub(example, 23, 25)\n\n[1] \"mps\"\n\n\n\n\n\n\npat2 &lt;- \"[^aeiouAEIOU ][aeiouAEIOU]{2}[^aeiouAEIOU ]{1}\" # consonant followed by two vowels followed by a consonant\nstr_extract(example2, pat2) # extract first match\n\n[1] \"road\"\n\nstr_extract_all(example2, pat2, simplify = TRUE) # extract all matches\n\n     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  \n[1,] \"road\" \"wood\" \"coul\" \"tood\" \"look\" \"coul\"\n\n\n\n\n\n\nstr_length(example2)\n\n[1] 148\n\n\n\n\n\n\nstr_to_lower(example2)\n\n[1] \"two roads diverged in a yellow wood, / and sorry i could not travel both / and be one traveler, long i stood / and looked down one as far as i could\"\n\n\n\n\n\n\ndf &lt;- tibble(ex = example2)\ndf &lt;- separate(df, ex, c(\"line1\", \"line2\", \"line3\", \"line4\"), sep = \" / \")\ndf$line1\n\n[1] \"Two roads diverged in a yellow wood,\"\n\ndf$line2\n\n[1] \"And sorry I could not travel both\"\n\ndf$line3\n\n[1] \"And be one traveler, long I stood\"\n\ndf$line4\n\n[1] \"And looked down one as far as I could\"\n\n\nNote: The function separate() is in the tidyr package.\n\n\n\n\nThe tibble courses has the Fall 2022 enrollment information from the Macalester Registrar’s website, which we could gain with web scraping tools. See code below if you are interested in trying out web scraping!\n\nfall2022 &lt;- read_html(\"https://www.macalester.edu/registrar/schedules/2022fall/class-schedule\")\n\n# Retrieve and inspect course numbers\ncourse_nums &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-course-number\") %&gt;%\n  html_text()\n\n# Retrieve and inspect course names\ncourse_names &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-course-title\") %&gt;%\n  html_text()\n\ncourse_nums_clean &lt;- stringr::str_sub(course_nums, end = nchar(course_nums) - 6)\n\ncrn &lt;- stringr::str_sub(course_nums, start = nchar(course_nums) - 4)\n\ncourse_instructors &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(6)\") %&gt;%\n  html_text()\ncourse_instructors_short &lt;- stringr::str_sub(trimws(course_instructors), start = 13)\n\ncourse_days &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(3)\") %&gt;%\n  html_text()\ncourse_days_short &lt;- trimws(stringr::str_sub(trimws(course_days), start = 7))\n\ncourse_times &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(4)\") %&gt;%\n  html_text()\ncourse_times_short &lt;- stringr::str_sub(trimws(course_times), start = 7)\n\ncourse_rooms &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(5)\") %&gt;%\n  html_text()\ncourse_rooms_short &lt;- stringr::str_sub(trimws(course_rooms), start = 7)\n\ncourse_avail &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(7)\") %&gt;%\n  html_text()\ncourse_avail_short &lt;- stringr::str_sub(trimws(course_avail), start = 14)\n\nsafe_html &lt;- possibly(.f = read_html,otherwise = NA)\nSITES &lt;- paste0(\"https://webapps.macalester.edu/registrardata/classdata/Fall2022/\", crn) %&gt;%\n  purrr::map(~ posshtml(.x))\n\nsafe_read &lt;- possibly(.f = function(x) html_nodes(x, \"p:nth-child(1)\") %&gt;%\n    html_text() %&gt;%\n    trimws(), otherwise = NA)\ncourse_desc &lt;- SITES %&gt;%\n  purrr::map_chr(~ safe_read(.x))\n\nsafe_read2 &lt;- possibly(.f = function(x) html_nodes(x, \"p:nth-child(2)\") %&gt;%\n    html_text() %&gt;%\n    trimws() %&gt;% stringr::str_sub(start = 32) %&gt;%\n    trimws(), otherwise = NA)\ngen_ed &lt;- SITES %&gt;%\n  purrr::map_chr(~ safe_read2(.x))\n\n\ncourses &lt;-\n  tibble(\n    number = course_nums_clean,\n    crn = crn,\n    name = course_names,\n    days = course_days_short,\n    time = course_times_short,\n    room = course_rooms_short,\n    instructor = course_instructors_short,\n    avail_max = course_avail_short,\n    desc = course_desc,\n    gen_ed = gen_ed\n  )\n\nwrite_csv(courses, file = 'Mac2022Courses.csv')\n\n\n\nError: 'data/Mac2022Courses.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'courses' not found\n\n\nYou can read in the data created from the webscrapping code above from the course site:\n\ncourses &lt;- read_csv('https://bcheggeseth.github.io/112_fall_2023/data/Mac2022Courses.csv')\n\n\nMake the following changes to the `courses` data table (save the updated table as `courses`):\n\na) Split `number` into three separate columns: `dept`, `number`, and `section`.\nb) Split the `avail_max` variable into two separate variables: `avail` and `max`. It might be helpful to first remove all appearances of \"Closed \". \nc) Use `avail` and `max` to generate a new variable called `enrollment`.\nd) Split the `time` variable into two separate columns: `start_time` and `end_time`. Convert all of these times into continuous 24 hour times (e.g., 2:15 pm should become 14.25). *Hint: check out the documentation for the function `parse_date_time()`.*\n\n\nMake a bar plot showing the number of Fall 2022 sections satisfying the Writing WA requirement, sorted by department code.^[For this exercise, you can count cross-listed courses towards both departments' WA counts.] Note: some courses satisfy multiple requirements.\n\n\n\nIn the next series of exercises, we are going to build up an analysis to examine the number of student enrollments for each faculty member.\n\nFor this particular analysis, we do not want to consider certain types of sections. Remove all of the following from the data table (save subset dataset as `courses2`):\n\na) All sections in `PE` or `INTD`.\nb) All music ensembles and dance practicum sections (these are all of the MUSI and THDA classes with numbers less than 100).\nc) All lab sections. This is one is a bit tricky. You can search for \"Lab\" or \"Laboratory\", but be careful not to eliminate courses with words such as \"Labor\". Some of these have section numbers that end in \"-L1\", for example.\n\n\n\nSome sections are listed under multiple different departments (cross-listed courses), and you will find the same instructor, day, time, etc. Note: they may have different course numbers. \n\nFor this activity, we only want to include each actual section once and it doesn't really matter which department code we associate with this section. Eliminate all duplicated cross-listed courses from `courses2`, keeping each actual section just once (save updated table as `courses3`). Hint: look into the `R` function `distinct`, and think carefully about how to find duplicates.\n\n\n\nUsing `courses3` (i.e. after removing non 4-credit courses and cross-listed duplicates), make a table with all Fall 2022 co-taught courses (i.e., more than one instructor). You don't need to save the new table. Hint: There was a class in Fall 2022 called Land/Water that was co-taught. Look for patterns in the data that could let you distinguish which courses were co-taught.\n\n\n\nUsing `courses3` (i.e. after removing non 4-credit courses and cross-listed duplicates), make a table where each row contains a faculty, the number of sections they are teaching in Fall 2022, and the total enrollments in those section. Sort the table from highest total enrollments to lowest. You should find two current Comp/Stat 112 instructors in the top 3.^[For the purposes of this exercise, we are just going to leave co-taught courses as is so that you will have an extra row for each pair or triplet of instructors. Alternatives would be to allocate the enrollment number to each of the faculty members or to split it up between the members. The first option would usually be the most appropriate, although it might depend on the course.]\n\n\n\nCreate and display a new table with all night courses (i.e., a subset of `courses3`). Also make a bar plot showing the number of these courses by day of the week.",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/14-Regex.html#learning-goals",
    "href": "src/14-Regex.html#learning-goals",
    "title": "Regular Expressions",
    "section": "",
    "text": "Develop comfort in working with strings of text data\nUse regular expressions to search and replace, detect patterns, locate patterns, extract patterns, and separate text with the stringr package.\n\nCreate a new Rmd file (save it as 14-Regex.Rmd). Put this file in a folder Assignment_09 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/14-Regex.html#regular-expressions-and-character-strings",
    "href": "src/14-Regex.html#regular-expressions-and-character-strings",
    "title": "Regular Expressions",
    "section": "",
    "text": "Regular expressions allow us to describe character patterns. Regular expressions allow us to:1\n\nSearch for particular items within a large body of text. For example, you may wish to identify and extract all email addresses.\nReplace particular items. For example, you may wish to clean up some poorly formatted HTML by replacing all uppercase tags with lowercase equivalents.\nValidate input. For example, you may want to check that a password meets certain criteria such as, a mix of uppercase and lowercase, digits and punctuation.\nCoordinate actions. For example, you may wish to process certain files in a directory, but only if they meet particular conditions.\nReformat text. For example, you may want to split strings into different parts, each to form new variables.\nand more…\n\nStart by doing this interactive tutorial. Note that neither the tutorial nor regular expressions more generally are specific to R.\n\nSome of the syntax in the tutorial is slightly different from what we’ll use in R, but it will still help you get acclimated to the main ideas of regular expressions.",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/14-Regex.html#wrangling-with-regular-expressions-in-r",
    "href": "src/14-Regex.html#wrangling-with-regular-expressions-in-r",
    "title": "Regular Expressions",
    "section": "",
    "text": "Now that we have some idea how regular expressions work, let’s examine how to use them to achieve various tasks in R. It will be helpful to have your cheat sheet handy.\nNote: Many of these tasks can either be accomplished with functions from the base (built-in) package in R or from the stringr package, which is part of the Tidyverse. In general, the stringr functions are faster, which will be noticeable when processing a large amount of text data.\n\nexample &lt;- \"The quick brown fox jumps over the lazy dog.\"\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\n\n\n\nTo search for a pattern and replace it, we can use the function str_replace and str_replace_all in the stringr package. Note that str_replace only replaces the first matched pattern, while str_replace_all replaces all. Here are some examples:\n\nstr_replace(example, \"quick\", \"really quick\")\n\n[1] \"The really quick brown fox jumps over the lazy dog.\"\n\nstr_replace_all(example, \"(fox|dog)\", \"****\") # | reads as OR\n\n[1] \"The quick brown **** jumps over the lazy ****.\"\n\nstr_replace_all(example, \"(fox|dog).\", \"****\") # \".\" for any character\n\n[1] \"The quick brown ****jumps over the lazy ****\"\n\nstr_replace_all(example, \"(fox|dog)\\\\.$\", \"****\") # at end of sentence only, \"\\\\.\" only for a period\n\n[1] \"The quick brown fox jumps over the lazy ****\"\n\nstr_replace_all(example, \"the\", \"a\") # case-sensitive only matches one\n\n[1] \"The quick brown fox jumps over a lazy dog.\"\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # # will match either t or T; could also make \"a\" conditional on capitalization of t\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # first match only\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\n\n\n\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\nexamples &lt;- c(example, example2, example3)\n\npat &lt;- \"[^aeiouAEIOU ]{3}\" # Regular expression for three straight consonants. Note that I've excluded spaces as well\n\nstr_detect(examples, pat) # TRUE/FALSE if it detects pattern\n\n[1]  TRUE  TRUE FALSE\n\nstr_subset(examples, pat) # Pulls out those that detects pattern\n\n[1] \"The quick brown fox jumps over the lazy dog.\"                                                                                                        \n[2] \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\n\n\n\n\n\n\nstr_locate(example, pat) # starting position and ending position of first match\n\n     start end\n[1,]    23  25\n\n\nLet’s check the answer:\n\nstr_sub(example, 23, 25)\n\n[1] \"mps\"\n\n\n\n\n\n\npat2 &lt;- \"[^aeiouAEIOU ][aeiouAEIOU]{2}[^aeiouAEIOU ]{1}\" # consonant followed by two vowels followed by a consonant\nstr_extract(example2, pat2) # extract first match\n\n[1] \"road\"\n\nstr_extract_all(example2, pat2, simplify = TRUE) # extract all matches\n\n     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  \n[1,] \"road\" \"wood\" \"coul\" \"tood\" \"look\" \"coul\"\n\n\n\n\n\n\nstr_length(example2)\n\n[1] 148\n\n\n\n\n\n\nstr_to_lower(example2)\n\n[1] \"two roads diverged in a yellow wood, / and sorry i could not travel both / and be one traveler, long i stood / and looked down one as far as i could\"\n\n\n\n\n\n\ndf &lt;- tibble(ex = example2)\ndf &lt;- separate(df, ex, c(\"line1\", \"line2\", \"line3\", \"line4\"), sep = \" / \")\ndf$line1\n\n[1] \"Two roads diverged in a yellow wood,\"\n\ndf$line2\n\n[1] \"And sorry I could not travel both\"\n\ndf$line3\n\n[1] \"And be one traveler, long I stood\"\n\ndf$line4\n\n[1] \"And looked down one as far as I could\"\n\n\nNote: The function separate() is in the tidyr package.",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/14-Regex.html#practice-fall-2022-enrollment-exploration",
    "href": "src/14-Regex.html#practice-fall-2022-enrollment-exploration",
    "title": "Regular Expressions",
    "section": "",
    "text": "The tibble courses has the Fall 2022 enrollment information from the Macalester Registrar’s website, which we could gain with web scraping tools. See code below if you are interested in trying out web scraping!\n\nfall2022 &lt;- read_html(\"https://www.macalester.edu/registrar/schedules/2022fall/class-schedule\")\n\n# Retrieve and inspect course numbers\ncourse_nums &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-course-number\") %&gt;%\n  html_text()\n\n# Retrieve and inspect course names\ncourse_names &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-course-title\") %&gt;%\n  html_text()\n\ncourse_nums_clean &lt;- stringr::str_sub(course_nums, end = nchar(course_nums) - 6)\n\ncrn &lt;- stringr::str_sub(course_nums, start = nchar(course_nums) - 4)\n\ncourse_instructors &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(6)\") %&gt;%\n  html_text()\ncourse_instructors_short &lt;- stringr::str_sub(trimws(course_instructors), start = 13)\n\ncourse_days &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(3)\") %&gt;%\n  html_text()\ncourse_days_short &lt;- trimws(stringr::str_sub(trimws(course_days), start = 7))\n\ncourse_times &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(4)\") %&gt;%\n  html_text()\ncourse_times_short &lt;- stringr::str_sub(trimws(course_times), start = 7)\n\ncourse_rooms &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(5)\") %&gt;%\n  html_text()\ncourse_rooms_short &lt;- stringr::str_sub(trimws(course_rooms), start = 7)\n\ncourse_avail &lt;-\n  fall2022 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(7)\") %&gt;%\n  html_text()\ncourse_avail_short &lt;- stringr::str_sub(trimws(course_avail), start = 14)\n\nsafe_html &lt;- possibly(.f = read_html,otherwise = NA)\nSITES &lt;- paste0(\"https://webapps.macalester.edu/registrardata/classdata/Fall2022/\", crn) %&gt;%\n  purrr::map(~ posshtml(.x))\n\nsafe_read &lt;- possibly(.f = function(x) html_nodes(x, \"p:nth-child(1)\") %&gt;%\n    html_text() %&gt;%\n    trimws(), otherwise = NA)\ncourse_desc &lt;- SITES %&gt;%\n  purrr::map_chr(~ safe_read(.x))\n\nsafe_read2 &lt;- possibly(.f = function(x) html_nodes(x, \"p:nth-child(2)\") %&gt;%\n    html_text() %&gt;%\n    trimws() %&gt;% stringr::str_sub(start = 32) %&gt;%\n    trimws(), otherwise = NA)\ngen_ed &lt;- SITES %&gt;%\n  purrr::map_chr(~ safe_read2(.x))\n\n\ncourses &lt;-\n  tibble(\n    number = course_nums_clean,\n    crn = crn,\n    name = course_names,\n    days = course_days_short,\n    time = course_times_short,\n    room = course_rooms_short,\n    instructor = course_instructors_short,\n    avail_max = course_avail_short,\n    desc = course_desc,\n    gen_ed = gen_ed\n  )\n\nwrite_csv(courses, file = 'Mac2022Courses.csv')\n\n\n\nError: 'data/Mac2022Courses.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'courses' not found\n\n\nYou can read in the data created from the webscrapping code above from the course site:\n\ncourses &lt;- read_csv('https://bcheggeseth.github.io/112_fall_2023/data/Mac2022Courses.csv')\n\n\nMake the following changes to the `courses` data table (save the updated table as `courses`):\n\na) Split `number` into three separate columns: `dept`, `number`, and `section`.\nb) Split the `avail_max` variable into two separate variables: `avail` and `max`. It might be helpful to first remove all appearances of \"Closed \". \nc) Use `avail` and `max` to generate a new variable called `enrollment`.\nd) Split the `time` variable into two separate columns: `start_time` and `end_time`. Convert all of these times into continuous 24 hour times (e.g., 2:15 pm should become 14.25). *Hint: check out the documentation for the function `parse_date_time()`.*\n\n\nMake a bar plot showing the number of Fall 2022 sections satisfying the Writing WA requirement, sorted by department code.^[For this exercise, you can count cross-listed courses towards both departments' WA counts.] Note: some courses satisfy multiple requirements.\n\n\n\nIn the next series of exercises, we are going to build up an analysis to examine the number of student enrollments for each faculty member.\n\nFor this particular analysis, we do not want to consider certain types of sections. Remove all of the following from the data table (save subset dataset as `courses2`):\n\na) All sections in `PE` or `INTD`.\nb) All music ensembles and dance practicum sections (these are all of the MUSI and THDA classes with numbers less than 100).\nc) All lab sections. This is one is a bit tricky. You can search for \"Lab\" or \"Laboratory\", but be careful not to eliminate courses with words such as \"Labor\". Some of these have section numbers that end in \"-L1\", for example.\n\n\n\nSome sections are listed under multiple different departments (cross-listed courses), and you will find the same instructor, day, time, etc. Note: they may have different course numbers. \n\nFor this activity, we only want to include each actual section once and it doesn't really matter which department code we associate with this section. Eliminate all duplicated cross-listed courses from `courses2`, keeping each actual section just once (save updated table as `courses3`). Hint: look into the `R` function `distinct`, and think carefully about how to find duplicates.\n\n\n\nUsing `courses3` (i.e. after removing non 4-credit courses and cross-listed duplicates), make a table with all Fall 2022 co-taught courses (i.e., more than one instructor). You don't need to save the new table. Hint: There was a class in Fall 2022 called Land/Water that was co-taught. Look for patterns in the data that could let you distinguish which courses were co-taught.\n\n\n\nUsing `courses3` (i.e. after removing non 4-credit courses and cross-listed duplicates), make a table where each row contains a faculty, the number of sections they are teaching in Fall 2022, and the total enrollments in those section. Sort the table from highest total enrollments to lowest. You should find two current Comp/Stat 112 instructors in the top 3.^[For the purposes of this exercise, we are just going to leave co-taught courses as is so that you will have an extra row for each pair or triplet of instructors. Alternatives would be to allocate the enrollment number to each of the faculty members or to split it up between the members. The first option would usually be the most appropriate, although it might depend on the course.]\n\n\n\nCreate and display a new table with all night courses (i.e., a subset of `courses3`). Also make a bar plot showing the number of these courses by day of the week.",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/14-Regex.html#footnotes",
    "href": "src/14-Regex.html#footnotes",
    "title": "Regular Expressions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: regular expression tutorial.↩︎",
    "crumbs": [
      "Src",
      "Regular Expressions"
    ]
  },
  {
    "objectID": "src/12-Data_Import.html#learning-goals",
    "href": "src/12-Data_Import.html#learning-goals",
    "title": "(PART) Data Acquisition & Cleaning",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nDevelop comfort in finding an existing data set to import into R\nDevelop comfort in importing data of a variety of file types into R\nUnderstand and implement the data cleaning process to make values consistent\nUnderstand the implications of different ways of dealing with missing values with replace_na and drop_na\n\nCreate a new Rmd file (save it as 12-Data Import.Rmd). Put this file in a folder Assignment_08 in your COMP_STAT_112 folder. - Make sure to add alt text to visualizations using fig.alt!",
    "crumbs": [
      "Src",
      "(PART) Data Acquisition & Cleaning"
    ]
  },
  {
    "objectID": "src/12-Data_Import.html#finding-importing-and-cleaning-data",
    "href": "src/12-Data_Import.html#finding-importing-and-cleaning-data",
    "title": "(PART) Data Acquisition & Cleaning",
    "section": "Finding, Importing, and Cleaning Data",
    "text": "Finding, Importing, and Cleaning Data\nAdditional resources and readings:\n1. Data Import Cheat Sheet\n2. readr documentation\n3. Data import from Wickham and Grolemund  4. Missing data from Wickham and Grolemund  5. Data intake from Baumer, Kaplan, and Horton\n6. Using the import wizard from Prof. Lendway\nIn practice, data science is not as glamorous as building fancy classifiers and creating visualizations all the time. Data scientists spend 80% of their time acquiring and cleaning data. While the skill of data acquisition is best learned through experience, this section of the course will outline the most common approaches to acquiring data.\nWhen importing and cleaning a dataset, take careful notes in your R Markdown. Explain where you found the dataset (the source). Record the steps you took to clean and import the data in case somebody else needs to replicate your analysis. You should also make sure to cite and credit the creator of the dataset.\n\nFinding Existing Data Sets\n\n\nError in knitr::include_graphics(\"images/csv_search.jpeg\"): Cannot find the file(s): \"images/csv_search.jpeg\"\n\n\nThe easiest way to get data is by finding an existing dataset that has been created by somebody else. Search engines such as Google can be excellent tools, especially when using file type filters. For example, if you are looking for a dataset about movie reviews, you might search for “movie reviews filetype:csv”. You could also try searching for other common filetypes that are compatible with R, such as .tsv, .xls, .xlsx, or .rds.\nAnother good resource for datasets are compendiums of datasets such as the excellent and continuously-evolving awesome-public-datasets GitHub repo, Kaggle datasets or the data.world website website. You can find links to other similar compendiums at the end of the awesome-public-datasets page.\n\n\nSaving Datasets Locally\nOnce you’ve found a dataset you are interested in, you need to download the file and save it to a location on your computer.\nThe best location to put a dataset is within a folder that is dedicated to the project or assignment. For example, you’ve created a folder for Assignment_08 and any datasets you use for this assignment should be saved in that folder.\nQuick Note: One key idea when you are working with datasets on your local machine is that you need to:\n\nknow where the files are located\nknow how to tell the computer program where the files are located (a file path)\n\nThere are two common ways we refer to file locations: absolute file path and relative file path.\nAbsolute file path\n\nAn absolute file path describes the location of a file from the root directory or folder, typically the user directory.\n\nOn a Mac, ~ refers to the user root directory.\nOn a Windows, the root directory is typically C:\\\n\nExample: A file called data.csv is located in the Assignment_08 folder in Comp_Stat_112 folder on the Desktop\n\nOn a Mac, the absolute file path is ~/Desktop/Comp_Stat_112/Assignment_08/data.csv\nOn a Windows, the absolute file path is C:/Desktop/Comp_Stat_112/Assignment_08/data.csv\n*Windows Note: Switch from backslash to forward slashes in R or you need to use \\ instead of *\n\n\nRelative file path\n\nA relative file path describes the location of a file from the current working directory (or in the case of an Rmd, the location of the Rmd file).\n\nWhen working within an Rmd file, it will first look for files relative to the location of the Rmd file. Therefore, it is good practice to put the data file in the same folder as the Rmd file you are working on.\nIf you are working in the Console, you can change the working directory (Session &gt; Set Working Directory).\n\nExample: A file called data.csv is located in a data folder within Comp_Stat_112 folder on the Desktop\n\nIf the working directory is ~/Desktop/Comp_Stat_112/Assignment_08/, the relative path is ../data/data.csv. The .. refers to the parent directory (go up one level to the folder containing Assignment_08).\nIf the working directory is ~/Desktop/Comp_Stat_112/, the relative path is data/data.csv.\nIf the working directory is ~/Desktop/Comp_Stat_112/data, the relative path is data.csv.\n\n\n\n\nLoading Datasets\nOnce you have a dataset, it’s time to load it into R. Don’t be frustrated if this step takes some time.\nThe table below lists some common data import functions and when you would use them.\n\n\n\n\n\n\n\nFunction\nUse when\n\n\n\n\nread_csv()\ndata are saved in .csv (comma delimited or comma separated values) format - you can save Excel files and Google Sheets in this format\n\n\nread_delim()\ndata are saved in other delimited formats (tab, space, etc.)\n\n\nread_sheet()\ndata are in a Google Sheet\n\n\nst_read()\nreading in a shapefile\n\n\n\nA few tips:\n\nWhen reading in data from a file, one tip is to initially use the Import Wizard to help write the code and file path. DO NOT use it to import the data as you will need the code to read in the data in order to knit your document. Check out a video tutorial on the Import Wizard\n\nThe import functions read_csv, read_csv2, and read_tsv from the readr package are faster than their counterparts read.csv, read.csv2, and read.tsv from the base package for large files. They also have more flexible parsers (e.g., for dates, times, percentages). We recommend you use these functions instead of the base functions like read.csv. The package fread has other import functions and is also faster for large datasets. For smaller data sets (say 1MB or less), there won’t be that much difference in time for the three different packages.\nread_csv2 is for semi-colon delimited files, whereas read_csv is for comma delimited files.\nThe readr functions automatically guess the type of data in each column (e.g., character, double, integer). You will often see a message just after the import telling you what it chose for each column. If you think there is an issue, you can use the function problems() to detect problems, and/or specify how the columns should be imported. See the section on “column specification” in the Data Import Cheat Sheet for more info.\nIf you have trouble importing a dataset, try to first import it into a different data such as Google Sheets or Excel tool and then export it as a TSV or CSV before reading it into R.\nFor really messy data, OpenRefine is complicated but powerful (YouTube demo).\nWhen you are importing a large file, you might want to first try importing a subset of the data. For example, if you want to take the first 17 rows only, you can write read_csv(\"file.csv\",n_max=17)\nSimilarly, you might want to skip the first \\(n\\) lines of the file when importing, select only certain columns to read in, or choose a random subset of the rows. See the cheat sheet for instructions on these tasks or just google!\n\n\n\nChecking the Imported Datasets\nAfter reading in new data, it is ALWAYS a good idea to do some quick checks of the data. Here are two first steps that are especially useful:\n\nOpen the data in the spreadsheet-like viewer with View(dataset_name) and take a look at it. Sort it by different variables by clicking on the arrows next to the variable name. Make sure there isn’t anything unexpected.\nDo a quick summary of the data. The code below is one way to do this. For quantitative variables, it provides summary statistics and will let you know if there are missing values. For factors (they need to be factors, not just character variables - the mutate() changes them to factors), it shows you counts for the top categories and tells you if there are any missing values.\n\ndataset_name %&gt;% \n  mutate(across(where(is.character), as.factor)) %&gt;% \n  summary()\n\n\nCleaning Datasets\nCleaning Categorical Variables\nFirst we want to make sure the factors are “clean”, meaning consistent values in the correct format. For example, true and TRUE and T will be three different factors. The easiest way to manage this is to look at the levels for the factor and replace values with a messy factor to a clean one. For example, the following code cleans up values in true/false values in column X in a data set called df:\ndf &lt;- df %&gt;% mutate(X = fct_recode(X, \"TRUE\" = \"T\", \"TRUE\" = \"true\", \"FALSE\" = \"f\", \"FALSE\" = \"N\", \"FALSE\" = \"No\"))\n\nWe will use a slightly \"messied\" [version](data/imdb_5000_messy.csv) of the [IMDB 5000 Dataset](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset), collected by chuansun76 on Kaggle.^[Another option for part (e) would be to leave them as strings and then use string processing to define the levels. We'll learn this technique soon.]\n\n\nDownload the csv file of the IMDB 5000 dataset from “https://bcheggeseth.github.io/112_spring_2023/data/imdb_5000_messy.csv” (right-click, save file as), put it in your Assignment_08 folder, use read_csv to load it into RStudio, and save it as imdbMessy in R.\n\n\n\nSolution\n\n\nimdbMessy &lt;- read_csv(\"imdb_5000_messy.csv\") # Relative Path: If your Rmd file and csv file are in the same folder\n\n\n  b. Print out the variable names.\n\n\nSolution\n\n\nnames(imdbMessy) #order = order in dataset\n\n [1] \"...1\"                      \"color\"                    \n [3] \"director_name\"             \"num_critic_for_reviews\"   \n [5] \"duration\"                  \"director_facebook_likes\"  \n [7] \"actor_3_facebook_likes\"    \"actor_2_name\"             \n [9] \"actor_1_facebook_likes\"    \"gross\"                    \n[11] \"genres\"                    \"actor_1_name\"             \n[13] \"movie_title\"               \"num_voted_users\"          \n[15] \"cast_total_facebook_likes\" \"actor_3_name\"             \n[17] \"facenumber_in_poster\"      \"plot_keywords\"            \n[19] \"movie_imdb_link\"           \"num_user_for_reviews\"     \n[21] \"language\"                  \"country\"                  \n[23] \"content_rating\"            \"budget\"                   \n[25] \"title_year\"                \"actor_2_facebook_likes\"   \n[27] \"imdb_score\"                \"aspect_ratio\"             \n[29] \"movie_facebook_likes\"     \n\nls(imdbMessy) #order = alphabetical order\n\n [1] \"actor_1_facebook_likes\"    \"actor_1_name\"             \n [3] \"actor_2_facebook_likes\"    \"actor_2_name\"             \n [5] \"actor_3_facebook_likes\"    \"actor_3_name\"             \n [7] \"aspect_ratio\"              \"budget\"                   \n [9] \"cast_total_facebook_likes\" \"color\"                    \n[11] \"content_rating\"            \"country\"                  \n[13] \"director_facebook_likes\"   \"director_name\"            \n[15] \"duration\"                  \"facenumber_in_poster\"     \n[17] \"genres\"                    \"gross\"                    \n[19] \"imdb_score\"                \"language\"                 \n[21] \"movie_facebook_likes\"      \"movie_imdb_link\"          \n[23] \"movie_title\"               \"num_critic_for_reviews\"   \n[25] \"num_user_for_reviews\"      \"num_voted_users\"          \n[27] \"plot_keywords\"             \"title_year\"               \n\n\n\n  c. Examine the color variable. What are the existing values?\n\n\nSolution\n\n\nimdbMessy %&gt;% select(color) %&gt;% head()\n\n# A tibble: 6 × 1\n  color\n  &lt;chr&gt;\n1 Color\n2 Color\n3 Color\n4 Color\n5 &lt;NA&gt; \n6 Color\n\nlevels(factor(imdbMessy$color))\n\n[1] \"B&W\"             \"Black and White\" \"color\"           \"Color\"          \n[5] \"COLOR\"          \n\nunique(imdbMessy$color)\n\n[1] \"Color\"           NA                \"Black and White\" \"COLOR\"          \n[5] \"color\"           \"B&W\"            \n\n\n\n\nHow often does each color occur? Hint: table or count (which is a short hand for a group_by/summarize(n=n()))\n\n\n\nSolution\n\n\nimdbMessy %&gt;% count(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\ntable(imdbMessy$color)\n\n\n            B&W Black and White           color           Color           COLOR \n             10             199              30            4755              30 \n\n\n\n\nThe read_csv read in the color values as strings. For this exercise, let’s convert them to factor using the code: imdbMessy &lt;- imdbMessy %&gt;% mutate(color = factor(color)).\n\n\n\nSolution\n\n\nimdbMessy &lt;- imdbMessy %&gt;% mutate(color = factor(color))\n\n\n\nSelect what you think is the best value for each level and replace “messy” versions of the value with clean ones with the fct_recode function as shown above. How many entries are there for each level now?\n\n\n\nSolution\n\n\nimdbMessy &lt;- imdbMessy %&gt;% mutate(color = fct_recode(color, \"B&W\" = \"Black and White\", \"Color\" = \"color\", \"Color\" = \"COLOR\"))\nimdbMessy %&gt;% count(color)\n\n# A tibble: 3 × 2\n  color     n\n  &lt;fct&gt; &lt;int&gt;\n1 B&W     209\n2 Color  4815\n3 &lt;NA&gt;     19\n\n\n\nAddressing Missing Data\nFinally, you should look for and address missing data, encoded as NA (not available) in R. There is no single formula for dealing with NAs. You should first look to see how many NAs appear in each column:\n\ncolSums(is.na(imdbMessy))\n\nStudy the individual observations with NAs carefully. Why do you think they are missing? Are certain types of observations more likely to have NAs?\nYou have several options for dealing with NAs (and they have different consequences):\n\nYou can remove observations with one or more NAs (see drop_na).\nYou can remove columns with many NA values.\nYou can replace NAs with a reasonable value (called imputing values). This could be a default value (like zero), or the average for a column. (see replace_na)\nYou can use packages such as missForest that fill in missing values with statistical predictions.1\n\nThere is no perfect approach to dealing with NAs, and you must think carefully about how removing or replacing missing data may affect your work.\n\nConsider `imdbMessy`.\n\n\nPrint out the number of NAs in each of the columns.\n\n\n\nSolution\n\n\ncolSums(is.na(imdbMessy))\n\n                     ...1                     color             director_name \n                        0                        19                       104 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                       50                        15                       104 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                       23                        13                         7 \n                    gross                    genres              actor_1_name \n                      884                         0                         7 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                       23                        13                       153 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                        21                        12 \n                  country            content_rating                    budget \n                        5                       303                       492 \n               title_year    actor_2_facebook_likes                imdb_score \n                      108                        13                         0 \n             aspect_ratio      movie_facebook_likes \n                      329                         0 \n\n\n\n\nConsider the actor_1_facebook_likes column. Take a look at a few of the records that have NA values. Why do you think there are NAs?\n\n\n\nSolution\n\nThis variable is missing if actor_1_name is missing, which suggests that this movie doesn’t have an actor 1 listed.\n\nimdbMessy %&gt;% filter(is.na(actor_1_facebook_likes)) %&gt;% head()\n\n# A tibble: 6 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1  4503 Color Léa Pool                              23       97\n2  4520 Color Harry Gantz                           12      105\n3  4721 Color U. Roberto Romano                      3       80\n4  4838 Color Pan Nalin                             15      102\n5  4946 Color Amal Al-Agroobi                       NA       62\n6  4947 Color Andrew Berends                        12       90\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\n\n\nCreate a new dataframe that removes observations that have NAs for actor_1_facebook_likes.\n\n\n\nSolution\n\n\nimdbMessysub &lt;- imdbMessy %&gt;% filter(!is.na(actor_1_facebook_likes))  #Notice how I saved this new smaller dataset to a new name\n\n\n\nCreate a second new data frame that replaces NAs in actor_1_facebook_likes with 0.\n\n\n\nSolution\n\n\nimdbMessysub2 &lt;- imdbMessy %&gt;% mutate(actor_1_facebook_likes = replace_na(actor_1_facebook_likes,0))",
    "crumbs": [
      "Src",
      "(PART) Data Acquisition & Cleaning"
    ]
  },
  {
    "objectID": "src/12-Data_Import.html#additional-practice",
    "href": "src/12-Data_Import.html#additional-practice",
    "title": "(PART) Data Acquisition & Cleaning",
    "section": "Additional Practice",
    "text": "Additional Practice\n\nFind a dataset that is not built into `R` and is related to one of the following topics:  \n  \n\n\nA personal hobby or passion\nYour hometown, or a place you have lived\n\nLoad the data into R, make sure it is clean, and construct one interesting visualization of the data and include alt text.",
    "crumbs": [
      "Src",
      "(PART) Data Acquisition & Cleaning"
    ]
  },
  {
    "objectID": "src/12-Data_Import.html#footnotes",
    "href": "src/12-Data_Import.html#footnotes",
    "title": "(PART) Data Acquisition & Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is dangerous unless you know what you are doing.↩︎",
    "crumbs": [
      "Src",
      "(PART) Data Acquisition & Cleaning"
    ]
  },
  {
    "objectID": "src/10-Factors.html",
    "href": "src/10-Factors.html",
    "title": "Categorical Variables as Factors",
    "section": "",
    "text": "Understand the difference between a variable stored as a character vs. a factor\nBe able to convert a character variable to a factor\nDevelop comfort in manipulating the order and values of a factor with the forcats package to improve summaries and visualizations.\n\nCreate a new Rmd file (save it as 10-Factors.Rmd). Put this file in a folder Assignment_06 in your COMP_STAT_112 folder.\n\nYou are used to using template Rmd files but you are ready to create your own!\n\nLook at previous Rmd files and notice patterns. You can copy the top YAML section over and adjust.\nNote that if you use {exercise}, you can copy the text of the exercise and that it will automatically number your exercises! Continue to use {r} for R chunks.\nMake sure to add fig.alt for your visualizations!\n\nFrom now on, you’ll be expected to create your own Rmd files and make them look clean and organized so that preceptors and instructors can easily read the knitted file and give feedback.\n\n\n\n\n\n\nError: 'data/grades.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\nGrades &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/grades.csv\")\nGrades &lt;- Grades %&gt;%\n  select(sid, sessionID, grade) %&gt;%\n  distinct(sid, sessionID, .keep_all = TRUE)\n\nWe will continue with the grades data from the previous activity. Here is a sample to remember what it looks like:\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nHere is a bar chart of the grade distribution:\n\nggplot(Grades, aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nWe can also wrangle a table that just has each grade and the number of times it appears:\n\nGradeDistribution &lt;- Grades %&gt;%\n  group_by(grade) %&gt;%\n  summarize(count = n())\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n# Alternatively, we can use the count() function the creates a variable called n\nGrades %&gt;%\n  count(grade) \n\n\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\nWhat could be improved about this graphic and table?\nThe grades are listed alphabetically, which isn’t particularly meaningful. Why are they listed that way? Because the variable grade is a character string type:\n\nclass(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nWhen dealing with categorical variables that take a finite number of values (levels, formally), it is often useful to store the variable as a factor, and specify a meaningful order for the levels.\nFor example, when the entries are stored as character strings, we cannot use the levels command to see the full list of values:\n\nlevels(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nLet’s first convert the grade variable to a factor:\n\nGrades &lt;- Grades %&gt;%\n  mutate(grade = factor(grade))\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nNow we can see the levels:\n\nlevels(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nMoreover, the forcats package (part of tidyverse) allows us to manipulate these factors. Its commands include the following.\n\n\n\n\nfct_relevel(): manually reorder levels\n\nfct_infreq(): order levels from highest to lowest frequency\n\nfct_reorder(): reorder levels by values of another variable\n\nfct_rev(): reverse the current order\n\n\n\n\n\nfct_recode(): manually change levels\n\nfct_lump(): group together least common levels\n\nMore details on these and other commands can be found on the forcats cheat sheet or in Wickham & Grolemund’s chapter on factors.\n\n\n\nLet's reorder the grades so that they are in a more meaningful order for the bar chart above. Here are three options:\n   \n\n\n\nOption 1: From high grade to low grade, with “S” and “AU” at the end:\n\nGrades %&gt;%\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) %&gt;%\n  ggplot(aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nOption 2: In terms of ascending frequency:\n\nggplot(GradeDistribution) +\n  geom_col(aes(x = fct_reorder(grade, count), y = count)) +\n  labs(x = \"grade\")\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\nOption 3: In terms of descending frequency:\n\nggplot(GradeDistribution) +\n  geom_col(aes(x = fct_reorder(grade, count, .desc = TRUE), y = count)) +\n  labs(x = \"grade\")\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\n\nBecause it may not be clear what \"AU\" and \"S\" stand for, let's rename them to \"Audit\" and \"Satisfactory\".   \n\n\nGrades %&gt;%\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) %&gt;%\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) %&gt;%\n  ggplot(aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\nNow that you've developed your data visualization and wrangling skills, \n\n\ndevelop a research question to address with the grades and courses data,\ncreate a high quality visualization that addresses your research question,\nwrite a brief description of the visualization and include the insight you gain about the research question.\n\nCourses &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/courses.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nfct_relevel()\nmanually reorder levels of a factor\nGrades %&gt;% mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\")))\n\n\nfct_infreq()\norder levels from highest to lowest frequency\nggplot(Grades) + geom_bar(aes(x = fct_infreq(grade)))\n\n\nfct_reorder()\nreorder levels by values of another variable\nggplot(GradeDistribution) + geom_col(aes(x = fct_reorder(grade, count), y = count))\n\n\nfct_rev()\nreverse the current order\nggplot(Grades) + geom_bar(aes(x = fct_rev(fct_infreq(grade))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nfct_recode()\nmanually change levels\nGrades %&gt;%  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\"))\n\n\nfct_lump()\ngroup together least common levels\nGrades %&gt;% mutate(grade = fct_lump(grade, n = 5))",
    "crumbs": [
      "Src",
      "Categorical Variables as Factors"
    ]
  },
  {
    "objectID": "src/10-Factors.html#learning-goals",
    "href": "src/10-Factors.html#learning-goals",
    "title": "Categorical Variables as Factors",
    "section": "",
    "text": "Understand the difference between a variable stored as a character vs. a factor\nBe able to convert a character variable to a factor\nDevelop comfort in manipulating the order and values of a factor with the forcats package to improve summaries and visualizations.\n\nCreate a new Rmd file (save it as 10-Factors.Rmd). Put this file in a folder Assignment_06 in your COMP_STAT_112 folder.\n\nYou are used to using template Rmd files but you are ready to create your own!\n\nLook at previous Rmd files and notice patterns. You can copy the top YAML section over and adjust.\nNote that if you use {exercise}, you can copy the text of the exercise and that it will automatically number your exercises! Continue to use {r} for R chunks.\nMake sure to add fig.alt for your visualizations!\n\nFrom now on, you’ll be expected to create your own Rmd files and make them look clean and organized so that preceptors and instructors can easily read the knitted file and give feedback.",
    "crumbs": [
      "Src",
      "Categorical Variables as Factors"
    ]
  },
  {
    "objectID": "src/10-Factors.html#example-grade-distribution",
    "href": "src/10-Factors.html#example-grade-distribution",
    "title": "Categorical Variables as Factors",
    "section": "",
    "text": "Error: 'data/grades.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\nGrades &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/grades.csv\")\nGrades &lt;- Grades %&gt;%\n  select(sid, sessionID, grade) %&gt;%\n  distinct(sid, sessionID, .keep_all = TRUE)\n\nWe will continue with the grades data from the previous activity. Here is a sample to remember what it looks like:\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nHere is a bar chart of the grade distribution:\n\nggplot(Grades, aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nWe can also wrangle a table that just has each grade and the number of times it appears:\n\nGradeDistribution &lt;- Grades %&gt;%\n  group_by(grade) %&gt;%\n  summarize(count = n())\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n# Alternatively, we can use the count() function the creates a variable called n\nGrades %&gt;%\n  count(grade) \n\n\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\nWhat could be improved about this graphic and table?\nThe grades are listed alphabetically, which isn’t particularly meaningful. Why are they listed that way? Because the variable grade is a character string type:\n\nclass(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nWhen dealing with categorical variables that take a finite number of values (levels, formally), it is often useful to store the variable as a factor, and specify a meaningful order for the levels.\nFor example, when the entries are stored as character strings, we cannot use the levels command to see the full list of values:\n\nlevels(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nLet’s first convert the grade variable to a factor:\n\nGrades &lt;- Grades %&gt;%\n  mutate(grade = factor(grade))\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nNow we can see the levels:\n\nlevels(Grades$grade)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nMoreover, the forcats package (part of tidyverse) allows us to manipulate these factors. Its commands include the following.\n\n\n\n\nfct_relevel(): manually reorder levels\n\nfct_infreq(): order levels from highest to lowest frequency\n\nfct_reorder(): reorder levels by values of another variable\n\nfct_rev(): reverse the current order\n\n\n\n\n\nfct_recode(): manually change levels\n\nfct_lump(): group together least common levels\n\nMore details on these and other commands can be found on the forcats cheat sheet or in Wickham & Grolemund’s chapter on factors.\n\n\n\nLet's reorder the grades so that they are in a more meaningful order for the bar chart above. Here are three options:\n   \n\n\n\nOption 1: From high grade to low grade, with “S” and “AU” at the end:\n\nGrades %&gt;%\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) %&gt;%\n  ggplot(aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nOption 2: In terms of ascending frequency:\n\nggplot(GradeDistribution) +\n  geom_col(aes(x = fct_reorder(grade, count), y = count)) +\n  labs(x = \"grade\")\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\nOption 3: In terms of descending frequency:\n\nggplot(GradeDistribution) +\n  geom_col(aes(x = fct_reorder(grade, count, .desc = TRUE), y = count)) +\n  labs(x = \"grade\")\n\nError in eval(expr, envir, enclos): object 'GradeDistribution' not found\n\n\n\nBecause it may not be clear what \"AU\" and \"S\" stand for, let's rename them to \"Audit\" and \"Satisfactory\".   \n\n\nGrades %&gt;%\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) %&gt;%\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) %&gt;%\n  ggplot(aes(x = grade)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\nNow that you've developed your data visualization and wrangling skills, \n\n\ndevelop a research question to address with the grades and courses data,\ncreate a high quality visualization that addresses your research question,\nwrite a brief description of the visualization and include the insight you gain about the research question.\n\nCourses &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/courses.csv\")",
    "crumbs": [
      "Src",
      "Categorical Variables as Factors"
    ]
  },
  {
    "objectID": "src/10-Factors.html#appendix-r-functions",
    "href": "src/10-Factors.html#appendix-r-functions",
    "title": "Categorical Variables as Factors",
    "section": "",
    "text": "Function/Operator\nAction\nExample\n\n\n\n\nfct_relevel()\nmanually reorder levels of a factor\nGrades %&gt;% mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\")))\n\n\nfct_infreq()\norder levels from highest to lowest frequency\nggplot(Grades) + geom_bar(aes(x = fct_infreq(grade)))\n\n\nfct_reorder()\nreorder levels by values of another variable\nggplot(GradeDistribution) + geom_col(aes(x = fct_reorder(grade, count), y = count))\n\n\nfct_rev()\nreverse the current order\nggplot(Grades) + geom_bar(aes(x = fct_rev(fct_infreq(grade))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nfct_recode()\nmanually change levels\nGrades %&gt;%  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\"))\n\n\nfct_lump()\ngroup together least common levels\nGrades %&gt;% mutate(grade = fct_lump(grade, n = 5))",
    "crumbs": [
      "Src",
      "Categorical Variables as Factors"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html",
    "href": "src/08-Reshaping_Data.html",
    "title": "Reshaping Data",
    "section": "",
    "text": "Understand the difference between wide and long data format and distinguish the case (unit of observation) for a given data set\nDevelop comfort in using pivot_wider and pivot_longer in the tidyr package\n\nYou can download a template .Rmd of this activity here. Put the file in the existing folder Assignment_06 in your COMP_STAT_112 folder.\n\n\n\nAdditional reading:\n\nWickham and Grolemund on pivoting, or\nBaumer, Kaplan, and Horton on reshaping data\n\nAs we are transforming data, it is important to keep in mind what constitutes each case (row) of the data. For example, in the initial babynames data below, each case is a single name-sex-year combination. So if we have the same name and sex assigned at birth but a different year, that would be a different case.\n\n\n\nEach case is one name-sex-year combination.\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1880\nF\nMary\n7065\n0.0723836\n\n\n1880\nF\nAnna\n2604\n0.0266790\n\n\n1880\nF\nEmma\n2003\n0.0205215\n\n\n1880\nF\nElizabeth\n1939\n0.0198658\n\n\n1880\nF\nMinnie\n1746\n0.0178884\n\n\n1880\nF\nMargaret\n1578\n0.0161672\n\n\n\n\n\nIt is often necessary to rearrange your data in order to create visualizations, run statistical analysis, etc. We have already seen some ways to rearrange the data to change the unit of observation (also known as the case). For example, what is the case after performing the following command?\n\nbabynamesTotal &lt;- babynames %&gt;%\n  group_by(name, sex) %&gt;%\n  summarise(total = sum(n))\n\nEach case now represents one name-sex combination:\n\n\n\nLong format where each case is one name-sex combination.\n\n\nname\nsex\ntotal\n\n\n\n\nAaban\nM\n107\n\n\nAabha\nF\n35\n\n\nAabid\nM\n10\n\n\nAabir\nM\n5\n\n\nAabriella\nF\n32\n\n\nAada\nF\n5\n\n\n\n\n\nIn this activity, we are going to learn two new operations to reshape and reorganize the data: pivot_wider() and pivot_longer().\n\n\n\nWe want to find the common names that are the most gender neutral (used roughly equally for males and females). How should we rearrange the data? \n\nWell, one nice way would be to have a single row for each name, and then have separate variables for the number of times that name is used for males and females. Using these two columns, we can then compute a third column that gives the ratio between these two columns. That is, we’d like to transform the data into a wide format with each of the possible values of the sex variable becoming its own column. The operation we need to perform this transformation is pivot_wider().\nThe inputs for this function are:\n\nnames_from (the original variable sex in this case) that identifies the variable in the initial long format data whose values should become the names of the new variables in the wide format data.\nnames_sort = TRUE dictates that the variables are listed in alphabetical order; when it is FALSE, they are listed in order of first appearance.\nvalues_from (total in this case) representing the variable to be divided into multiple new variables,\nvalues_fill = 0 specifies that if there are, e.g., no females named Aadam, we should include a zero in the corresponding entry of the wide format table\n\n\n\nError in knitr::include_graphics(\"images/pivot_wider.png\"): Cannot find the file(s): \"images/pivot_wider.png\"\n\n\n\nBabyWide &lt;- babynamesTotal %&gt;%\n  pivot_wider(names_from = sex, values_from = total, values_fill = 0, names_sort = TRUE)\n\n\n\n\nA wide format with one case per name enables us to examine gender balance.\n\n\nname\nF\nM\n\n\n\n\nAaban\n0\n107\n\n\nAabha\n35\n0\n\n\nAabid\n0\n10\n\n\nAabir\n0\n5\n\n\nAabriella\n32\n0\n\n\nAada\n5\n0\n\n\n\n\n\nNow we can choose common names with frequency greater than 25,000 for both males and females, and sort by the ratio to identify gender-neutral names.\n\nNeutral &lt;- BabyWide %&gt;%\n  filter(M &gt; 25000, F &gt; 25000) %&gt;%\n  mutate(ratio = pmin(M / F, F / M)) %&gt;% #pmin() stands for parallel min, finds the min(M/F, F/M) within each row\n  arrange(desc(ratio))\n\n\n\n\nThe most gender-neutral common names, in wide format.\n\n\nname\nF\nM\nratio\n\n\n\n\nKerry\n48534\n49596\n0.9785870\n\n\nRiley\n100881\n92789\n0.9197867\n\n\nQuinn\n28283\n31230\n0.9056356\n\n\nJackie\n90604\n78405\n0.8653591\n\n\nFrankie\n33236\n40552\n0.8195897\n\n\nJaime\n49673\n67582\n0.7350034\n\n\nCasey\n76020\n110165\n0.6900558\n\n\nPeyton\n69256\n47682\n0.6884891\n\n\nPat\n40123\n26731\n0.6662264\n\n\nJessie\n167010\n110027\n0.6588049\n\n\nKendall\n58026\n33821\n0.5828594\n\n\nJody\n55691\n31206\n0.5603419\n\n\n\n\n\n\n\n\nNext, let’s filter these names to keep only those with a ratio of 0.5 or greater (no more than 2 to 1), and then switch back to long format. We can do this with the following pivot_longer() operation. It gathers the columns listed c(F,M) under the cols argument into a single column whose name is given by the names_to argument (“sex”), and includes the values in a column called total, which is the input to the values_to argument.\n\n\nError in knitr::include_graphics(\"images/pivot_longer.png\"): Cannot find the file(s): \"images/pivot_longer.png\"\n\n\n\nNeutralLong &lt;- Neutral %&gt;%\n  filter(ratio &gt;= .5) %&gt;%\n  pivot_longer(cols = c(`F`, `M`), names_to = \"sex\", values_to = \"total\") %&gt;%\n  select(name, sex, total) %&gt;%\n  arrange(name)\n\n\n\n\nLong format for the most gender-neutral common names.\n\n\nname\nsex\ntotal\n\n\n\n\nCasey\nF\n76020\n\n\nCasey\nM\n110165\n\n\nFrankie\nF\n33236\n\n\nFrankie\nM\n40552\n\n\nJackie\nF\n90604\n\n\nJackie\nM\n78405\n\n\nJaime\nF\n49673\n\n\nJaime\nM\n67582\n\n\nJessie\nF\n167010\n\n\nJessie\nM\n110027\n\n\nJody\nF\n55691\n\n\nJody\nM\n31206\n\n\nKendall\nF\n58026\n\n\nKendall\nM\n33821\n\n\nKerry\nF\n48534\n\n\nKerry\nM\n49596\n\n\nPat\nF\n40123\n\n\nPat\nM\n26731\n\n\nPeyton\nF\n69256\n\n\nPeyton\nM\n47682\n\n\nQuinn\nF\n28283\n\n\nQuinn\nM\n31230\n\n\nRiley\nF\n100881\n\n\nRiley\nM\n92789\n\n\n\n\n\n\n\n\n\nThe data associated with this article is available in the fivethirtyeight package, and is loaded into Daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show. Note that when multiple people appeared together, each person receives their own line.\n\nDaily &lt;- daily_show_guests\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ngoogle_knowledge_occupation\nshow\ngroup\nraw_guest_list\n\n\n\n\n1999\nsinger\n1999-07-26\nMusician\nDonny Osmond\n\n\n1999\nactress\n1999-07-27\nActing\nWendie Malick\n\n\n1999\nvocalist\n1999-07-28\nMusician\nVince Neil\n\n\n1999\nfilm actress\n1999-07-29\nActing\nJaneane Garofalo\n\n\n1999\ncomedian\n1999-08-10\nComedy\nDom Irrera\n\n\n1999\nactor\n1999-08-11\nActing\nPierce Brosnan\n\n\n1999\ndirector\n1999-08-12\nMedia\nEduardo Sanchez and Daniel Myrick\n\n\n1999\nfilm director\n1999-08-12\nMedia\nEduardo Sanchez and Daniel Myrick\n\n\n1999\namerican television personality\n1999-08-16\nMedia\nCarson Daly\n\n\n1999\nactress\n1999-08-17\nActing\nMolly Ringwald\n\n\n1999\nactress\n1999-08-18\nActing\nSarah Jessica Parker\n\n\n\n\n\n\n\n\nCreate the following table containing 19 columns. The first column should have the ten guests with the highest number of total apperances on the show, listed in descending order of number of appearances. The next 17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column). The final column should show the total number of appearances for the corresponding guest over the entire duration of the show (these entries should be in decreasing order). Hint: the function `rowSums()` adds up all of the entries in each row of a table. Try using it in a `mutate()`.\n\n\n\n\n\nThe original data has 18 different entries for the group variable:\n\nunique(Daily$group)\n\n [1] \"Acting\"         \"Comedy\"         \"Musician\"       \"Media\"         \n [5] NA               \"Politician\"     \"Athletics\"      \"Business\"      \n [9] \"Advocacy\"       \"Political Aide\" \"Misc\"           \"Academic\"      \n[13] \"Government\"     \"media\"          \"Clergy\"         \"Science\"       \n[17] \"Consultant\"     \"Military\"      \n\n\nIn order to help you recreate the first figure from the article, I have added a new variable with three broader groups: (i) entertainment; (ii) politics, business, and government, and (iii) commentators. The data is available here. We will learn in the next activity what the inner_join in this code chunk is doing.\n\nDailyGroups &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/daily-group-assignment.csv\")\nDaily &lt;- Daily %&gt;%\n  inner_join(DailyGroups, by = c(\"group\" = \"group\"))\n\n\n\nError: 'data/daily-group-assignment.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: object 'DailyGroups' not found\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ngoogle_knowledge_occupation\nshow\ngroup\nraw_guest_list\n\n\n\n\n1999\nactor\n1999-01-11\nActing\nMichael J. Fox\n\n\n1999\ncomedian\n1999-01-12\nComedy\nSandra Bernhard\n\n\n1999\ntelevision actress\n1999-01-13\nActing\nTracey Ullman\n\n\n1999\nfilm actress\n1999-01-14\nActing\nGillian Anderson\n\n\n1999\nactor\n1999-01-18\nActing\nDavid Alan Grier\n\n\n1999\nactor\n1999-01-19\nActing\nWilliam Baldwin\n\n\n\n\n\n\nUsing the group assignments contained in the `broad_group` variable, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Hint: first think about what your case should be for the glyph-ready form.\n\n\n\n\n\n\nA typical situation that requires a pivot_longer command is when the columns represent the possible values of a variable. Table @ref(tab:lesotho-table) shows example data set from opendataforafrica.org with different years in different columns. You can find the data here.\n\nLesotho &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/Lesotho.csv\")\n\n\n\nError: 'data/Lesotho.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\n\nError in eval(expr, envir, enclos): object 'Lesotho' not found\n\n\n\nMake a side-by-side bar chart with the `year` on the horizontal axis, and three side-by-side vertical columns for average interest rate on deposits, average interest rate on loans, and inflation rate for each year. In order to get the data into glyph-ready form, you'll need to use `pivot_longer`. Hint: `pivot_longer` uses the `dplyr::select()` notation, so you can, e.g., list the columns you want to select, use colon notation, or use `starts_with(a string)`. See [Wickham and Grolemund](https://r4ds.had.co.nz/transform.html#select) for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\npivot_wider()\nTakes a long data set and spreads information in columns into many new variables (wider)\nbabynamesTotal %&gt;% pivot_wider(names_from = sex, values_from = total, values_fill = 0, names_sort = TRUE)\n\n\npivot_longer()\nTakes a wide data set and gathers information in columns into fewer variables (longer)\nNeutral %&gt;% pivot_longer(cols = c(F,M), names_to = \"sex\", values_to = \"total\")",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html#learning-goals",
    "href": "src/08-Reshaping_Data.html#learning-goals",
    "title": "Reshaping Data",
    "section": "",
    "text": "Understand the difference between wide and long data format and distinguish the case (unit of observation) for a given data set\nDevelop comfort in using pivot_wider and pivot_longer in the tidyr package\n\nYou can download a template .Rmd of this activity here. Put the file in the existing folder Assignment_06 in your COMP_STAT_112 folder.",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html#wide-and-long-data-formats",
    "href": "src/08-Reshaping_Data.html#wide-and-long-data-formats",
    "title": "Reshaping Data",
    "section": "",
    "text": "Additional reading:\n\nWickham and Grolemund on pivoting, or\nBaumer, Kaplan, and Horton on reshaping data\n\nAs we are transforming data, it is important to keep in mind what constitutes each case (row) of the data. For example, in the initial babynames data below, each case is a single name-sex-year combination. So if we have the same name and sex assigned at birth but a different year, that would be a different case.\n\n\n\nEach case is one name-sex-year combination.\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1880\nF\nMary\n7065\n0.0723836\n\n\n1880\nF\nAnna\n2604\n0.0266790\n\n\n1880\nF\nEmma\n2003\n0.0205215\n\n\n1880\nF\nElizabeth\n1939\n0.0198658\n\n\n1880\nF\nMinnie\n1746\n0.0178884\n\n\n1880\nF\nMargaret\n1578\n0.0161672\n\n\n\n\n\nIt is often necessary to rearrange your data in order to create visualizations, run statistical analysis, etc. We have already seen some ways to rearrange the data to change the unit of observation (also known as the case). For example, what is the case after performing the following command?\n\nbabynamesTotal &lt;- babynames %&gt;%\n  group_by(name, sex) %&gt;%\n  summarise(total = sum(n))\n\nEach case now represents one name-sex combination:\n\n\n\nLong format where each case is one name-sex combination.\n\n\nname\nsex\ntotal\n\n\n\n\nAaban\nM\n107\n\n\nAabha\nF\n35\n\n\nAabid\nM\n10\n\n\nAabir\nM\n5\n\n\nAabriella\nF\n32\n\n\nAada\nF\n5\n\n\n\n\n\nIn this activity, we are going to learn two new operations to reshape and reorganize the data: pivot_wider() and pivot_longer().\n\n\n\nWe want to find the common names that are the most gender neutral (used roughly equally for males and females). How should we rearrange the data? \n\nWell, one nice way would be to have a single row for each name, and then have separate variables for the number of times that name is used for males and females. Using these two columns, we can then compute a third column that gives the ratio between these two columns. That is, we’d like to transform the data into a wide format with each of the possible values of the sex variable becoming its own column. The operation we need to perform this transformation is pivot_wider().\nThe inputs for this function are:\n\nnames_from (the original variable sex in this case) that identifies the variable in the initial long format data whose values should become the names of the new variables in the wide format data.\nnames_sort = TRUE dictates that the variables are listed in alphabetical order; when it is FALSE, they are listed in order of first appearance.\nvalues_from (total in this case) representing the variable to be divided into multiple new variables,\nvalues_fill = 0 specifies that if there are, e.g., no females named Aadam, we should include a zero in the corresponding entry of the wide format table\n\n\n\nError in knitr::include_graphics(\"images/pivot_wider.png\"): Cannot find the file(s): \"images/pivot_wider.png\"\n\n\n\nBabyWide &lt;- babynamesTotal %&gt;%\n  pivot_wider(names_from = sex, values_from = total, values_fill = 0, names_sort = TRUE)\n\n\n\n\nA wide format with one case per name enables us to examine gender balance.\n\n\nname\nF\nM\n\n\n\n\nAaban\n0\n107\n\n\nAabha\n35\n0\n\n\nAabid\n0\n10\n\n\nAabir\n0\n5\n\n\nAabriella\n32\n0\n\n\nAada\n5\n0\n\n\n\n\n\nNow we can choose common names with frequency greater than 25,000 for both males and females, and sort by the ratio to identify gender-neutral names.\n\nNeutral &lt;- BabyWide %&gt;%\n  filter(M &gt; 25000, F &gt; 25000) %&gt;%\n  mutate(ratio = pmin(M / F, F / M)) %&gt;% #pmin() stands for parallel min, finds the min(M/F, F/M) within each row\n  arrange(desc(ratio))\n\n\n\n\nThe most gender-neutral common names, in wide format.\n\n\nname\nF\nM\nratio\n\n\n\n\nKerry\n48534\n49596\n0.9785870\n\n\nRiley\n100881\n92789\n0.9197867\n\n\nQuinn\n28283\n31230\n0.9056356\n\n\nJackie\n90604\n78405\n0.8653591\n\n\nFrankie\n33236\n40552\n0.8195897\n\n\nJaime\n49673\n67582\n0.7350034\n\n\nCasey\n76020\n110165\n0.6900558\n\n\nPeyton\n69256\n47682\n0.6884891\n\n\nPat\n40123\n26731\n0.6662264\n\n\nJessie\n167010\n110027\n0.6588049\n\n\nKendall\n58026\n33821\n0.5828594\n\n\nJody\n55691\n31206\n0.5603419\n\n\n\n\n\n\n\n\nNext, let’s filter these names to keep only those with a ratio of 0.5 or greater (no more than 2 to 1), and then switch back to long format. We can do this with the following pivot_longer() operation. It gathers the columns listed c(F,M) under the cols argument into a single column whose name is given by the names_to argument (“sex”), and includes the values in a column called total, which is the input to the values_to argument.\n\n\nError in knitr::include_graphics(\"images/pivot_longer.png\"): Cannot find the file(s): \"images/pivot_longer.png\"\n\n\n\nNeutralLong &lt;- Neutral %&gt;%\n  filter(ratio &gt;= .5) %&gt;%\n  pivot_longer(cols = c(`F`, `M`), names_to = \"sex\", values_to = \"total\") %&gt;%\n  select(name, sex, total) %&gt;%\n  arrange(name)\n\n\n\n\nLong format for the most gender-neutral common names.\n\n\nname\nsex\ntotal\n\n\n\n\nCasey\nF\n76020\n\n\nCasey\nM\n110165\n\n\nFrankie\nF\n33236\n\n\nFrankie\nM\n40552\n\n\nJackie\nF\n90604\n\n\nJackie\nM\n78405\n\n\nJaime\nF\n49673\n\n\nJaime\nM\n67582\n\n\nJessie\nF\n167010\n\n\nJessie\nM\n110027\n\n\nJody\nF\n55691\n\n\nJody\nM\n31206\n\n\nKendall\nF\n58026\n\n\nKendall\nM\n33821\n\n\nKerry\nF\n48534\n\n\nKerry\nM\n49596\n\n\nPat\nF\n40123\n\n\nPat\nM\n26731\n\n\nPeyton\nF\n69256\n\n\nPeyton\nM\n47682\n\n\nQuinn\nF\n28283\n\n\nQuinn\nM\n31230\n\n\nRiley\nF\n100881\n\n\nRiley\nM\n92789",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html#example-the-daily-show-guests",
    "href": "src/08-Reshaping_Data.html#example-the-daily-show-guests",
    "title": "Reshaping Data",
    "section": "",
    "text": "The data associated with this article is available in the fivethirtyeight package, and is loaded into Daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show. Note that when multiple people appeared together, each person receives their own line.\n\nDaily &lt;- daily_show_guests\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ngoogle_knowledge_occupation\nshow\ngroup\nraw_guest_list\n\n\n\n\n1999\nsinger\n1999-07-26\nMusician\nDonny Osmond\n\n\n1999\nactress\n1999-07-27\nActing\nWendie Malick\n\n\n1999\nvocalist\n1999-07-28\nMusician\nVince Neil\n\n\n1999\nfilm actress\n1999-07-29\nActing\nJaneane Garofalo\n\n\n1999\ncomedian\n1999-08-10\nComedy\nDom Irrera\n\n\n1999\nactor\n1999-08-11\nActing\nPierce Brosnan\n\n\n1999\ndirector\n1999-08-12\nMedia\nEduardo Sanchez and Daniel Myrick\n\n\n1999\nfilm director\n1999-08-12\nMedia\nEduardo Sanchez and Daniel Myrick\n\n\n1999\namerican television personality\n1999-08-16\nMedia\nCarson Daly\n\n\n1999\nactress\n1999-08-17\nActing\nMolly Ringwald\n\n\n1999\nactress\n1999-08-18\nActing\nSarah Jessica Parker\n\n\n\n\n\n\n\n\nCreate the following table containing 19 columns. The first column should have the ten guests with the highest number of total apperances on the show, listed in descending order of number of appearances. The next 17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column). The final column should show the total number of appearances for the corresponding guest over the entire duration of the show (these entries should be in decreasing order). Hint: the function `rowSums()` adds up all of the entries in each row of a table. Try using it in a `mutate()`.\n\n\n\n\n\nThe original data has 18 different entries for the group variable:\n\nunique(Daily$group)\n\n [1] \"Acting\"         \"Comedy\"         \"Musician\"       \"Media\"         \n [5] NA               \"Politician\"     \"Athletics\"      \"Business\"      \n [9] \"Advocacy\"       \"Political Aide\" \"Misc\"           \"Academic\"      \n[13] \"Government\"     \"media\"          \"Clergy\"         \"Science\"       \n[17] \"Consultant\"     \"Military\"      \n\n\nIn order to help you recreate the first figure from the article, I have added a new variable with three broader groups: (i) entertainment; (ii) politics, business, and government, and (iii) commentators. The data is available here. We will learn in the next activity what the inner_join in this code chunk is doing.\n\nDailyGroups &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/daily-group-assignment.csv\")\nDaily &lt;- Daily %&gt;%\n  inner_join(DailyGroups, by = c(\"group\" = \"group\"))\n\n\n\nError: 'data/daily-group-assignment.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: object 'DailyGroups' not found\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ngoogle_knowledge_occupation\nshow\ngroup\nraw_guest_list\n\n\n\n\n1999\nactor\n1999-01-11\nActing\nMichael J. Fox\n\n\n1999\ncomedian\n1999-01-12\nComedy\nSandra Bernhard\n\n\n1999\ntelevision actress\n1999-01-13\nActing\nTracey Ullman\n\n\n1999\nfilm actress\n1999-01-14\nActing\nGillian Anderson\n\n\n1999\nactor\n1999-01-18\nActing\nDavid Alan Grier\n\n\n1999\nactor\n1999-01-19\nActing\nWilliam Baldwin\n\n\n\n\n\n\nUsing the group assignments contained in the `broad_group` variable, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Hint: first think about what your case should be for the glyph-ready form.",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html#more-practice-pivoting-longer",
    "href": "src/08-Reshaping_Data.html#more-practice-pivoting-longer",
    "title": "Reshaping Data",
    "section": "",
    "text": "A typical situation that requires a pivot_longer command is when the columns represent the possible values of a variable. Table @ref(tab:lesotho-table) shows example data set from opendataforafrica.org with different years in different columns. You can find the data here.\n\nLesotho &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/Lesotho.csv\")\n\n\n\nError: 'data/Lesotho.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\n\nError in eval(expr, envir, enclos): object 'Lesotho' not found\n\n\n\nMake a side-by-side bar chart with the `year` on the horizontal axis, and three side-by-side vertical columns for average interest rate on deposits, average interest rate on loans, and inflation rate for each year. In order to get the data into glyph-ready form, you'll need to use `pivot_longer`. Hint: `pivot_longer` uses the `dplyr::select()` notation, so you can, e.g., list the columns you want to select, use colon notation, or use `starts_with(a string)`. See [Wickham and Grolemund](https://r4ds.had.co.nz/transform.html#select) for more information.",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/08-Reshaping_Data.html#appendix-r-functions",
    "href": "src/08-Reshaping_Data.html#appendix-r-functions",
    "title": "Reshaping Data",
    "section": "",
    "text": "Function/Operator\nAction\nExample\n\n\n\n\npivot_wider()\nTakes a long data set and spreads information in columns into many new variables (wider)\nbabynamesTotal %&gt;% pivot_wider(names_from = sex, values_from = total, values_fill = 0, names_sort = TRUE)\n\n\npivot_longer()\nTakes a wide data set and gathers information in columns into fewer variables (longer)\nNeutral %&gt;% pivot_longer(cols = c(F,M), names_to = \"sex\", values_to = \"total\")",
    "crumbs": [
      "Src",
      "Reshaping Data"
    ]
  },
  {
    "objectID": "src/05-Multivariate_Viz.html",
    "href": "src/05-Multivariate_Viz.html",
    "title": "Multivariate Visualizations",
    "section": "",
    "text": "Understand how we can use additional aesthetics such as color and size to incorporate a third (or more variables) to a bivariate plot\nDevelop comfort with interpreting heat maps and star plots, which allow you to look for patterns in variation in many variables.\n\nYou can download a template .Rmd of this activity here. Put this in a new folder called Assignment_04 in your folder for COMP_STAT_112.\n\n\n\n\n\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education data contain various education variables for each state:\n\n\nWarning in file(file, \"rt\"): cannot open file 'data/sat.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\neducation &lt;- read.csv(\"https://bcheggeseth.github.io/112_spring_2023/data/sat.csv\")\n\n\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\n\nError in knitr::include_graphics(\"images/SATcodebook.png\"): Cannot find the file(s): \"images/SATcodebook.png\"\n\n\nTo examine the variability in average SAT scores from state to state, let’s start with a univariate density plot:\n\nggplot(education, aes(x = sat)) +\n  geom_density(fill = \"blue\", alpha = .5) + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\nThe first question we’d like to answer is to what degree do per pupil spending (expend) and teacher salary explain this variability? We can start by plotting each against sat, along with a best fit linear regression model:\n\nggplot(education, aes(y = sat, x = salary)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\nggplot(education, aes(y = sat, x = expend)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\nIs there anything that surprises you in the above plots? What are the relationship trends?\n\n\n\nSolution\n\nThese seem to suggest that spending more money on students or teacher salaries correlates with lower SAT scores. Say it ain’t so!\n\n\n\n\nMake a single scatterplot visualization that demonstrates the relationship between `sat`, `salary`, and `expend`. Summarize the trivariate relationship between `sat`, `salary`, and `expend`. \n\nHints: 1. Try using the color or size aesthetics to incorporate the expenditure data. 2. Include some model smooths with geom_smooth() to help highlight the trends.\n\n\nSolution\n\nBelow are four different plots that one could make. There seems to be a high correlation between expend and salary, and both seem to be negatively correlated with sat.\n\n#plot 1\ng1 &lt;- ggplot(education, aes(y=sat, x=salary, color=expend)) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 2\ng2 &lt;- ggplot(education, aes(y=sat, x=salary, size=expend)) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 3\ng3 &lt;- ggplot(education, aes(y=sat, x=salary, color=cut(expend,2))) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 4\ng4 &lt;- ggplot(education, aes(y=sat, x=salary, color=cut(expend,3))) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\nlibrary(gridExtra)\ngrid.arrange(g1, g2, g3, g4, ncol=2)\n\nError in eval(expr, envir, enclos): object 'g1' not found\n\n\n\n\n\n\nThe `fracCat` variable in the `education` data categorizes the fraction of the state's students that take the SAT into `low` (below 15%), `medium` (15-45%), and `high` (at least 45%).\n\n\nMake a univariate visualization of the fracCat variable to better understand how many states fall into each category.\n\nMake a bivariate visualization that demonstrates the relationship between fracCat and sat. What story does your graphic tell?\nMake a trivariate visualization that demonstrates the relationship between fracCat, sat, and expend. Incorporate fracCat as the color of each point, and use a single call to geom_smooth to add three trendlines (one for each fracCat). What story does your graphic tell?\n\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why does it appear that SAT scores decrease as spending increases even though the opposite is true?\n\n\n\nSolution\n\n\n\n\n\nggplot(education, aes(x = fracCat)) +\n  geom_bar() + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\n\n\n\nggplot(education, aes(x = fracCat, y = sat)) +\n  geom_boxplot() + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\n\n\n\nggplot(education, aes(color = fracCat, y = sat, x = expend)) +\n  geom_point() + geom_smooth(se = FALSE, method = 'lm') + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\nAmong student participation tends to be lower among states with lower expenditures. Those same states tend to have higher SAT scores because of the self-selection of who participates; only those most prepared take the SAT in those states.\n\n\n\n\n\n\n\n\n\n\nNote that each variable (column) is scaled to indicate states (rows) with high values (yellow) to low values (purple/blue). With this in mind you can scan across rows & across columns to visually assess which states & variables are related, respectively. You can also play with the color scheme. Type ?cm.colors in the console to see various options.\n\ned &lt;- as.data.frame(education) # convert from tibble to data frame\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n# convert to a matrix with State names as the row names\nrow.names(ed) &lt;- ed$State  #added state names as the row names rather than a variable\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\ned &lt;- ed %&gt;% select(2:8) #select the 2nd through 8th columns\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\ned_mat &lt;- data.matrix(ed) #convert to a matrix format\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\nheatmap.2(ed_mat,\n  Rowv = NA, Colv = NA, scale = \"column\",\n  keysize = 0.7, density.info = \"none\",\n  col = hcl.colors(256), margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"none\"\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat do you notice? What insight do you gain about the variation across U.S. states?\n\nHeat map with row clusters\nIt can be tough to identify interesting patterns by visually comparing across rows and columns. Including dendrograms helps to identify interesting clusters.\n\nheatmap.2(ed_mat,\n  Rowv = TRUE, #this argument changed\n  Colv = NA, scale = \"column\", keysize = .7,\n  density.info = \"none\", col = hcl.colors(256),\n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"row\" #this argument changed\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states, now that states are grouped and ordered by similarity?\n\nHeat map with column clusters\nWe can also construct a heat map which identifies interesting clusters of columns (variables).\n\nheatmap.2(ed_mat,\n  Colv = TRUE, #this argument changed\n  Rowv = NA, scale = \"column\", keysize = .7,\n  density.info = \"none\", col = hcl.colors(256),\n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"column\" #this argument changed\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states, now that variables are grouped and ordered by similarity?\n\n\n\n\nThere’s more than one way to visualize multivariate patterns. Like heat maps, these star plot visualizations indicate the relative scale of each variable for each state. With this in mind, you can use the star maps to identify which state is the most “unusual.” You can also do a quick scan of the second image to try to cluster states. How does that clustering compare to the one generated in the heat map with row clusters above?\n\nstars(ed_mat,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)), #added external data to arrange by geo location\n  key.loc = c(-70, 30), cex = 1\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\nstars(ed_mat,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)), #added external data to arrange by geo location\n  key.loc = c(-70, 30), cex = 1, \n  draw.segments = TRUE #changed argument\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states with the star plots (arranged geographically) as compared to heat plots?",
    "crumbs": [
      "Src",
      "Multivariate Visualizations"
    ]
  },
  {
    "objectID": "src/05-Multivariate_Viz.html#learning-goals",
    "href": "src/05-Multivariate_Viz.html#learning-goals",
    "title": "Multivariate Visualizations",
    "section": "",
    "text": "Understand how we can use additional aesthetics such as color and size to incorporate a third (or more variables) to a bivariate plot\nDevelop comfort with interpreting heat maps and star plots, which allow you to look for patterns in variation in many variables.\n\nYou can download a template .Rmd of this activity here. Put this in a new folder called Assignment_04 in your folder for COMP_STAT_112.",
    "crumbs": [
      "Src",
      "Multivariate Visualizations"
    ]
  },
  {
    "objectID": "src/05-Multivariate_Viz.html#adding-more-aesthetic-attributes",
    "href": "src/05-Multivariate_Viz.html#adding-more-aesthetic-attributes",
    "title": "Multivariate Visualizations",
    "section": "",
    "text": "Though far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education data contain various education variables for each state:\n\n\nWarning in file(file, \"rt\"): cannot open file 'data/sat.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\neducation &lt;- read.csv(\"https://bcheggeseth.github.io/112_spring_2023/data/sat.csv\")\n\n\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\n\nError in knitr::include_graphics(\"images/SATcodebook.png\"): Cannot find the file(s): \"images/SATcodebook.png\"\n\n\nTo examine the variability in average SAT scores from state to state, let’s start with a univariate density plot:\n\nggplot(education, aes(x = sat)) +\n  geom_density(fill = \"blue\", alpha = .5) + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\nThe first question we’d like to answer is to what degree do per pupil spending (expend) and teacher salary explain this variability? We can start by plotting each against sat, along with a best fit linear regression model:\n\nggplot(education, aes(y = sat, x = salary)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\nggplot(education, aes(y = sat, x = expend)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = \"lm\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\nIs there anything that surprises you in the above plots? What are the relationship trends?\n\n\n\nSolution\n\nThese seem to suggest that spending more money on students or teacher salaries correlates with lower SAT scores. Say it ain’t so!\n\n\n\n\nMake a single scatterplot visualization that demonstrates the relationship between `sat`, `salary`, and `expend`. Summarize the trivariate relationship between `sat`, `salary`, and `expend`. \n\nHints: 1. Try using the color or size aesthetics to incorporate the expenditure data. 2. Include some model smooths with geom_smooth() to help highlight the trends.\n\n\nSolution\n\nBelow are four different plots that one could make. There seems to be a high correlation between expend and salary, and both seem to be negatively correlated with sat.\n\n#plot 1\ng1 &lt;- ggplot(education, aes(y=sat, x=salary, color=expend)) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 2\ng2 &lt;- ggplot(education, aes(y=sat, x=salary, size=expend)) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 3\ng3 &lt;- ggplot(education, aes(y=sat, x=salary, color=cut(expend,2))) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n#plot 4\ng4 &lt;- ggplot(education, aes(y=sat, x=salary, color=cut(expend,3))) + \n    geom_point() + \n    geom_smooth(se=FALSE, method=\"lm\") + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\nlibrary(gridExtra)\ngrid.arrange(g1, g2, g3, g4, ncol=2)\n\nError in eval(expr, envir, enclos): object 'g1' not found\n\n\n\n\n\n\nThe `fracCat` variable in the `education` data categorizes the fraction of the state's students that take the SAT into `low` (below 15%), `medium` (15-45%), and `high` (at least 45%).\n\n\nMake a univariate visualization of the fracCat variable to better understand how many states fall into each category.\n\nMake a bivariate visualization that demonstrates the relationship between fracCat and sat. What story does your graphic tell?\nMake a trivariate visualization that demonstrates the relationship between fracCat, sat, and expend. Incorporate fracCat as the color of each point, and use a single call to geom_smooth to add three trendlines (one for each fracCat). What story does your graphic tell?\n\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why does it appear that SAT scores decrease as spending increases even though the opposite is true?\n\n\n\nSolution\n\n\n\n\n\nggplot(education, aes(x = fracCat)) +\n  geom_bar() + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\n\n\n\nggplot(education, aes(x = fracCat, y = sat)) +\n  geom_boxplot() + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\n\n\n\nggplot(education, aes(color = fracCat, y = sat, x = expend)) +\n  geom_point() + geom_smooth(se = FALSE, method = 'lm') + theme_classic()\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n\n\nAmong student participation tends to be lower among states with lower expenditures. Those same states tend to have higher SAT scores because of the self-selection of who participates; only those most prepared take the SAT in those states.",
    "crumbs": [
      "Src",
      "Multivariate Visualizations"
    ]
  },
  {
    "objectID": "src/05-Multivariate_Viz.html#other-multivariate-visualization-techniques",
    "href": "src/05-Multivariate_Viz.html#other-multivariate-visualization-techniques",
    "title": "Multivariate Visualizations",
    "section": "",
    "text": "Note that each variable (column) is scaled to indicate states (rows) with high values (yellow) to low values (purple/blue). With this in mind you can scan across rows & across columns to visually assess which states & variables are related, respectively. You can also play with the color scheme. Type ?cm.colors in the console to see various options.\n\ned &lt;- as.data.frame(education) # convert from tibble to data frame\n\nError in eval(expr, envir, enclos): object 'education' not found\n\n# convert to a matrix with State names as the row names\nrow.names(ed) &lt;- ed$State  #added state names as the row names rather than a variable\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\ned &lt;- ed %&gt;% select(2:8) #select the 2nd through 8th columns\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\ned_mat &lt;- data.matrix(ed) #convert to a matrix format\n\nError in eval(expr, envir, enclos): object 'ed' not found\n\nheatmap.2(ed_mat,\n  Rowv = NA, Colv = NA, scale = \"column\",\n  keysize = 0.7, density.info = \"none\",\n  col = hcl.colors(256), margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"none\"\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat do you notice? What insight do you gain about the variation across U.S. states?\n\nHeat map with row clusters\nIt can be tough to identify interesting patterns by visually comparing across rows and columns. Including dendrograms helps to identify interesting clusters.\n\nheatmap.2(ed_mat,\n  Rowv = TRUE, #this argument changed\n  Colv = NA, scale = \"column\", keysize = .7,\n  density.info = \"none\", col = hcl.colors(256),\n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"row\" #this argument changed\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states, now that states are grouped and ordered by similarity?\n\nHeat map with column clusters\nWe can also construct a heat map which identifies interesting clusters of columns (variables).\n\nheatmap.2(ed_mat,\n  Colv = TRUE, #this argument changed\n  Rowv = NA, scale = \"column\", keysize = .7,\n  density.info = \"none\", col = hcl.colors(256),\n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", cexRow = 2, cexCol = 2, trace = \"none\",\n  dendrogram = \"column\" #this argument changed\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states, now that variables are grouped and ordered by similarity?\n\n\n\n\nThere’s more than one way to visualize multivariate patterns. Like heat maps, these star plot visualizations indicate the relative scale of each variable for each state. With this in mind, you can use the star maps to identify which state is the most “unusual.” You can also do a quick scan of the second image to try to cluster states. How does that clustering compare to the one generated in the heat map with row clusters above?\n\nstars(ed_mat,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)), #added external data to arrange by geo location\n  key.loc = c(-70, 30), cex = 1\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\nstars(ed_mat,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)), #added external data to arrange by geo location\n  key.loc = c(-70, 30), cex = 1, \n  draw.segments = TRUE #changed argument\n)\n\nError in eval(expr, envir, enclos): object 'ed_mat' not found\n\n\n\nWhat new insight do you gain about the variation across U.S. states with the star plots (arranged geographically) as compared to heat plots?",
    "crumbs": [
      "Src",
      "Multivariate Visualizations"
    ]
  },
  {
    "objectID": "src/03-Effective_Viz.html",
    "href": "src/03-Effective_Viz.html",
    "title": "Effective Visualizations",
    "section": "",
    "text": "Understand and apply the guiding principles of effective visualizations\n\nYou can download a template .Rmd of this activity here. Put the file in a Assignment_03 folder within your COMP_STAT_112 folder.\n\n\n\n\n\nVisualizations help us understand what we’re working with:\n\nWhat are the scales of our variables?\n\nAre there any outliers, i.e. unusual cases?\n\nWhat are the patterns among our variables?\n\nThis understanding will inform our next steps:\n\nWhat method of analysis / model is appropriate?\n\nOnce our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story.\n\n\n\nThere is not one right way to visualize a data set.\nHowever, there are guiding principles that distinguish between “good” and “bad” graphics.\nOne of the best ways to learn is by reading graphics and determining which ways of arranging thing are better or worse. So before jumping directly into theoretical principles, let’s try some critical analysis on specific examples.\n\nFor your assigned graphics or sets of graphics, identify the following:\n\n1. the story the graphic is aiming to communicate to the audience\n2. effective features of the graphic\n3. areas for improvement\n\n\n\nError in knitr::include_graphics(\"images/badviz3.png\"): Cannot find the file(s): \"images/badviz3.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/trilogies.gif\"): Cannot find the file(s): \"images/trilogies.gif\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/chernoff.png\"): Cannot find the file(s): \"images/chernoff.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz2.png\"): Cannot find the file(s): \"images/badviz2.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/obama1.png\"): Cannot find the file(s): \"images/obama1.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/breakup.png\"): Cannot find the file(s): \"images/breakup.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/heatmap.png\"): Cannot find the file(s): \"images/heatmap.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz1.png\"): Cannot find the file(s): \"images/badviz1.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/slopegraph1.png\"): Cannot find the file(s): \"images/slopegraph1.png\"\n\n\n\n\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/piechart.png\"): Cannot find the file(s): \"images/piechart.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/Nightingale.png\"): Cannot find the file(s): \"images/Nightingale.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/compensation.png\"): Cannot find the file(s): \"images/compensation.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/employees.png\"): Cannot find the file(s): \"images/employees.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/clutter.png\"): Cannot find the file(s): \"images/clutter.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz4.png\"): Cannot find the file(s): \"images/badviz4.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/plagiarism.png\"): Cannot find the file(s): \"images/plagiarism.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/tv-size-by-year1.png\"): Cannot find the file(s): \"images/tv-size-by-year1.png\"\n\n\n\n\n\n\nMore examples:\n\nFlowingData: blog and Best visualizations of 2016\nWTF Visualizations\n\n\n\n\n\n\nRemember …\n\nGraphics are designed by the human expert (you!) in order to reveal information that’s in the data.\n\nYour choices depend on what information you want to reveal and convey. So before you complete a graphic, you should clearly identify what story you want the graphic to tell to the audience, and double check that this story is being told.1\nHere is a nice example from FiveThirtyEight where each chart tells a story in answer to a particular question about the [then] upcoming German election.\nHere is an interactive visualization that tells a story about gun violence.\nAnother important contextual question to ask is whether the graphic is for an explanatory (explain why) or exploratory (discovering something new) analysis.\n\n\n\nIn addition to considering the story you are telling, you need to consider what audiences can access your story.\nAlternative (Alt) Text: In order for data visualizations to be accessible to people who are blind and use screen readers, we can provide alt text. Alt text should concisely articulate (1) what your visualization is (e.g. a bar chart showing which the harvest rate of cucumbers), (2) a one sentence description of the what you think is the most important takeaway your visualization is showing, and (3) a link to your data source if it’s not already in the caption (check out this great resource on writing alt text for data visualizations).\nTo add the alt text to your the HTML created from knitting the Rmd, you can include it as an option at the top of your r chunk. For example: {r, fig.alt = “Bar chart showing the daily harvest of cucumbers. The peak cucumber collection day is August 18th”}. To see the alt text in the knitted html file, you can hover over the image.\nColor-blind friendly color palettes: In order for data visualizations to be accessible to people with color blindness, we need to be thoughtful about the colors we use in our data visualizations. The most common variety of color-blindness makes it hard for individuals to detect differences between red and green. Some types make it hard detect differences between blue and yellow. Other types make it hard to see different shades of a color.\nThis Chromatic Vision Simulator can give you a sense of how this could impact your perception of colors (see image below). You could also upload a visualization to this simulator to see how well your chosen color palette works.\n\n\nError in knitr::include_graphics(\"images/colorblind.jpg\"): Cannot find the file(s): \"images/colorblind.jpg\"\n\n\nTry to use a color-blind friendly / safe palette whenever possible. One easy way to do this is to include + scale_fill_viridis_d() or + scale_color_viridis_d() when you are filling or coloring by a discrete or categorical variable and + scale_fill_viridis_c() or + scale_color_viridis_c() when you are filling or coloring by a continuous or quantitative variable. There are many other color-blind friendly palettes in R; you can check out other resources here.\n\n\n\nMichael Correll of Tableau Research writes “Data visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor.” in his article from 2018.\nVisualization operates at the intersection of science, communication, and data science & statistics. There are professional standards of ethics in these fields of the power they hold over other people as it relates to making data-driven decisions.\nCorrell describes three ethical challenges of visualization work:\n\nVisibility Make the invisible visible\n\n\nVisualize hidden labor\nVisualize hidden uncertainty\nVisualize hidden impacts\n\nVisualizations can be complex and one must consider the accessibility of the visualization to the audience. Managing complexity is, therefore, a virtue in design that can be in direct opposition with the desire to visualize the invisible.\n\nPrivacy Collect data with empathy\n\n\nEncourage Small Data\nAnthropomorphize data\nObfuscate data to protect privacy\n\nRestricting the type and amount of data that is collected has a direct impact on the quality and scope of the analyses hence obligation to provide context, and analytical power can, therefore, stand in direct opposition to the empathic collection of data.\n\nPower Challenge structures of power\n\n\nSupport data due process.\nAct as data advocates.\nPressure unethical analytical behavior.\n\nThe goal of promoting truth and suppressing falsehood may require amplifying existing structures of expertise and power, and suppressing conflicts for the sake of rhetorical impact.\nAt a minimum, you should always\n\nPresent data in a way that avoids misleading the audience.\nAlways include your data source. Doing so attributes credit for labor, provides credibility to your work, and provides context for your graphic.\n\n\n\n\nA basic principle is that a graphic is about comparison. Good graphics make it easy for people to perceive things that are similar and things that are different. Good graphics put the things to be compared “side-by-side,” that is, in perceptual proximity to one another. The following aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor\n\nColor is the most difficult, because it is a 3-dimensional quantity. We are pretty good at color gradients, but discrete colors must be selected carefully. We need to be particularly aware of red/green color blindness issues.\nVisual perception and effective visualizations\nHere are some facts to keep in mind about visual perception from Now You See It:\n\nVisual perception is selective, and our attention is often drawn to constrasts from the norm.\n\n\n\nError in knitr::include_graphics(\"images/contrast.png\"): Cannot find the file(s): \"images/contrast.png\"\n\n\n\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\n\n\nOur eyes are drawn to familiar patterns. We see what we know and expect.\n\n\n\nError in knitr::include_graphics(\"images/rose1.png\"): Cannot find the file(s): \"images/rose1.png\"\n\n\n\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\n\n\nMemory plays an important role in human cognition, but working memory is extremely limited.\n\n\nImplication: Visualizations must serve as external aids to augment working memory. If a visualization is unfamiliar, then it won’t be as effective.\n\nGestalt principles\nThe Gestalt principles (more info here or here) were developed by psychologists including Max Wertheimer in the early 1900s to explain how humans perceive organized patterns and objects.\nIn a design setting, they help us understand how to incorporate preattentive features into visualizations. The figure below shows some preattentive features, all of which are processed prior to conscious attention (“at a glance”) and can help the reader focus on relevant information in a visualization.\n\n\nError in knitr::include_graphics(\"images/gestalt.png\"): Cannot find the file(s): \"images/gestalt.png\"\n\n\nOther design tips from Visualize This and Storytelling with Data:\n\nPut yourself in a reader’s shoes when you design data graphics. What parts of the data need explanation? We can minimize ambiguity by providing guides, label axes, etc.\nData graphics are meant to shine a light on your data. Try to remove any elements that don’t help you do that. That is, eliminate “chart junk” (distracting and unnecessary adornments).\nVary color and stroke styles to emphasize the parts in your graphic that are most important to the story you’re telling\nIt is easier to judge length than it is to judge area or angles\nBe thoughtful about how your categories (levels) are ordered for categorical data. There may be a natural ordering\nPie charts, donut charts, and 3D are evil\n\n\n\n\n\nInstead of memorizing which plot is appropriate for which situation, it’s best to simply start to recognize patterns in constructing graphics:\n\nEach quantitative variable requires a new axis.\n\nEach categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping).\nFor visualizations in which overlap in glyphs or plots obscures the patterns, try faceting or transparency.\n\n\n\n\nWhile we will not cover all of visualization theory – you can take a whole course on that at Macalester and it is a proper field in its own right – we will touch on the following types of visualizations in the coming weeks:\n\nBivariate visualizations\nVisualizations of higher dimensional data\nTemporal structures: timelines and flows\nHierarchical structures: trees\nRelational structures: networks\nSpatial structures: maps\nSpatio-temporal structures\nTextual structures\nInteractive graphics (e.g., gganimate, shiny)\n\n\n\n\n\n\nConsider one of the more complicated data graphics listed at (http://mdsr-book.github.io/exercises.html#exercise_25):\n\na. What story does the data graphic tell? What is the main message that you take away from it?\nb. Can the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nc. Critique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Brilliant? Are there things that you would have done differently? Justify your response.\n\n\n\nSolution\n\nExample: http://hint.fm/wind/\n\nThe dynamic visual shows the chaos and beauty of the wind. Since the graphic is updated regularly, the exact wind patterns will be different so the overall message is that there is an interesting relationship between topography and wind with a large amount of uncertainty and chaos.\nframe: longitude (x) and latitude (y); glyph: paths/lines, geography boundaries (lower 48 states), circles for cities, text for city names; aesthetics: white color of paths/lines, speed of animation path/line, size of white city circles corresponds to population, dark grey color for geographic polygon boundary, white outlined black text of city names; no facets; guide for speed of animation of paths, no guide for city circle size.\nSince the graphic is only for one time and date, you can’t compare how the patterns change over time. It would be interesting to include elevation in the background map to see how the wind patterns are impacted by the topography.",
    "crumbs": [
      "Src",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "src/03-Effective_Viz.html#learning-goals",
    "href": "src/03-Effective_Viz.html#learning-goals",
    "title": "Effective Visualizations",
    "section": "",
    "text": "Understand and apply the guiding principles of effective visualizations\n\nYou can download a template .Rmd of this activity here. Put the file in a Assignment_03 folder within your COMP_STAT_112 folder.",
    "crumbs": [
      "Src",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "src/03-Effective_Viz.html#effective-visualizations-1",
    "href": "src/03-Effective_Viz.html#effective-visualizations-1",
    "title": "Effective Visualizations",
    "section": "",
    "text": "Visualizations help us understand what we’re working with:\n\nWhat are the scales of our variables?\n\nAre there any outliers, i.e. unusual cases?\n\nWhat are the patterns among our variables?\n\nThis understanding will inform our next steps:\n\nWhat method of analysis / model is appropriate?\n\nOnce our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story.\n\n\n\nThere is not one right way to visualize a data set.\nHowever, there are guiding principles that distinguish between “good” and “bad” graphics.\nOne of the best ways to learn is by reading graphics and determining which ways of arranging thing are better or worse. So before jumping directly into theoretical principles, let’s try some critical analysis on specific examples.\n\nFor your assigned graphics or sets of graphics, identify the following:\n\n1. the story the graphic is aiming to communicate to the audience\n2. effective features of the graphic\n3. areas for improvement\n\n\n\nError in knitr::include_graphics(\"images/badviz3.png\"): Cannot find the file(s): \"images/badviz3.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/trilogies.gif\"): Cannot find the file(s): \"images/trilogies.gif\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/chernoff.png\"): Cannot find the file(s): \"images/chernoff.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz2.png\"): Cannot find the file(s): \"images/badviz2.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/obama1.png\"): Cannot find the file(s): \"images/obama1.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/breakup.png\"): Cannot find the file(s): \"images/breakup.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/heatmap.png\"): Cannot find the file(s): \"images/heatmap.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz1.png\"): Cannot find the file(s): \"images/badviz1.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/slopegraph1.png\"): Cannot find the file(s): \"images/slopegraph1.png\"\n\n\n\n\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\nDiamond data visualizations from R for Data Science, 2017\n\n\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/piechart.png\"): Cannot find the file(s): \"images/piechart.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/Nightingale.png\"): Cannot find the file(s): \"images/Nightingale.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/compensation.png\"): Cannot find the file(s): \"images/compensation.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/employees.png\"): Cannot find the file(s): \"images/employees.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/clutter.png\"): Cannot find the file(s): \"images/clutter.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/badviz4.png\"): Cannot find the file(s): \"images/badviz4.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/plagiarism.png\"): Cannot find the file(s): \"images/plagiarism.png\"\n\n\n\n\n\n\n\n\nError in knitr::include_graphics(\"images/tv-size-by-year1.png\"): Cannot find the file(s): \"images/tv-size-by-year1.png\"\n\n\n\n\n\n\nMore examples:\n\nFlowingData: blog and Best visualizations of 2016\nWTF Visualizations\n\n\n\n\n\n\nRemember …\n\nGraphics are designed by the human expert (you!) in order to reveal information that’s in the data.\n\nYour choices depend on what information you want to reveal and convey. So before you complete a graphic, you should clearly identify what story you want the graphic to tell to the audience, and double check that this story is being told.1\nHere is a nice example from FiveThirtyEight where each chart tells a story in answer to a particular question about the [then] upcoming German election.\nHere is an interactive visualization that tells a story about gun violence.\nAnother important contextual question to ask is whether the graphic is for an explanatory (explain why) or exploratory (discovering something new) analysis.\n\n\n\nIn addition to considering the story you are telling, you need to consider what audiences can access your story.\nAlternative (Alt) Text: In order for data visualizations to be accessible to people who are blind and use screen readers, we can provide alt text. Alt text should concisely articulate (1) what your visualization is (e.g. a bar chart showing which the harvest rate of cucumbers), (2) a one sentence description of the what you think is the most important takeaway your visualization is showing, and (3) a link to your data source if it’s not already in the caption (check out this great resource on writing alt text for data visualizations).\nTo add the alt text to your the HTML created from knitting the Rmd, you can include it as an option at the top of your r chunk. For example: {r, fig.alt = “Bar chart showing the daily harvest of cucumbers. The peak cucumber collection day is August 18th”}. To see the alt text in the knitted html file, you can hover over the image.\nColor-blind friendly color palettes: In order for data visualizations to be accessible to people with color blindness, we need to be thoughtful about the colors we use in our data visualizations. The most common variety of color-blindness makes it hard for individuals to detect differences between red and green. Some types make it hard detect differences between blue and yellow. Other types make it hard to see different shades of a color.\nThis Chromatic Vision Simulator can give you a sense of how this could impact your perception of colors (see image below). You could also upload a visualization to this simulator to see how well your chosen color palette works.\n\n\nError in knitr::include_graphics(\"images/colorblind.jpg\"): Cannot find the file(s): \"images/colorblind.jpg\"\n\n\nTry to use a color-blind friendly / safe palette whenever possible. One easy way to do this is to include + scale_fill_viridis_d() or + scale_color_viridis_d() when you are filling or coloring by a discrete or categorical variable and + scale_fill_viridis_c() or + scale_color_viridis_c() when you are filling or coloring by a continuous or quantitative variable. There are many other color-blind friendly palettes in R; you can check out other resources here.\n\n\n\nMichael Correll of Tableau Research writes “Data visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor.” in his article from 2018.\nVisualization operates at the intersection of science, communication, and data science & statistics. There are professional standards of ethics in these fields of the power they hold over other people as it relates to making data-driven decisions.\nCorrell describes three ethical challenges of visualization work:\n\nVisibility Make the invisible visible\n\n\nVisualize hidden labor\nVisualize hidden uncertainty\nVisualize hidden impacts\n\nVisualizations can be complex and one must consider the accessibility of the visualization to the audience. Managing complexity is, therefore, a virtue in design that can be in direct opposition with the desire to visualize the invisible.\n\nPrivacy Collect data with empathy\n\n\nEncourage Small Data\nAnthropomorphize data\nObfuscate data to protect privacy\n\nRestricting the type and amount of data that is collected has a direct impact on the quality and scope of the analyses hence obligation to provide context, and analytical power can, therefore, stand in direct opposition to the empathic collection of data.\n\nPower Challenge structures of power\n\n\nSupport data due process.\nAct as data advocates.\nPressure unethical analytical behavior.\n\nThe goal of promoting truth and suppressing falsehood may require amplifying existing structures of expertise and power, and suppressing conflicts for the sake of rhetorical impact.\nAt a minimum, you should always\n\nPresent data in a way that avoids misleading the audience.\nAlways include your data source. Doing so attributes credit for labor, provides credibility to your work, and provides context for your graphic.\n\n\n\n\nA basic principle is that a graphic is about comparison. Good graphics make it easy for people to perceive things that are similar and things that are different. Good graphics put the things to be compared “side-by-side,” that is, in perceptual proximity to one another. The following aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor\n\nColor is the most difficult, because it is a 3-dimensional quantity. We are pretty good at color gradients, but discrete colors must be selected carefully. We need to be particularly aware of red/green color blindness issues.\nVisual perception and effective visualizations\nHere are some facts to keep in mind about visual perception from Now You See It:\n\nVisual perception is selective, and our attention is often drawn to constrasts from the norm.\n\n\n\nError in knitr::include_graphics(\"images/contrast.png\"): Cannot find the file(s): \"images/contrast.png\"\n\n\n\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\n\n\nOur eyes are drawn to familiar patterns. We see what we know and expect.\n\n\n\nError in knitr::include_graphics(\"images/rose1.png\"): Cannot find the file(s): \"images/rose1.png\"\n\n\n\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\n\n\nMemory plays an important role in human cognition, but working memory is extremely limited.\n\n\nImplication: Visualizations must serve as external aids to augment working memory. If a visualization is unfamiliar, then it won’t be as effective.\n\nGestalt principles\nThe Gestalt principles (more info here or here) were developed by psychologists including Max Wertheimer in the early 1900s to explain how humans perceive organized patterns and objects.\nIn a design setting, they help us understand how to incorporate preattentive features into visualizations. The figure below shows some preattentive features, all of which are processed prior to conscious attention (“at a glance”) and can help the reader focus on relevant information in a visualization.\n\n\nError in knitr::include_graphics(\"images/gestalt.png\"): Cannot find the file(s): \"images/gestalt.png\"\n\n\nOther design tips from Visualize This and Storytelling with Data:\n\nPut yourself in a reader’s shoes when you design data graphics. What parts of the data need explanation? We can minimize ambiguity by providing guides, label axes, etc.\nData graphics are meant to shine a light on your data. Try to remove any elements that don’t help you do that. That is, eliminate “chart junk” (distracting and unnecessary adornments).\nVary color and stroke styles to emphasize the parts in your graphic that are most important to the story you’re telling\nIt is easier to judge length than it is to judge area or angles\nBe thoughtful about how your categories (levels) are ordered for categorical data. There may be a natural ordering\nPie charts, donut charts, and 3D are evil\n\n\n\n\n\nInstead of memorizing which plot is appropriate for which situation, it’s best to simply start to recognize patterns in constructing graphics:\n\nEach quantitative variable requires a new axis.\n\nEach categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping).\nFor visualizations in which overlap in glyphs or plots obscures the patterns, try faceting or transparency.\n\n\n\n\nWhile we will not cover all of visualization theory – you can take a whole course on that at Macalester and it is a proper field in its own right – we will touch on the following types of visualizations in the coming weeks:\n\nBivariate visualizations\nVisualizations of higher dimensional data\nTemporal structures: timelines and flows\nHierarchical structures: trees\nRelational structures: networks\nSpatial structures: maps\nSpatio-temporal structures\nTextual structures\nInteractive graphics (e.g., gganimate, shiny)",
    "crumbs": [
      "Src",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "src/03-Effective_Viz.html#practice",
    "href": "src/03-Effective_Viz.html#practice",
    "title": "Effective Visualizations",
    "section": "",
    "text": "Consider one of the more complicated data graphics listed at (http://mdsr-book.github.io/exercises.html#exercise_25):\n\na. What story does the data graphic tell? What is the main message that you take away from it?\nb. Can the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nc. Critique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Brilliant? Are there things that you would have done differently? Justify your response.\n\n\n\nSolution\n\nExample: http://hint.fm/wind/\n\nThe dynamic visual shows the chaos and beauty of the wind. Since the graphic is updated regularly, the exact wind patterns will be different so the overall message is that there is an interesting relationship between topography and wind with a large amount of uncertainty and chaos.\nframe: longitude (x) and latitude (y); glyph: paths/lines, geography boundaries (lower 48 states), circles for cities, text for city names; aesthetics: white color of paths/lines, speed of animation path/line, size of white city circles corresponds to population, dark grey color for geographic polygon boundary, white outlined black text of city names; no facets; guide for speed of animation of paths, no guide for city circle size.\nSince the graphic is only for one time and date, you can’t compare how the patterns change over time. It would be interesting to include elevation in the background map to see how the wind patterns are impacted by the topography.",
    "crumbs": [
      "Src",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "src/03-Effective_Viz.html#footnotes",
    "href": "src/03-Effective_Viz.html#footnotes",
    "title": "Effective Visualizations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA “negative” result (e.g., there is no correlation between two variables) is a perfectly fine story to tell.↩︎\nThis list is from B. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15. For more of the theory of perception, see also W.S. Cleveland and R. McGill, “Graphical perception: Theory, experimentation, and application to the development of graphical methods,” Journal of the American Statistical Association, 1984.↩︎",
    "crumbs": [
      "Src",
      "Effective Visualizations"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#learning-goals",
    "href": "src/02-Intro_Data_Viz.html#learning-goals",
    "title": "(PART) Visualization",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nUnderstand the Grammar of Graphics\nUse ggplot2 to create basic layers of graphics\nUnderstand the different basic univariate visualizations for categorical and quantitative variables\n\nYou can download a template .Rmd of the examples and exercises in this activity here. Put this file in a new folder called Assignment_02 in your folder for COMP_STAT_112.",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#benefits-of-visualizations",
    "href": "src/02-Intro_Data_Viz.html#benefits-of-visualizations",
    "title": "(PART) Visualization",
    "section": "Benefits of Visualizations",
    "text": "Benefits of Visualizations\n\nVisualizations help us understand what we’re working with:\n\nWhat are the scales of our variables?\n\nAre there any outliers, i.e. unusual cases?\n\nWhat are the patterns among our variables?\n\n\nThis understanding will inform our next steps:\n\nWhat method of analysis / model is appropriate?\n\nOnce our analysis is complete, visualizations are a powerful way to communicate our findings and tell a story.",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#glyphs",
    "href": "src/02-Intro_Data_Viz.html#glyphs",
    "title": "(PART) Visualization",
    "section": "Glyphs",
    "text": "Glyphs\nIn its original sense, in archaeology, a glyph is a carved symbol.\n\n\n\n\n\n\n\nHeiroglyph\nMayan glyph\n\n\n\n\n\n\n\n\n\n\nData Glyph\nA data glyph is also a mark, e.g.\n              \nThe features of a data glyph encodes the value of variables.\n\nSome are very simple, e.g. a dot: \nSome combine different elements, e.g. a pointrange: \nSome are complicated, e.g. a dotplot: \n\n\n\nComponents of Graphics\n\n\n\n\n\n\n\n\nFigure 1: Blood pressure readings from a random subset of the NHANES data set.\n\n\n\n\n\n\nframe: The position scale describing how data are mapped to x and y\nglyph: The basic graphical unit that represents one case.\n\nother terms used include mark and symbol.\n\naesthetic: a visual property of a glyph such as position, size, shape, color, etc.\n\nmay be mapped based on data values: smoker -&gt; color\nmay be set to particular non-data related values: color is black\n\nfacet: a subplot that shows one subset of the data\n\nrather than represent sex by shape, we could split into two subplots\n\nscale: A mapping that translates data values into aesthetics.\n\nexample: never-&gt; pink; former-&gt; aqua; current-&gt; green\n\nguide: An indication for the human viewer of the scale. This allows the viewer to translate aesthetics back into data values.\n\nexamples: x- and y-axes, various sorts of legends\n\n\n\n\nEye Training for the Layered Grammar of Graphics\n\nFor your assigned graphic, discuss the following seven questions with your partner(s):\n\n1. What variables constitute the frame?\n2. What glyphs are used?\n3. What are the aesthetics for those glyphs?\n4. Which variable is mapped to each aesthetic?\n5. Which variable, if any, is used for faceting?\n6. Which scales are displayed with a guide?\n7. What raw data would be required for this plot, and what form should it be in?\n\nHere are the graphics examples, all taken from the New York Times website:\n\na. [Admissions gap](http://www.nytimes.com/interactive/2013/05/07/education/college-admissions-gap.html?_r=0)\n#. [Medicare hospital charges](https://www.nytimes.com/interactive/2014/06/02/business/how-much-hospitals-charged-medicare-for-the-same-procedures.html)\n#. [Football conferences](http://www.nytimes.com/newsgraphics/2013/11/30/football-conferences/)\n#. [Housing prices](https://www.nytimes.com/interactive/2014/01/23/business/case-shiller-slider.html)\n#. [Baseball pitching](http://www.nytimes.com/interactive/2013/03/29/sports/baseball/Strikeouts-Are-Still-Soaring.html)\n#. [Phillips curve](http://www.nytimes.com/interactive/2013/10/09/us/yellen-fed-chart.html)\n#. [School mathematics ratings](http://www.nytimes.com/interactive/2013/02/04/science/girls-lead-in-science-exam-but-not-in-the-united-states.html)\n#. [Corporate taxes](http://www.nytimes.com/interactive/2013/05/25/sunday-review/corporate-taxes.html)\n\n\n\n\nGlyph-Ready Data\nNote the mapping of data to aesthetics for Figure @ref(fig:fig-bp):\n   sbp [Systolic Blood Pressure] -&gt; x      \n   dbp [Diastolic Blood Pressure] -&gt; y     \nsmoker -&gt; color\n   sex -&gt; shape\nGlyph-ready data has this form:\n\nThere is one row for each glyph to be drawn.\nThe variables in that row are mapped to aesthetics of the glyph (including position).\n\n\n\n\nA subset of the NHANES data set.\n\n\nsbp\ndbp\nsex\nsmoker\n\n\n\n\n112\n55\nmale\nformer\n\n\n144\n84\nmale\nnever\n\n\n143\n84\nfemale\nnever\n\n\n110\n62\nfemale\nnever\n\n\n121\n72\nfemale\nnever\n\n\n129\n60\nfemale\nnever",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#data-visualization-workflow-ggplot",
    "href": "src/02-Intro_Data_Viz.html#data-visualization-workflow-ggplot",
    "title": "(PART) Visualization",
    "section": "Data Visualization Workflow + ggplot",
    "text": "Data Visualization Workflow + ggplot\n\nLayers – Building up Complex Plots\nUsing the ggplot2 package, we can create graphics by building up layers, each of which may have its own data, glyphs, aesthetic mapping, etc. As an example, let’s peel back the layers used to create Figure @ref(fig:fig-bp).\nThe first layer just identifies the data set. It sets up a blank canvas, but does not actually plot anything:\n\nggplot(data = Tmp)\n\n\n\n\n\n\n\n\nNext, we add a geometry layer to identify the mapping of data to aesthetics for each of the glyphs:\n\nggplot(data = Tmp) +\n  geom_point(mapping = aes(x = sbp, y = dbp, shape = sex, color = smoker), size = 5, alpha = .8)\n\n\n\n\n\n\n\n\nNext, we can add some axes labels as guides:\n\nggplot(data = Tmp) +\n  geom_point(mapping = aes(x = sbp, y = dbp, shape = sex, color = smoker), size = 5, alpha = .8) +\n  xlab(\"Systolic BP\") + ylab(\"Diastolic BP\")\n\n\n\n\n\n\n\n\nAnd, finally, we can change the scale of the color used for smoker status:\n\nggplot(data = Tmp) +\n  geom_point(mapping = aes(x = sbp, y = dbp, shape = sex, color = smoker), size = 5, alpha = .8) +\n  xlab(\"Systolic BP\") + ylab(\"Diastolic BP\") +\n  scale_color_manual(values = c(\"#F8766D\", \"#00BFC4\", \"#00BA38\"))\n\n\n\n\n\n\n\n\nIf instead we wanted to facet into columns based on smoker status, we could add another layer for that:\n\nggplot(data = Tmp) +\n  geom_point(mapping = aes(x = sbp, y = dbp, shape = sex, color = smoker), size = 5, alpha = .8) +\n  xlab(\"Systolic BP\") + ylab(\"Diastolic BP\") +\n  scale_color_manual(values = c(\"#F8766D\", \"#00BFC4\", \"#00BA38\")) +\n  facet_grid(. ~ smoker)\n\n\n\n\n\n\n\n\nFor more information on all of the different types of layers we can add to graphics, see the ggplot2 reference page and the data visualization with ggplot2 cheat sheet.\n\n\nGetting Started\nThere’s no end to the number and type of visualizations you could make. Thus the process can feel overwhelming. FlowingData makes good recommendations for data viz workflow:\n\nAsk the data questions. Simple research questions will guide the types of visualizations that you should construct.\n\nStart with the basics and work incrementally. Before constructing complicated or multivariate or interactive graphics, start with simple visualizations. An understanding of the simple patterns provides a foundation upon which to build more advanced analyses and visualizations. This incremental process works particularly well with the layered grammar of graphics in ggplot.\nFocus. Reporting a large number of visualizations can overwhelm the audience and obscure your conclusions. Instead, pick out a focused yet comprehensive set of visualizations. Here is an example of one dataset visualized 25 different ways, each with a different focus and interpretation, and what can happen if you let the data ramble on without a focus.\n\nIn this course we’ll largely construct visualizations using the ggplot function in RStudio. Though the ggplot learning curve can be steep, its “grammar” is intuitive and generalizable once mastered. The ggplot plotting function is stored in the ggplot2 package:\n\nlibrary(ggplot2)\n\nThe best way to learn about ggplot is to just play around. Focus on the patterns and potential of their application. It will be helpful to have the RStudio Data Visualization cheat sheet handy as you complete this activity.\n\n\nAn Example\nThe “Bechdel test”, named after cartoonist Alison Bechdel, tests whether movies meet the following criteria:\n\nThere are \\(\\ge\\) 2 (named) female characters;\n\nthese women talk to each other…\n\nabout something other than a man.\n\nIn the fivethirtyeight.com article “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”, the authors analyze which Hollywood movies do/don’t pass the test. Their data are available in the fivethirtyeight package:\n\nlibrary(fivethirtyeight)\ndata(bechdel)\nhead(bechdel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nimdb\ntitle\nclean_test\nbinary\nbudget_2013\ndomgross_2013\nintgross_2013\n\n\n\n\n2013\ntt1711425\n21 & Over\nnotalk\nFAIL\n13000000\n25682380\n42195766\n\n\n2012\ntt1343727\nDredd 3D\nok\nPASS\n45658735\n13611086\n41467257\n\n\n2013\ntt2024544\n12 Years a Slave\nnotalk\nFAIL\n20000000\n53107035\n158607035\n\n\n2013\ntt1272878\n2 Guns\nnotalk\nFAIL\n61000000\n75612460\n132493015\n\n\n2013\ntt0453562\n42\nmen\nFAIL\n40000000\n95020213\n95020213\n\n\n2013\ntt1335975\n47 Ronin\nmen\nFAIL\n225000000\n38362475\n145803842\n\n\n\n\n\n\nBefore diving into any visualizations of these data, we first must understand its structure and contents. Discuss the following:  \n  \n  a. What are the units of observation and how many units are in this sample? \n  b. What are the levels of the `clean_test` and `binary` categorical variables?    \n  c. Check out the codebook for `bechdel` (`?bechdel`).  What's the difference between `domgross_2013` and `domgross`?    \n\n\n\nSolution\n\n\n#units of observation are movies; there are 1794 movies in this sample\ndim(bechdel)\n## [1] 1794   15\n\n#clean_test has values of \"nowomen\", \"notalk\", \"men\", \"dubious\", \"ok\"\n#View(bchedel) and look at values or summarize like below\ntable(bechdel$clean_test)\n## \n## nowomen  notalk     men dubious      ok \n##     141     514     194     142     803\nlevels(bechdel$clean_test)\n## [1] \"nowomen\" \"notalk\"  \"men\"     \"dubious\" \"ok\"\n\n#binary has values of PASS or FAIL\ntable(bechdel$binary)\n## \n## FAIL PASS \n##  991  803\nlevels(factor(bechdel$binary))\n## [1] \"FAIL\" \"PASS\"\n\n# domgross_2013 is the domestic gross in US dollars but it is inflation adjusted with respect to 2013\n#?bechdel\n\n\n\n\n\nWe'll consider *univariate* visualizations of the `clean_test` and `budget_2013` variables. Discuss the following:\n  \n  a. What features would we like a visualization of the *categorical* `clean_test` variable to capture?    \n  b. What features would we like a visualization of the *quantitative* `budget_2013` variable to capture?    \n\n\n\n\nSolution\n\n\ncapture the frequency of each way a movie can fail or pass the Bechdel test\ncapture the typical budget as well as how much variation there is across movies and if there are any outliers\n\n\n\n\n\n\nCategorical univariate visualization\nWe begin by stating a clear research question:\n\nAmong the movies in our sample, what fraction pass the Bechdel test? Among those that fail the test, in which way do they fail (e.g., there are no women, there are women but they only talk about men,…)?\n\nTo answer the above research question, we can explore the categorical clean_test variable. A table provides a simple summary of the number of movies that fall into each clean_test category:\n\ntable(bechdel$clean_test)\n\n\nnowomen  notalk     men dubious      ok \n    141     514     194     142     803 \n\n\n\nExamine the table of `clean_test` data, and try to interpret it. What insights does it provide about the original research question?\n\n\n\nSolution\n\nAmong the categories, the “ok” category was most frequent, meaning that 803 of the 1794 movies in the sample passed the Bechdel Test. However, among those 991 movies that did not pass the test, most of them (514 of them) did not pass because the women did not talk.\n\n\n\nBecause clean_test is a categorical variable, a bar chart provides an appropriate visualization of this table. In examining the bar chart, keep your eyes on the following.\n\nvariability: Are cases evenly spread out among the categories or are some categories more common than others?\n\ncontextual implications: In the context of your research, what do you learn from the bar chart? How would you describe your findings to a broad audience?\n\n\nTry out the code below that builds up from a simple to a customized bar chart. At each step determine how each piece of code contributes to the plot and add a comment describe the addition.    \n\n\n# plot 1: set up a plotting frame (a blank canvas)\nggplot(bechdel, aes(x = clean_test))\n\n# plot 2: what changed / how did we change it?\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar()\n\n# plot 3: what changed / how did we change it?\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar() +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n# plot 4: what changed / how did we change it?\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar(color = \"purple\") +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n# plot 5: what changed / how did we change it?\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar(fill = \"purple\") +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n\n\nSolution\n\n\n# plot 1: set up a plotting frame (a blank canvas)\nggplot(bechdel, aes(x = clean_test))\n\n\n\n\n\n\n\n\n# plot 2: Added bars that reflect the count or frequency of the movies within each category\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar()\n\n\n\n\n\n\n\n\n# plot 3: Added/changed the text labels for the x and y axes\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar() +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n\n\n\n\n\n\n\n# plot 4: Changed the outline color of the bars to purple\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar(color = \"purple\") +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n\n\n\n\n\n\n\n# plot 5: Changed the fill  color of the bars to purple\nggplot(bechdel, aes(x = clean_test)) +\ngeom_bar(fill = \"purple\") +\nlabs(x = \"Outcome of Bechdel Test\", y = \"Number of movies\")\n\n\n\n\n\n\n\n\n\n\n\n\nSummarize the visualization: what did you learn about the distribution of the `clean_test` variable?    \n\n\n\n\nSolution\n\nAmong the categories, the “ok” category was most frequent. However, among those movies that did not pass the test, most of them did not pass because the women in the movie did not talk.\n\n\n\n\nLet's return to our research question: What percent of movies in the sample pass the Bechdel test? Among those that fail the test, in which way do they fail? \n\n\n\n\nSolution\n\n\ntable(bechdel$binary)\n## \n## FAIL PASS \n##  991  803\n803/(991 + 803)\n## [1] 0.4476031\n\n\ntable(bechdel$clean_test)[1:4]/991\n## \n##   nowomen    notalk       men   dubious \n## 0.1422805 0.5186680 0.1957619 0.1432896\n\n\n\n\n\n\nQuantitative univariate visualization\nTo motivate quantitative visualizations, consider a second research question\n\nAmong the movies in our sample, what’s the range of budgets? What’s the typical budget? The largest/smallest?\n\nWe can answer the above research question by exploring the quantitative budget_2013 variable. Quantitative variables require different summary tools than categorical variables. We’ll explore two methods for graphing quantitative variables: histograms and density plots. Both of these has strengths/weaknesses in helping us visualize the distribution of observed values.\nIn their examination, keep your eyes on the following.\n\ncenter: Where’s the center of the distribution? What’s a typical value of the variable?\nvariability: How spread out are the values? A lot or a little?\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)?\n\noutliers: Are there any outliers, ie. values that are unusually large/small relative to the bulk of other values?\n\ncontextual implications: Interpret these features in the context of your research. How would you describe your findings to a broad audience?\n\n\nHistograms\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin.\n\nTry out the code below.  At each step determine how each piece of code contributes to the plot.    \n\n\n\n# plot 1: set up a plotting frame\nggplot(bechdel, aes(x = budget_2013))\n\n# plot 2: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram()\n\n# plot 3: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram() +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 4: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 5: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(fill = \"white\") +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 6: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\", binwidth = 500000) +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 7: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\", binwidth = 200000000) +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n\n\nSolution\n\n\n# plot 1: set up a plotting frame\nggplot(bechdel, aes(x = budget_2013))\n\n# plot 2: Added bars the represent the count of movies within budget intervals\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram()\n\n# plot 3: Updated the text on the x and y axis labels\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram() +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 4: The outline of the bars is now white\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 5: The fill of the bars is now white\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(fill = \"white\") +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 6: The width of the interval or bin is decreased to $500,000\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\", binwidth = 500000) +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n# plot 7: The width of the interval or bin is increased to $200,000,000\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_histogram(color = \"white\", binwidth = 200000000) +\n  labs(x = \"Budget ($)\", y = \"Number of movies\")\n\n\n\n\n\nSummarize the visualizations.    \n  \n  a. Describe the problem in choosing a bin width that's not too wide and not too narrow, but just right.    \n  b. What did you learn about the distribution of the `budget_2013` variable?    \n  c. Why does adding `color = \"white\"` improve the visualization?\n\n\n\n\nSolution\n\n\nIf the intervals (bars, bins) are too wide, then we lose information about the variation in the budget. Take it to the extreme with just 1 bar with the bar ranging from the minimum to the maximum. If the intervals are too small, then we have the frequency of the bars go up and down quite a bit. We might say that the shape of the bars isn’t very smooth.\nMost of the movies have small budgets; the majority less of budgets are less than $100,000,000 (in 2013 dollars) but there are some movies with upwards of $300,000,000 (in 2013 dollars).\nAdding the white outline to the bars adds contrast and helps the viewer see where each bar starts and ends.\n\n\n\n\n\n\nDensity plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting cases into discrete bins, the “density” of cases is calculated across the entire range of values. The greater the number of cases, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\n\nTry the following code and assess what each line does.\n\n\n\n# plot 1: set up the plotting frame\nggplot(bechdel, aes(x = budget_2013))\n\n# plot 2: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density()\n\n# plot 3: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density() +\n  labs(x = \"Budget ($)\")\n\n# plot 4: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density(color = \"red\") +\n  labs(x = \"Budget ($)\")\n\n# plot 5: what changed / how did we change it?\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density(fill = \"red\") +\n  labs(x = \"Budget ($)\")\n\n\n\nSolution\n\n\n# plot 1: set up the plotting frame\nggplot(bechdel, aes(x = budget_2013))\n\n# plot 2: add a smooth curve (shape of the histogram)\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density()\n\n# plot 3: updated the x axis label\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density() +\n  labs(x = \"Budget ($)\")\n\n# plot 4: changed the color of the curve to red\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density(color = \"red\") +\n  labs(x = \"Budget ($)\")\n\n# plot 5: filled the area under the curve to be red\nggplot(bechdel, aes(x = budget_2013)) +\n  geom_density(fill = \"red\") +\n  labs(x = \"Budget ($)\")\n\n\n\nThe histogram and density plot both allow us to visualize the distribution of a quantitative variable.  What are the pros/cons of each?  Discuss.",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#practice",
    "href": "src/02-Intro_Data_Viz.html#practice",
    "title": "(PART) Visualization",
    "section": "Practice",
    "text": "Practice\n\nIn July 2016, fivethirtyeight.com published the article [\"Hip-Hop is Turning on Donald Trump\"\"](https://projects.fivethirtyeight.com/clinton-trump-hip-hop-lyrics/).  You can find the supporting data table `hiphop_cand_lyrics` in the `fivethirtyeight` package:    \n  \n\n\nlibrary(fivethirtyeight)\ndata(hiphop_cand_lyrics)\n\n\nWhat are the cases in this data set?\n\nUse RStudio functions to:\n\n\n\nsummarize the number of cases in hiphop_cand_lyrics\n\nexamine the first cases of hiphop_cand_lyrics\n\nlist out the names of all variables in hiphop_cand_lyrics\n\n\nLet's start our investigation of hip hop data by asking \"Who?\"; that is, let's identify patterns in which 2016 presidential candidates popped up in hip hop lyrics.    \n  \n  a. Use an RStudio function to determine the category labels used for the `candidate` variable.    \n  b. Use `table` to construct a table of the number of cases that fall into each `candidate` category.    \n  c. Construct a single plot that allows you to investigate the prevalence of each candidate in hip hop.  Make the following modifications:    \n    - change the axis labels    \n    - change the fill colors    \n  d. Summarize your findings about the 2016 candidates in hip hop.\n        \n\n\nNext, consider the release dates of the hip hop songs.    \n  \n  a. Construct a histogram of the release dates with the following modifications:    \n    - change the fill color of the bins    \n    - change the bin width to a meaningful size    \n  b. Construct a density plot of the release dates with the following modifications:    \n    - change the fill color    \n  c. Summarize your findings about release date\n\n\n\nNo class will teach you everything you need to know about RStudio or programming in general. Thus, being able to find help online is an important skill.  To this end, make a single visualization that incorporates the following modifications to your density plot from above.  This will require a little Googling and/or use of the visualization cheat sheet.    \n\n  - Add a title or caption.    \n  - Add *transparency* to the fill color.    \n  - Calculate the mean (ie. average) release date and median release date:\n\n\n\nmean(hiphop_cand_lyrics$album_release_date)\nmedian(hiphop_cand_lyrics$album_release_date)\n\nAdd two vertical lines to your plot: one representing the mean and the other representing the median. Use two different colors and/or line types.\n\nChange the limits of the x-axis to range from 1980-2020.",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/02-Intro_Data_Viz.html#appendix-r-functions",
    "href": "src/02-Intro_Data_Viz.html#appendix-r-functions",
    "title": "(PART) Visualization",
    "section": "Appendix: R Functions",
    "text": "Appendix: R Functions\n\nBasic R functions\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\ntable(x)\nFrequency count of categories in x\ntable(bechdel$clean_test)\n\n\nmean(x)\nAverage or mean of numeric values in x\nmean(bechdel$budget_2013)\n\n\nmedian(x)\nMedian of numeric values in x\nmedian(bechdel$budget_2013)\n\n\n\n\n\nggplot2 foundation functions\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nggplot(data)\nCreate a blank canvas that can create a visualization based on data\nggplot(data = bechdel)\n\n\nggplot(data,aes())\nCreate a blank canvas that can create a visualization based on data with aesthetic mapping\nggplot(data = bechdel, aes(x = budget_2013))\n\n\n+ geom_bar(aes(x))\nAdd a bar plot\ngeom_bar(aes(x = clean_test))\n\n\n+ geom_point(aes(x,y))\nAdd a scatterplot\ngeom_bar(aes(x = year,y=budget_2013))\n\n\n+ geom_histogram(aes(x))\nAdd a histogram\ngeom_histogram(aes(x = budget_2013))\n\n\n+ geom_density(aes(x))\nAdd a density plot\ngeom_density(aes(x = budget_2013))\n\n\n\n\n\nmore ggplot2 functions\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\n+ xlab()\nAdd an label for the x-axis\nxlab('X axis')\n\n\n+ ylab()\nAdd an label for the y-axis\nylab('Y axis')\n\n\n+ labs(x,y)\nAdd labels for the x and y-axis\nlabs(y = 'Y axis', x = 'X axis')\n\n\n+ scale_color_manual()\nSet a color palette for the color aesthetic\nscale_color_manual(values = c('blue','red'))\n\n\n+ facet_grid()\nCreate subplots based on categorical variables, groupvar_along_yaxis ~ groupvar_along_xaxis\n+ facet_grid(. ~ smoker)\n\n\n\n\n\nggplot2 aesthetic mapping options\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nx\nvariable for x-axis\naes(x = clean_test)\n\n\ny\nvariable for y-axis\naes(y = budget_2013)\n\n\ncolor\nvariable for colors of points or strokes/outline\naes(color = clean_test)\n\n\nfill\nvariable for fill of bars or shapes\naes(fill = clean_test)\n\n\nsize\nvariable for size shapes\naes(size = budget_2013)\n\n\nshape\nvariable for shape type\naes(shape = clean_test)",
    "crumbs": [
      "Src",
      "(PART) Visualization"
    ]
  },
  {
    "objectID": "src/04-Bivariate_Viz.html",
    "href": "src/04-Bivariate_Viz.html",
    "title": "Bivariate Visualizations",
    "section": "",
    "text": "Identify appropriate types of bivariate visualizations, depending on the type of variables (categorical, quantitative)\nCreate basic bivariate visualizations based on real data\n\nYou can download a template .Rmd of this activity here. Put the file in the existing Assignment_03 folder within your COMP_STAT_112 folder.\n\n\n\nYou should write alt text for every visualization to create.\nFrom the last activity: Alt text should concisely articulate (1) what your visualization is (e.g. a bar chart showing which the harvest rate of cucumbers), (2) a one sentence description of the what you think is the most important takeaway your visualization is showing, and (3) a link to your data source if it’s not already in the caption (check out this great resource on writing alt text for data visualizations).\nTo add the alt text to your the HTML created from knitting the Rmd, you can include it as an option at the top of your R chunk. For example: {r, fig.alt = “Bar chart showing the daily harvest of cucumbers. The peak cucumber collection day is August 18th”}. In this activity, there will be prompts in the template Rmd but you should try to continue doing this for future assignments.\n\n\n\nThe outcome of the 2016 presidential election surprised many people. In this activity we will analyze data from the 2016 presidential election. To better understand it ourselves, we’ll explore county-level election outcomes and demographics. The data set, prepared by Alicia Johnson, combines 2008/2012/2016 county-level election returns from Tony McGovern on github, county-level demographics from the df_county_demographics data set within the choroplethr R package, and red/purple/blue state designations from http://www.270towin.com/.\n\n\n\nBegin by loading the [election data](data/electionDemographics16.csv) from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\" and getting to know the data. Write out R functions to get to know the data using the prompts below to guide you.\n\n\n# Load data from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\"\nelect &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\")\n\n# Check out the first rows of elect.  What are the units of observation?\n\n\n# How much data do we have?\n\n\n# What are the names of the variables?\n\n\n\nSolution\n\n\n# Load data from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\"\nelect &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\")\n## Error in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = &lt;pointer: 0x0000000000000266&gt;), : Recv failure: Connection was reset\n\n# Check out the first rows of elect.\n# The units of observation are county election results\n#  The variables are county name, vote counts for parties and total for presidential elections, and more\nhead(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n# There are 3,112 counties and 34 variables\ndim(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n# See the long list below\nnames(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nExplore the win column:\n    The `winrep_2016` variable indicates whether or not the Republican (Trump) won the county in 2016, thus is *categorical*.  Let's construct both numerical and visual summaries of Trump wins/losses.  (Before you do, what do you anticipate?) \n\n\n# Construct a table (a numerical summary) of the number of counties that Trump won/lost\ntable(xxx) # fill in the xxx\n\n# Attach a library needed for ggplots\nlibrary(xxx)\n\n# Construct a bar chart (a visual summary) of this variable.\nggplot(xxx, aes(xxx)) +\n  geom_xxx()\n\n\n\nSolution\n\n\n# Construct a table (a numerical summary) of the number of counties that Trump won/lost\ntable(elect$winrep_2016)\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# Attach a library needed for ggplots\nlibrary(ggplot2)\n\n\n# Construct a bar chart (a visual summary) of this variable.\nggplot(elect, aes(x = winrep_2016)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\nThe `perrep_2016` variable includes a bit more detail about Trump's support in each county.    \n\n\n\nSince it’s quantitative we need different tools to visually explore the variability in perrep_2016. To this end, construct & interpret both a histogram and density plot of perrep_2016. (Before you do, what do you anticipate?)\n\n\n# histogram\nggplot(elect, aes(xxx)) +\n  geom_xxx(color = \"white\")\n\n# density plot\nggplot(elect, aes(xxx)) +\n  geom_xxx()\n\n\n\nSolution\n\n\n# histogram\nggplot(elect, aes(x = perrep_2016)) +\n  geom_histogram(color = \"white\")\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# density plot\nggplot(elect, aes(x = perrep_2016)) +\n  geom_density()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nThe vast majority of counties in the U.S. had a Republican majority vote (&gt; 50%) within that county.\n\n\nThus far, we have a good sense for how Trump’s support varied from county to county. We don’t yet have a good sense for why. What other variables (ie. county features) might explain some of the variability in Trump’s support from county to county? Which of these variables do you think will be the best predictors of support? The worst?\n\n\n\nSolution\n\nMaybe past election history and information about the people that live there and the social culture and values. Let’s see…\n\n\n\n\nWe’ve come up with a list of variables that might explain some of the variability in Trump’s support from county to county. Thus we’re interested in the relationship between:\n\nresponse variable: the variable whose variability we would like to explain (Trump’s percent of the vote)\n\npredictors: variables that might explain some of the variability in the response (percent white, per capita income, state color, etc)\n\nOur goal is to construct visualizations that allow us to examine/identify the following features of the relationships among these variables:\n\nrelationship trends (direction and form)\n\nrelationship strength (degree of variability from the trend)\n\noutliers in the relationship\n\nBefore constructing visualizations of the relationship among any set of these variables, we need to understand what features these should have. As with univariate plots, the appropriate visualization also depends upon whether the variables are quantitative or categorical.\nRecall some basic rules in constructing graphics:\n\nEach quantitative variable requires a new axis. (We’ll discuss later what to do when we run out of axes!)\n\nEach categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping)\n\nFor visualizations in which overlap in glyphs or plots obscures the patterns, try faceting or transparency.\n\n\nConsider a subset  of the variables: \n\n\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'fd' not found\n\n\nIn groups, sketch on paper a mock-up of a visualization of the relationship between the given pair of variables (i.e., what type of chart is appropriate to demonstrate the relationship?):\n\nThe relationship between perrep_2016 (the response) and perrep_2012 (the predictor).\nThe relationship between perrep_2016 (the response) and StateColor (the predictor). Think: how might we modify the below density plot of perrep_2016 to distinguish between counties in red/purple/blue states?\n\n\nggplot(elect, aes(x = perrep_2016)) +\n  geom_density()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\nThe relationship between Trump’s county-levels wins/losses winrep_2016 (the response) and StateColor (the predictor). Think: how might we modify the below bar plot of winrep_2016 to distinguish between counties in red/purple/blue states?\n\n\nggplot(elect, aes(x = winrep_2016)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nLet’s start by exploring the relationship between Trump’s 2016 support (perrep_2016) and Romney’s 2012 support (perrep_2012), both quantitative variables.\n\nBoth `perrep_2016` and `perrep_2012` are quantitative, thus require their own axes.  Traditionally, the response variable (what we are trying to predict or explain) is placed on the y-axis.  Once the axes are set up, each case is represented by a \"glyph\" at the coordinates defined by these axes.    \n\n\n\nMake a scatterplot of perrep_2016 vs perrep_2012 with different glyphs: points or text.\n\n\n# just a graphics frame\nggplot(elect, aes(y = perrep_2016, x = perrep_2012))\n\n# add a layer with \"point\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point()\n\n# add a layer with symbol glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(shape = 3)\n\n# add a layer with \"text\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_text(aes(label = abb))\n\n\n\nSolution\n\n\n# just a graphics frame\nggplot(elect, aes(y = perrep_2016, x = perrep_2012))\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with \"point\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with symbol glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(shape = 3)\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with \"text\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_text(aes(label = abb))\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\nSummarize the relationship between the Republican candidates’ support in 2016 and 2012. Be sure to comment on:\n- the strength of the relationship (weak/moderate/strong)\n- the direction of the relationship (positive/negative)\n- outliers (In what state do counties deviate from the national trend? Explain why this might be the case)\n\n\n\nSolution\n\nThere is a strong positive relationship between the Republican support from 2012 to 2016, meaning that if a county highly favors a Republican candidate in 2012, they were likely to highly favor a Republican in 2016. Counties in Utah seems to not quite follow this pattern with lower support in 2016 than what you’d expect given the support in 2012. This is because the 2012 candidate was from Utah (data context!).\n\n\n\n\nThe trend of the relationship between `perrep_2016` and `perrep_2012` is clearly positive and (mostly) linear.  We can highlight this trend by adding a model \"smooth\" to the plot.    \n\n\n\nAdd a layer with a model smooth:\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth()\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n \n\nConstruct a new plot that contains the model smooth but does not include the individual cases (eg: point glyphs).\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_smooth()\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nNotice that there are gray bands surrounding the blue model smooth line. What do these gray bars illustrate/capture and why are they widest at the “ends” of the model?\n\n\n\nSolution\n\nThere are fewer data points at the “ends” so there is more uncertainty about the relationship.\n\n\nBy default, geom_smooth adds a smooth, localized model line. To examine the “best” linear model, we can specify method=\"lm\":\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\nAs with univariate plots, we can change the aesthetics of scatterplots.    \n\n\n\nAdd appropriate axis labels to your scatterplot. Label the y-axis “Trump 2016 support (%)” and label the x-axis “Romney 2012 support (%)”.\n\nChange the color of the points.\n\nAdd some transparency to the points. NOTE: alpha can be between 0 (complete transparency) and 1 (no transparency).\n\nWhy is transparency useful in this particular graphic?\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(color = \"red\", alpha = 0.1) +\n  labs(x = \"Romney 2012 support (%)\", y = \"Trump 2016 support (%)\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\n2012 results aren't the only possible predictor of 2016 results.  Consider two more possibilities.    \n\n\n\nConstruct a scatterplot of perrep_2016 and median_rent. Summarize the relationship between these two variables.\n\nConstruct a scatterplot of perrep_2016 and percent_white. Summarize the relationship between these two variables.\n\nAmong perrep_2012, median_rent and percent_white, which is the best predictor of perrep_2016? Why?\n\n\n\n\nConsider a univariate histogram and density plot of perrep_2016:\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'g1' not found\n\n\nTo visualize the relationship between Trump’s 2016 support (perrep_2016) and the StateColor (categorical) we need to incorporate a grouping mechanism. Work through the several options below.\n\nWe can show density plots for each state color next to each other:\n\n\nConstruct a density plot for each group.\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density()\n\n\nNotice that ggplot randomly assigns colors to group based on alphabetical order. In this example, the random color doesn’t match the group itself (red/purple/blue)! We can fix this:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\nThe overlap between the groups makes it difficult to explore the features of each. One option is to add transparency to the density plots:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\nYet another option is to separate the density plots into separate “facets” defined by group:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ StateColor)\n\n\nLet's try a similar strategy using histograms to illustrate the relationship between `perrep_2016` and `StateColor`.    \n\n\nStart with the default histogram:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\ngeom_histogram(color = \"white\")\n\n\nThat’s not very helpful! Separate the histograms into separate facets for each StateColor group.\n\n\nDensity plots and histograms aren't the only type of viz we might use...    \n\n\nConstruct side-by-side violins and side-by-side boxplots (see description below).\n\n\n# violins instead\nggplot(elect, aes(y = perrep_2016, x = StateColor)) +\n  geom_violin()\n\n# boxes instead\nggplot(elect, aes(y = perrep_2016, x = StateColor)) +\n  geom_boxplot()\n\nBox plots are constructed from five numbers - the minimum, 25th percentile, median, 75th percentile, and maximum value of a quantitative variable:\n\n\nError in knitr::include_graphics(\"images/Boxplot.png\"): Cannot find the file(s): \"images/Boxplot.png\"\n\n\n\nIn the future, we’ll typically use density plots instead of histograms, violins, and boxes. Explain at least one pro and one con of the density plot.\n\n\nLet's not forget the most important purpose of these visualizations!  Summarize the relationship between Trump's 2016 county-level support among red/purple/blue states.  \n\n\n\n\n\nFinally, suppose that instead of Trump’s percentage support, we simply want to explore his county-level wins/losses:\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nSpecifically, let’s explore the relationship between winrep_2016 and StateColor, another categorical variable.\n\nWe saw above that we can incorporate a new categorical variable into a visualization by using grouping features such as color or facets.  Let's add information about `StateColor` to our bar plot of `winrep_2016`.    \n\n\n\nConstruct the following 4 bar plot visualizations.\n::: {.cell}\n# a stacked bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar()\n\n# a side-by-side bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar(position = \"dodge\")\n\n# a proportional bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar(position = \"fill\")\n\n# faceted bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar() +\n  facet_wrap(~winrep_2016)\n:::\nName one pro and one con of using the “proportional bar plot” instead of one of the other three options.\nWhat’s your favorite bar plot from part (a)? Why?\n\n\n\n\n\n\nIn the annual Nathan’s hot dog eating contest, people compete to eat as many hot dogs as possible in ten minutes. Data on past competitions were compiled by Nathan Yau for “Visualize This: The FlowingData Guide to Design, Visualization, and Statistics”:\n\nhotdogs &lt;- read_csv(\"http://datasets.flowingdata.com/hot-dog-contest-winners.csv\")\n\n\nAddress the following:\n   \na. Construct a visualization of the winning number of hot dogs by year. THINK: Which is the response variable?      \nb. Temporal trends are often visualized using a line plot.  Add a `geom_line()` layer to your plot from part (a).       \nc. Summarize your observations about the temporal trends in the hot dog contest.    \n\n\n\nAll but two of the past winners are from the U.S. or Japan:\n\n\n\ntable(hotdogs$Country)\n\n\n      Germany         Japan        Mexico United States \n            1             9             1            20 \n\n\nUse the following code to filter out just the winners from U.S. and Japan and name this hotdogsSub. (Don’t worry about the code itself - we’ll discuss similar syntax later in the semester!)\n\nlibrary(dplyr)\nhotdogsSub &lt;- hotdogs %&gt;%\n  filter(Country %in% c(\"Japan\", \"United States\"))\n\n\nUsing a density plot approach without facets, construct a visualization of how the number of hot dogs eaten varies by country.\nRepeat part a using a density plot approach with facets.\n\nRepeat part a using something other than a density plot approach. (There are a few options!)\n\nSummarize your observations about the number of hot dogs eaten by country.\n\n\n\n\nRecall the “Bechdel test” data from the previous activity. As a reminder, the “Bechdel test” tests whether movies meet the following criteria:\n\nthere are \\(\\ge\\) 2 female characters\n\nthe female characters talk to each other\n\nat least 1 time, they talk about something other than a male character\n\nIn the fivethirtyeight.com article “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”, the authors analyze which Hollywood movies do/don’t pass the test. Their data are available in the fivethirtyeight package:\n\nlibrary(fivethirtyeight)\ndata(bechdel)\n\nIn investigating budgets and profits, the authors “focus on films released from 1990 to 2013, since the data has significantly more depth since then.” Use the following code to filter out just the movies in these years and name the resulting data set Beyond1990 (don’t worry about the syntax):\n\nlibrary(dplyr)\nBeyond1990 &lt;- bechdel %&gt;%\n  filter(year &gt;= 1990)\n\n\nAddress the following:\n  \na. Construct a visualization that addresses the following research question: Do bigger budgets (`budget_2013`) pay off with greater box office returns (`domgross_2013`)?  In constructing this visualization, add a smooth to highlight trends and pay attention to which of these variables is the response.       \nb. Using your visualization as supporting evidence, answer the research question.          \nc. Part of the fivethirtyeight article focuses on how budgets (`budget_2013`) differ among movies with different degrees of female character development (`clean_test`).  Construct a visualization that highlights the relationship between these two variables.  There are many options - some are better than others!       \nd. Using your visualization as supporting evidence, address fivethirtyeight's concerns.  \n\n\nNOTE: The following exercise is inspired by a similar exercise proposed by Albert Kim, one of the `fivethirtyeight` package authors.    \n    Return to the fivethirtyeight.com article and examine the plot titled \"The Bechdel Test Over Time\".    \n\n\nSummarize the trends captured by this plot. (How has the representation of women in movies evolved over time?)\n\nRecreate this plot from the article!\n\nTo do so, you’ll need to create a new data set named newbechdel in which the order of the Bechdel categories (clean_test) and the year categories (yearCat) match those used by fivethirtyeight. Don’t worry about the syntax:\n\nlibrary(dplyr)\nnewbechdel &lt;- bechdel %&gt;%\nmutate(clean_test = factor(bechdel$clean_test, c(\"nowomen\", \"notalk\", \"men\", \"dubious\", \"ok\"))) %&gt;%\nmutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\nFurther, you’ll need to add the following layer in order to get a color scheme that’s close to that in the article:\n\nscale_fill_manual(values = c(\"red\", \"salmon\", \"pink\", \"steelblue1\", \"steelblue4\"))\n\nNOTE: that your plot won’t look exactly like the authors’, but should be close to this:\n\n\nError in knitr::include_graphics(\"images/bechdel_hist.jpeg\"): Cannot find the file(s): \"images/bechdel_hist.jpeg\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nfilter(data,condition)\nProvide rows of a data set that satisfy a condition\nbechdel %&gt;% filter(year &gt;= 1990)\n\n\nmutate(data,varname =)\nCreate a new variable\nbechdel %&gt;% mutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\n\ncut(x,breaks)\nCut a quantitative variable into categories by the break points\nbechdel %&gt;% mutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nggplot(data)\nCreate a blank canvas that can create a visualization based on data\nggplot(data = elect)\n\n\n+ geom_bar(aes(x))\nAdd a bar plot\ngeom_bar(aes(x = winrep_2016))\n\n\n+ geom_bar(aes(x,fill),position='fill')\nAdd a propotional bar plot\ngeom_bar(aes(x = winrep_2016,fill = StateColor),position='fill')\n\n\n+ geom_bar(aes(x,fill),position='dodge')\nAdd a side-by-side bar plot\ngeom_bar(aes(x = winrep_2016,fill = StateColor),position='dodge')\n\n\n+ geom_smooth(aes(x,y))\nAdd a smoothed average curve of scatterplot\ngeom_smooth()\n\n\n+ geom_smooth(aes(x,y),method='lm')\nAdd a best fit line to a scatterplot\ngeom_smooth(method='lm')\n\n\n+ geom_point(aes(x,y))\nAdd a scatterplot\ngeom_bar(aes(x = year,y=budget_2013))\n\n\n+ geom_text(aes(x,y,label))\nAdd a text to a plot\ngeom_text(aes(label=abb))\n\n\n+ facet_wrap(~x)\nFacet a plot (break into subplots based on groups)\nfacet_wrap(~StateColor)",
    "crumbs": [
      "Src",
      "options(htmltools.dir.version = FALSE)"
    ]
  },
  {
    "objectID": "src/04-Bivariate_Viz.html#learning-goals",
    "href": "src/04-Bivariate_Viz.html#learning-goals",
    "title": "Bivariate Visualizations",
    "section": "",
    "text": "Identify appropriate types of bivariate visualizations, depending on the type of variables (categorical, quantitative)\nCreate basic bivariate visualizations based on real data\n\nYou can download a template .Rmd of this activity here. Put the file in the existing Assignment_03 folder within your COMP_STAT_112 folder.",
    "crumbs": [
      "Src",
      "options(htmltools.dir.version = FALSE)"
    ]
  },
  {
    "objectID": "src/04-Bivariate_Viz.html#alterative-text-for-visualizations",
    "href": "src/04-Bivariate_Viz.html#alterative-text-for-visualizations",
    "title": "Bivariate Visualizations",
    "section": "",
    "text": "You should write alt text for every visualization to create.\nFrom the last activity: Alt text should concisely articulate (1) what your visualization is (e.g. a bar chart showing which the harvest rate of cucumbers), (2) a one sentence description of the what you think is the most important takeaway your visualization is showing, and (3) a link to your data source if it’s not already in the caption (check out this great resource on writing alt text for data visualizations).\nTo add the alt text to your the HTML created from knitting the Rmd, you can include it as an option at the top of your R chunk. For example: {r, fig.alt = “Bar chart showing the daily harvest of cucumbers. The peak cucumber collection day is August 18th”}. In this activity, there will be prompts in the template Rmd but you should try to continue doing this for future assignments.",
    "crumbs": [
      "Src",
      "options(htmltools.dir.version = FALSE)"
    ]
  },
  {
    "objectID": "src/04-Bivariate_Viz.html#bivariate-visualizations-1",
    "href": "src/04-Bivariate_Viz.html#bivariate-visualizations-1",
    "title": "Bivariate Visualizations",
    "section": "",
    "text": "The outcome of the 2016 presidential election surprised many people. In this activity we will analyze data from the 2016 presidential election. To better understand it ourselves, we’ll explore county-level election outcomes and demographics. The data set, prepared by Alicia Johnson, combines 2008/2012/2016 county-level election returns from Tony McGovern on github, county-level demographics from the df_county_demographics data set within the choroplethr R package, and red/purple/blue state designations from http://www.270towin.com/.\n\n\n\nBegin by loading the [election data](data/electionDemographics16.csv) from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\" and getting to know the data. Write out R functions to get to know the data using the prompts below to guide you.\n\n\n# Load data from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\"\nelect &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\")\n\n# Check out the first rows of elect.  What are the units of observation?\n\n\n# How much data do we have?\n\n\n# What are the names of the variables?\n\n\n\nSolution\n\n\n# Load data from \"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\"\nelect &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/electionDemographics16.csv\")\n## Error in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = &lt;pointer: 0x0000000000000266&gt;), : Recv failure: Connection was reset\n\n# Check out the first rows of elect.\n# The units of observation are county election results\n#  The variables are county name, vote counts for parties and total for presidential elections, and more\nhead(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n# There are 3,112 counties and 34 variables\ndim(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n# See the long list below\nnames(elect)\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nExplore the win column:\n    The `winrep_2016` variable indicates whether or not the Republican (Trump) won the county in 2016, thus is *categorical*.  Let's construct both numerical and visual summaries of Trump wins/losses.  (Before you do, what do you anticipate?) \n\n\n# Construct a table (a numerical summary) of the number of counties that Trump won/lost\ntable(xxx) # fill in the xxx\n\n# Attach a library needed for ggplots\nlibrary(xxx)\n\n# Construct a bar chart (a visual summary) of this variable.\nggplot(xxx, aes(xxx)) +\n  geom_xxx()\n\n\n\nSolution\n\n\n# Construct a table (a numerical summary) of the number of counties that Trump won/lost\ntable(elect$winrep_2016)\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# Attach a library needed for ggplots\nlibrary(ggplot2)\n\n\n# Construct a bar chart (a visual summary) of this variable.\nggplot(elect, aes(x = winrep_2016)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\nThe `perrep_2016` variable includes a bit more detail about Trump's support in each county.    \n\n\n\nSince it’s quantitative we need different tools to visually explore the variability in perrep_2016. To this end, construct & interpret both a histogram and density plot of perrep_2016. (Before you do, what do you anticipate?)\n\n\n# histogram\nggplot(elect, aes(xxx)) +\n  geom_xxx(color = \"white\")\n\n# density plot\nggplot(elect, aes(xxx)) +\n  geom_xxx()\n\n\n\nSolution\n\n\n# histogram\nggplot(elect, aes(x = perrep_2016)) +\n  geom_histogram(color = \"white\")\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# density plot\nggplot(elect, aes(x = perrep_2016)) +\n  geom_density()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nThe vast majority of counties in the U.S. had a Republican majority vote (&gt; 50%) within that county.\n\n\nThus far, we have a good sense for how Trump’s support varied from county to county. We don’t yet have a good sense for why. What other variables (ie. county features) might explain some of the variability in Trump’s support from county to county? Which of these variables do you think will be the best predictors of support? The worst?\n\n\n\nSolution\n\nMaybe past election history and information about the people that live there and the social culture and values. Let’s see…\n\n\n\n\nWe’ve come up with a list of variables that might explain some of the variability in Trump’s support from county to county. Thus we’re interested in the relationship between:\n\nresponse variable: the variable whose variability we would like to explain (Trump’s percent of the vote)\n\npredictors: variables that might explain some of the variability in the response (percent white, per capita income, state color, etc)\n\nOur goal is to construct visualizations that allow us to examine/identify the following features of the relationships among these variables:\n\nrelationship trends (direction and form)\n\nrelationship strength (degree of variability from the trend)\n\noutliers in the relationship\n\nBefore constructing visualizations of the relationship among any set of these variables, we need to understand what features these should have. As with univariate plots, the appropriate visualization also depends upon whether the variables are quantitative or categorical.\nRecall some basic rules in constructing graphics:\n\nEach quantitative variable requires a new axis. (We’ll discuss later what to do when we run out of axes!)\n\nEach categorical variable requires a new way to “group” the graphic (eg: using colors, shapes, separate facets, etc to capture the grouping)\n\nFor visualizations in which overlap in glyphs or plots obscures the patterns, try faceting or transparency.\n\n\nConsider a subset  of the variables: \n\n\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'fd' not found\n\n\nIn groups, sketch on paper a mock-up of a visualization of the relationship between the given pair of variables (i.e., what type of chart is appropriate to demonstrate the relationship?):\n\nThe relationship between perrep_2016 (the response) and perrep_2012 (the predictor).\nThe relationship between perrep_2016 (the response) and StateColor (the predictor). Think: how might we modify the below density plot of perrep_2016 to distinguish between counties in red/purple/blue states?\n\n\nggplot(elect, aes(x = perrep_2016)) +\n  geom_density()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\nThe relationship between Trump’s county-levels wins/losses winrep_2016 (the response) and StateColor (the predictor). Think: how might we modify the below bar plot of winrep_2016 to distinguish between counties in red/purple/blue states?\n\n\nggplot(elect, aes(x = winrep_2016)) +\n  geom_bar()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nLet’s start by exploring the relationship between Trump’s 2016 support (perrep_2016) and Romney’s 2012 support (perrep_2012), both quantitative variables.\n\nBoth `perrep_2016` and `perrep_2012` are quantitative, thus require their own axes.  Traditionally, the response variable (what we are trying to predict or explain) is placed on the y-axis.  Once the axes are set up, each case is represented by a \"glyph\" at the coordinates defined by these axes.    \n\n\n\nMake a scatterplot of perrep_2016 vs perrep_2012 with different glyphs: points or text.\n\n\n# just a graphics frame\nggplot(elect, aes(y = perrep_2016, x = perrep_2012))\n\n# add a layer with \"point\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point()\n\n# add a layer with symbol glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(shape = 3)\n\n# add a layer with \"text\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_text(aes(label = abb))\n\n\n\nSolution\n\n\n# just a graphics frame\nggplot(elect, aes(y = perrep_2016, x = perrep_2012))\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with \"point\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with symbol glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(shape = 3)\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n# add a layer with \"text\" glyphs\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_text(aes(label = abb))\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\nSummarize the relationship between the Republican candidates’ support in 2016 and 2012. Be sure to comment on:\n- the strength of the relationship (weak/moderate/strong)\n- the direction of the relationship (positive/negative)\n- outliers (In what state do counties deviate from the national trend? Explain why this might be the case)\n\n\n\nSolution\n\nThere is a strong positive relationship between the Republican support from 2012 to 2016, meaning that if a county highly favors a Republican candidate in 2012, they were likely to highly favor a Republican in 2016. Counties in Utah seems to not quite follow this pattern with lower support in 2016 than what you’d expect given the support in 2012. This is because the 2012 candidate was from Utah (data context!).\n\n\n\n\nThe trend of the relationship between `perrep_2016` and `perrep_2012` is clearly positive and (mostly) linear.  We can highlight this trend by adding a model \"smooth\" to the plot.    \n\n\n\nAdd a layer with a model smooth:\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth()\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n \n\nConstruct a new plot that contains the model smooth but does not include the individual cases (eg: point glyphs).\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_smooth()\n## Error in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\nNotice that there are gray bands surrounding the blue model smooth line. What do these gray bars illustrate/capture and why are they widest at the “ends” of the model?\n\n\n\nSolution\n\nThere are fewer data points at the “ends” so there is more uncertainty about the relationship.\n\n\nBy default, geom_smooth adds a smooth, localized model line. To examine the “best” linear model, we can specify method=\"lm\":\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\nAs with univariate plots, we can change the aesthetics of scatterplots.    \n\n\n\nAdd appropriate axis labels to your scatterplot. Label the y-axis “Trump 2016 support (%)” and label the x-axis “Romney 2012 support (%)”.\n\nChange the color of the points.\n\nAdd some transparency to the points. NOTE: alpha can be between 0 (complete transparency) and 1 (no transparency).\n\nWhy is transparency useful in this particular graphic?\n\n\n\nSolution\n\n\nggplot(elect, aes(y = perrep_2016, x = perrep_2012)) +\n  geom_point(color = \"red\", alpha = 0.1) +\n  labs(x = \"Romney 2012 support (%)\", y = \"Trump 2016 support (%)\") + \n  theme_classic()\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\n\n\n\n\n2012 results aren't the only possible predictor of 2016 results.  Consider two more possibilities.    \n\n\n\nConstruct a scatterplot of perrep_2016 and median_rent. Summarize the relationship between these two variables.\n\nConstruct a scatterplot of perrep_2016 and percent_white. Summarize the relationship between these two variables.\n\nAmong perrep_2012, median_rent and percent_white, which is the best predictor of perrep_2016? Why?\n\n\n\n\nConsider a univariate histogram and density plot of perrep_2016:\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nError in eval(expr, envir, enclos): object 'g1' not found\n\n\nTo visualize the relationship between Trump’s 2016 support (perrep_2016) and the StateColor (categorical) we need to incorporate a grouping mechanism. Work through the several options below.\n\nWe can show density plots for each state color next to each other:\n\n\nConstruct a density plot for each group.\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density()\n\n\nNotice that ggplot randomly assigns colors to group based on alphabetical order. In this example, the random color doesn’t match the group itself (red/purple/blue)! We can fix this:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\nThe overlap between the groups makes it difficult to explore the features of each. One option is to add transparency to the density plots:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\nYet another option is to separate the density plots into separate “facets” defined by group:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ StateColor)\n\n\nLet's try a similar strategy using histograms to illustrate the relationship between `perrep_2016` and `StateColor`.    \n\n\nStart with the default histogram:\n\n\nggplot(elect, aes(x = perrep_2016, fill = StateColor)) +\ngeom_histogram(color = \"white\")\n\n\nThat’s not very helpful! Separate the histograms into separate facets for each StateColor group.\n\n\nDensity plots and histograms aren't the only type of viz we might use...    \n\n\nConstruct side-by-side violins and side-by-side boxplots (see description below).\n\n\n# violins instead\nggplot(elect, aes(y = perrep_2016, x = StateColor)) +\n  geom_violin()\n\n# boxes instead\nggplot(elect, aes(y = perrep_2016, x = StateColor)) +\n  geom_boxplot()\n\nBox plots are constructed from five numbers - the minimum, 25th percentile, median, 75th percentile, and maximum value of a quantitative variable:\n\n\nError in knitr::include_graphics(\"images/Boxplot.png\"): Cannot find the file(s): \"images/Boxplot.png\"\n\n\n\nIn the future, we’ll typically use density plots instead of histograms, violins, and boxes. Explain at least one pro and one con of the density plot.\n\n\nLet's not forget the most important purpose of these visualizations!  Summarize the relationship between Trump's 2016 county-level support among red/purple/blue states.  \n\n\n\n\n\nFinally, suppose that instead of Trump’s percentage support, we simply want to explore his county-level wins/losses:\n\n\nError in eval(expr, envir, enclos): object 'elect' not found\n\n\nSpecifically, let’s explore the relationship between winrep_2016 and StateColor, another categorical variable.\n\nWe saw above that we can incorporate a new categorical variable into a visualization by using grouping features such as color or facets.  Let's add information about `StateColor` to our bar plot of `winrep_2016`.    \n\n\n\nConstruct the following 4 bar plot visualizations.\n::: {.cell}\n# a stacked bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar()\n\n# a side-by-side bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar(position = \"dodge\")\n\n# a proportional bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar(position = \"fill\")\n\n# faceted bar plot\nggplot(elect, aes(x = StateColor, fill = winrep_2016)) +\n  geom_bar() +\n  facet_wrap(~winrep_2016)\n:::\nName one pro and one con of using the “proportional bar plot” instead of one of the other three options.\nWhat’s your favorite bar plot from part (a)? Why?\n\n\n\n\n\n\nIn the annual Nathan’s hot dog eating contest, people compete to eat as many hot dogs as possible in ten minutes. Data on past competitions were compiled by Nathan Yau for “Visualize This: The FlowingData Guide to Design, Visualization, and Statistics”:\n\nhotdogs &lt;- read_csv(\"http://datasets.flowingdata.com/hot-dog-contest-winners.csv\")\n\n\nAddress the following:\n   \na. Construct a visualization of the winning number of hot dogs by year. THINK: Which is the response variable?      \nb. Temporal trends are often visualized using a line plot.  Add a `geom_line()` layer to your plot from part (a).       \nc. Summarize your observations about the temporal trends in the hot dog contest.    \n\n\n\nAll but two of the past winners are from the U.S. or Japan:\n\n\n\ntable(hotdogs$Country)\n\n\n      Germany         Japan        Mexico United States \n            1             9             1            20 \n\n\nUse the following code to filter out just the winners from U.S. and Japan and name this hotdogsSub. (Don’t worry about the code itself - we’ll discuss similar syntax later in the semester!)\n\nlibrary(dplyr)\nhotdogsSub &lt;- hotdogs %&gt;%\n  filter(Country %in% c(\"Japan\", \"United States\"))\n\n\nUsing a density plot approach without facets, construct a visualization of how the number of hot dogs eaten varies by country.\nRepeat part a using a density plot approach with facets.\n\nRepeat part a using something other than a density plot approach. (There are a few options!)\n\nSummarize your observations about the number of hot dogs eaten by country.\n\n\n\n\nRecall the “Bechdel test” data from the previous activity. As a reminder, the “Bechdel test” tests whether movies meet the following criteria:\n\nthere are \\(\\ge\\) 2 female characters\n\nthe female characters talk to each other\n\nat least 1 time, they talk about something other than a male character\n\nIn the fivethirtyeight.com article “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”, the authors analyze which Hollywood movies do/don’t pass the test. Their data are available in the fivethirtyeight package:\n\nlibrary(fivethirtyeight)\ndata(bechdel)\n\nIn investigating budgets and profits, the authors “focus on films released from 1990 to 2013, since the data has significantly more depth since then.” Use the following code to filter out just the movies in these years and name the resulting data set Beyond1990 (don’t worry about the syntax):\n\nlibrary(dplyr)\nBeyond1990 &lt;- bechdel %&gt;%\n  filter(year &gt;= 1990)\n\n\nAddress the following:\n  \na. Construct a visualization that addresses the following research question: Do bigger budgets (`budget_2013`) pay off with greater box office returns (`domgross_2013`)?  In constructing this visualization, add a smooth to highlight trends and pay attention to which of these variables is the response.       \nb. Using your visualization as supporting evidence, answer the research question.          \nc. Part of the fivethirtyeight article focuses on how budgets (`budget_2013`) differ among movies with different degrees of female character development (`clean_test`).  Construct a visualization that highlights the relationship between these two variables.  There are many options - some are better than others!       \nd. Using your visualization as supporting evidence, address fivethirtyeight's concerns.  \n\n\nNOTE: The following exercise is inspired by a similar exercise proposed by Albert Kim, one of the `fivethirtyeight` package authors.    \n    Return to the fivethirtyeight.com article and examine the plot titled \"The Bechdel Test Over Time\".    \n\n\nSummarize the trends captured by this plot. (How has the representation of women in movies evolved over time?)\n\nRecreate this plot from the article!\n\nTo do so, you’ll need to create a new data set named newbechdel in which the order of the Bechdel categories (clean_test) and the year categories (yearCat) match those used by fivethirtyeight. Don’t worry about the syntax:\n\nlibrary(dplyr)\nnewbechdel &lt;- bechdel %&gt;%\nmutate(clean_test = factor(bechdel$clean_test, c(\"nowomen\", \"notalk\", \"men\", \"dubious\", \"ok\"))) %&gt;%\nmutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\nFurther, you’ll need to add the following layer in order to get a color scheme that’s close to that in the article:\n\nscale_fill_manual(values = c(\"red\", \"salmon\", \"pink\", \"steelblue1\", \"steelblue4\"))\n\nNOTE: that your plot won’t look exactly like the authors’, but should be close to this:\n\n\nError in knitr::include_graphics(\"images/bechdel_hist.jpeg\"): Cannot find the file(s): \"images/bechdel_hist.jpeg\"",
    "crumbs": [
      "Src",
      "options(htmltools.dir.version = FALSE)"
    ]
  },
  {
    "objectID": "src/04-Bivariate_Viz.html#appendix-r-functions",
    "href": "src/04-Bivariate_Viz.html#appendix-r-functions",
    "title": "Bivariate Visualizations",
    "section": "",
    "text": "Function/Operator\nAction\nExample\n\n\n\n\nfilter(data,condition)\nProvide rows of a data set that satisfy a condition\nbechdel %&gt;% filter(year &gt;= 1990)\n\n\nmutate(data,varname =)\nCreate a new variable\nbechdel %&gt;% mutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\n\ncut(x,breaks)\nCut a quantitative variable into categories by the break points\nbechdel %&gt;% mutate(yearCat = cut(year, breaks = seq(1969, 2014, by = 5)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nggplot(data)\nCreate a blank canvas that can create a visualization based on data\nggplot(data = elect)\n\n\n+ geom_bar(aes(x))\nAdd a bar plot\ngeom_bar(aes(x = winrep_2016))\n\n\n+ geom_bar(aes(x,fill),position='fill')\nAdd a propotional bar plot\ngeom_bar(aes(x = winrep_2016,fill = StateColor),position='fill')\n\n\n+ geom_bar(aes(x,fill),position='dodge')\nAdd a side-by-side bar plot\ngeom_bar(aes(x = winrep_2016,fill = StateColor),position='dodge')\n\n\n+ geom_smooth(aes(x,y))\nAdd a smoothed average curve of scatterplot\ngeom_smooth()\n\n\n+ geom_smooth(aes(x,y),method='lm')\nAdd a best fit line to a scatterplot\ngeom_smooth(method='lm')\n\n\n+ geom_point(aes(x,y))\nAdd a scatterplot\ngeom_bar(aes(x = year,y=budget_2013))\n\n\n+ geom_text(aes(x,y,label))\nAdd a text to a plot\ngeom_text(aes(label=abb))\n\n\n+ facet_wrap(~x)\nFacet a plot (break into subplots based on groups)\nfacet_wrap(~StateColor)",
    "crumbs": [
      "Src",
      "options(htmltools.dir.version = FALSE)"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#learning-goals",
    "href": "src/07-Six_Main_Verbs.html#learning-goals",
    "title": "(PART) Data Wrangling",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nUnderstand and be able to use the following verbs appropriate: select, mutate, filter, arrange, summarize, group_by\nDevelop an understanding what code will do conceptually without running it\nDevelop working knowledge of working with dates and lubridate functions\n\nYou can download a template .Rmd of this activity here. Add it to a folder called Assignment_05 in your COMP_STAT_112 folder.",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#data-wrangling-introduction",
    "href": "src/07-Six_Main_Verbs.html#data-wrangling-introduction",
    "title": "(PART) Data Wrangling",
    "section": "Data Wrangling Introduction",
    "text": "Data Wrangling Introduction\n\nExample: US Births\nThe number of daily births in the US varies over the year and from day to day. What’s surprising to many people is that the variation from one day to the next can be huge: some days have only about 80% as many births as others. Why? In this activity we’ll use basic data wrangling skills to understand some drivers of daily births.\nThe data table Birthdays in the mosaicData package gives the number of births recorded on each day of the year in each state from 1969 to 1988.1\n\n\n\nA subset of the initial birthday data.\n\n\nstate\ndate\nyear\nbirths\n\n\n\n\nAK\n1969-01-01\n1969\n14\n\n\nAL\n1969-01-01\n1969\n174\n\n\nAR\n1969-01-01\n1969\n78\n\n\nAZ\n1969-01-01\n1969\n84\n\n\nCA\n1969-01-01\n1969\n824\n\n\nCO\n1969-01-01\n1969\n100\n\n\n\n\n\n\n\nTidy Data\nAdditional reading:\n\nWickham, Tidy Data\nWickham and Grolemund, Tidy Data\nBaumer, Kaplan, and Horton, Tidy Data\n\nThere are different ways to store and represent the same data. In order to be consistent and to also take advantage of the vectorized nature of R, the tidyverse packages we’ll use provide a set of three interrelated rules/conventions for a dataset to be tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nOne of the first things we’ll often do when acquiring new data is to “tidy it” into this form. For now, we can already start thinking of a data frame (tibble) as a table whose rows are the individual cases and whose columns are the variables on which we have information for each individual case. The first figure from the tidyr cheat sheet summarizes this principle.\n\n\nData Verbs\nAdditional reading:\n\nWickham and Grolemund, Data Transformation\nBaumer, Kaplan, and Horton, A Grammar for Data Wrangling\n\nThere are six main data transformation verbs in the dplyr library. Each verb takes an input data frame along with additional arguments specifying the action, and returns a new data frame. We’ll examine them in three pairs.\n\nVerbs that change the variables (columns) but not the cases (rows)\nThe first two verbs change which variables (columns) are included in the data frame, but preserve the same set of cases (rows).\n\nselect() chooses which columns to keep, or put another way, deletes those colummns that are not selected. To specify the columns, we can either list them out, or use functions like starts_with(), ends_with(), or contains() to specify the titles of the variables we wish to keep.\nmutate() adds one or more columns to the data frame. Each column is a function of the other columns that is applied on a row by row basis. For example, we can use arithmetic operations like adding two other variables or logical operations like checking if two columns are equal, or equal to a target number.\n\n\nConsider the `Birthdays` data\n\n\nAdd two new variables to the Birthdays data: one that has only the last two digits of the year, and one that states whether there were more than 100 births in the given state on the given date.\n\nThen form a new table that only has three columns: the state and your two new columns.\n\nWhat does the following operation return (describe the output): select(Birthdays, ends_with(\"te\"))?\n\n\n\nSolution\n\nThe commands for the first two parts are\n\nBirthdaysExtra &lt;- mutate(Birthdays,\n  year_short = year - 1900,\n  busy_birthday = (births &gt; 100)\n)\n\nBirthdaysExtraTable &lt;- select(\n  BirthdaysExtra, state,\n  year_short, busy_birthday\n)\n\nselect(Birthdays, ends_with(\"te\")) %&gt;% head()\n\n  state       date\n1    AK 1969-01-01\n2    AL 1969-01-01\n3    AR 1969-01-01\n4    AZ 1969-01-01\n5    CA 1969-01-01\n6    CO 1969-01-01\n\n\nThe operation in (c) provides a data set with only the two columns state and date because they end in ‘te’.\n\n\n\n\n\nVerbs that change the cases (rows) but not the variables (columns)\nThe next two verbs change which cases (rows) are included in the data frame, but preserve the same set of variables (columns).\n\nfilter() deletes some of the rows by specifying which rows to keep.\narrange() reorders the rows according to a specified criteria. To sort in reverse order based on the variable x, use arrange(desc(x)).\n\n\nCreate a table with only births in Massachusetts in 1979, and sort the days from those with the most births to those with the fewest.\n\n\n\n\nSolution\n\nWe want to filter and then arrange:\n\nMABirths1979 &lt;- filter(Birthdays, state == \"MA\", year == 1979)\nMABirths1979Sorted &lt;- arrange(MABirths1979, desc(births))\n\n\n\n\nBirthdays in Massachusetts in 1979, sorted from those dates with the most births to those dates with the fewest births.\n\n\nstate\ndate\nyear\nbirths\n\n\n\n\nMA\n1979-09-28\n1979\n262\n\n\nMA\n1979-09-11\n1979\n252\n\n\nMA\n1979-12-28\n1979\n249\n\n\nMA\n1979-09-26\n1979\n246\n\n\nMA\n1979-07-24\n1979\n245\n\n\nMA\n1979-04-27\n1979\n243\n\n\n\n\n\n\n\n\nWhen filtering, we often use logical comparison operators like ==, &gt;, &lt;, &gt;= (greater than or equal to), &lt;= (less than or equal to), and %in%, which compares the value to a list of entries.2 For example, if we want all births in AK, CA, and MA, we can write\nfilter(Birthdays, state %in% c(\"AK\",\"CA\",\"MA\"))\nThe c() here is for concatenate, which is how we form vectors in R.\n\n\nGrouped summaries\n\nsummarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics, such as mean(), sum(), sd(), n_distinct(), or n() (which just counts the number of entries/rows).\n\n\nsummarize(Birthdays,\n  total_births = sum(births),\n  average_births = mean(births),\n  nstates = n_distinct(state), ncases = n()\n)\n\n  total_births average_births nstates ncases\n1     70486538       189.0409      51 372864\n\n\nSo summarize changes both the cases and the variables. Alone, summarize is not all that useful, because we can also access individual variables directly with the dollar sign. For example, to find the total and average births, we can write\n\nsum(Birthdays$births)\n\n[1] 70486538\n\nmean(Birthdays$births)\n\n[1] 189.0409\n\n\nRather, we will mostly use it to create grouped summaries, which brings us to the last of the six main data verbs.\n\ngroup_by() groups the cases of a data frame by a specified set of variables. The size of the stored data frame does not actually change (neither the cases nor the variables change), but then other functions can be applied to the specified groups instead of the entire data set. We’ll often use group_by in conjunction with summarize to get a grouped summary.\n\n\nConsider the `Birthdays` data again.\n\n\nFind the average number of daily births in each year (average across states).\nFind the average number of daily births in each year, by state.\n\n\n\nSolution\n\nWe have to first group by the desired grouping and then perform a summarize.\n\nBirthdaysYear &lt;- group_by(Birthdays, year)\nsummarize(BirthdaysYear, average = mean(births))\n\n# A tibble: 20 × 2\n    year average\n   &lt;int&gt;   &lt;dbl&gt;\n 1  1969    192.\n 2  1970    200.\n 3  1971    191.\n 4  1972    175.\n 5  1973    169.\n 6  1974    170.\n 7  1975    169.\n 8  1976    170.\n 9  1977    179.\n10  1978    179.\n11  1979    188.\n12  1980    194.\n13  1981    195.\n14  1982    198.\n15  1983    196.\n16  1984    197.\n17  1985    202.\n18  1986    202.\n19  1987    205.\n20  1988    210.\n\nBirthdaysYearState &lt;- group_by(Birthdays, year, state)\nsummarize(BirthdaysYearState, average = mean(births))\n\n# A tibble: 1,020 × 3\n# Groups:   year [20]\n    year state average\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  1969 AK       18.6\n 2  1969 AL      174. \n 3  1969 AR       91.3\n 4  1969 AZ       93.3\n 5  1969 CA      954. \n 6  1969 CO      110. \n 7  1969 CT      134. \n 8  1969 DC       75.3\n 9  1969 DE       27.6\n10  1969 FL      292. \n# ℹ 1,010 more rows\n\n\n\n\n\n\n\n\nPiping\nAdditional reading:\n\nWickham and Grolemund, Combining Multiple Operations with the Pipe\nWickham and Grolemund, Pipes\n\nPipes offer an efficient way to execute multiple operations at once. Here is a more efficient way to redo Example 7.2 with the pipe:\n\nQuickMABirths1979 &lt;-\n  Birthdays %&gt;%\n  filter(state == \"MA\", year == 1979) %&gt;%\n  arrange(desc(births))\n\nWith the pipe notation, x %&gt;% f(y) becomes f(x,y), where in the first line here, x is Birthdays, the function f is filter, and y is state == \"MA\", year == 1979. The really nice thing about piping is that you can chain together a bunch of different operations without having to save the intermediate results. This is what we have done above by chaining together a filter followed by an arrange.\n\n\nManipulating Dates\nAdditional reading:\n\nWickham and Grolemund, Date and Times with lubridate\n\nThe date variable in Birthdays prints out in the conventional, human-readable way. But it is actually in a format (called POSIX date format) that automatically respects the order of time. The lubridate package contains helpful functions that will extract various information about any date. Here are some you might find useful:\n\nyear()\nmonth()\nweek()\nyday() — gives the day of the year as a number 1-366. This is often called the “Julian day.”\nmday() — gives the day of the month as a number 1-31\nwday() — gives the weekday (e.g. Monday, Tuesday, …). Use the optional argument label = TRUE to have the weekday spelled out rather than given as a number 1-7.\n\nUsing these lubridate functions, you can easily look at the data in more detail. For example, we can add columns to the date table for month and day of the week:3\n\nBirthdays &lt;-\n  Birthdays %&gt;%\n  mutate(\n    month = month(date, label = TRUE),\n    weekday = wday(date, label = TRUE)\n  )\n\nHere is what the data table looks like with our new columns:\n\n\n\nA subset of the birthday data with additional variables.\n\n\nstate\ndate\nyear\nbirths\nmonth\nweekday\n\n\n\n\nAK\n1969-01-01\n1969\n14\nJan\nWed\n\n\nAL\n1969-01-01\n1969\n174\nJan\nWed\n\n\nAR\n1969-01-01\n1969\n78\nJan\nWed\n\n\nAZ\n1969-01-01\n1969\n84\nJan\nWed\n\n\nCA\n1969-01-01\n1969\n824\nJan\nWed\n\n\nCO\n1969-01-01\n1969\n100\nJan\nWed\n\n\n\n\n\n\nMake a table showing the five states with the most births between September 9, 1979 and September 11, 1979, inclusive. Arrange the table in descending order of births.\n\n\n\nSolution\n\nThe plan of attack is to first filter the dates, then group by state, then use a summarize to add up totals for each state, and finally arrange them in descending order to find the top 5.4\n\nSepTable &lt;-\n  Birthdays %&gt;%\n  filter(date &gt;= ymd(\"1979-09-09\"), date &lt;= ymd(\"1979-09-11\")) %&gt;%\n  group_by(state) %&gt;%\n  summarize(total = sum(births)) %&gt;%\n  arrange(desc(total)) %&gt;%\n  head(n = 5)\n\n\nknitr::kable(\n  SepTable[, ],\n  caption = \"States with the\n  most births between September 9, 1979\n  and September 11, 1979, inclusive.\"\n)\n\n\nStates with the most births between September 9, 1979 and September 11, 1979, inclusive.\n\n\nstate\ntotal\n\n\n\n\nCA\n3246\n\n\nTX\n2347\n\n\nNY\n1943\n\n\nIL\n1673\n\n\nOH\n1408",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#practice-part-1-baby-names",
    "href": "src/07-Six_Main_Verbs.html#practice-part-1-baby-names",
    "title": "(PART) Data Wrangling",
    "section": "Practice Part 1: Baby Names",
    "text": "Practice Part 1: Baby Names\nWe are going to practice the six data verbs on the babynames dataset:\n\n\n\nA subset of the babynames data, which runs from 1880-2015 and is provided by the US Social Security Administration.\n\n\nyear\nsex\nname\nn\nprop\n\n\n\n\n1880\nF\nMary\n7065\n0.0723836\n\n\n1880\nF\nAnna\n2604\n0.0266790\n\n\n1880\nF\nEmma\n2003\n0.0205215\n\n\n1880\nF\nElizabeth\n1939\n0.0198658\n\n\n1880\nF\nMinnie\n1746\n0.0178884\n\n\n1880\nF\nMargaret\n1578\n0.0161672\n\n\n\n\n\n\nAdd a new boolean (true or false) variable called `has2000` that indicates whether there were more than 2000 babies of that sex assigned at birth with that name in each year. Display the first six rows of your new table.\n\n\n\nFind the number of total babies per year, sorted by most babies to least babies.\n\n\n\nFind the twelve most popular names overall (i.e., totaled over all year and sexes), ordered by popularity.\n\n\n\nFind the most popular names for males, over all years and ordered by popularity.\n\n\n\nFor each line of code below, describe the output passed to the next function. Write your answers as comments in the code. Try to see if you can describe the output without running the code, an important skill to develop! \n\n\nbabynames %&gt;%\n  filter( year &gt;= 1900, year &lt; 2000) %&gt;% \n  mutate(YearCat = cut(year, c(1900,1950,2000), right = FALSE, labels = c('Early 1900','Late 1900'))) %&gt;%\n  group_by(sex,YearCat,name) %&gt;%\n  summarize(n = sum(n)) %&gt;%\n  arrange(sex,YearCat, n) %&gt;%\n  group_by(sex,YearCat) %&gt;%\n  mutate(rank = dense_rank(-n))  %&gt;%\n  arrange(sex,rank) %&gt;%\n  head(10)\n\n\nCalculate the number of babies born each decade, and arrange them in descending order. Calculating the decade may be the trickiest part of this question!   \n  \n\n\n\nCalculate the most popular name for each year. Print out the answer for the years 2006-2015. This is tricky, but try Googling for hints.",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#practice-part-2-us-births",
    "href": "src/07-Six_Main_Verbs.html#practice-part-2-us-births",
    "title": "(PART) Data Wrangling",
    "section": "Practice Part 2: US Births",
    "text": "Practice Part 2: US Births\nNow we are ready to return to the Birthdays data set to investigate some drivers of daily births in the US.\n\nSeasonality\nFor this activity, we need to work with data aggregated across the states.\n\nCreate a new data table, `DailyBirths`, that adds up all the births for each day across all the states.  Plot out daily births vs date.\n\n\nFor all of the remaining exercises, start with your DailyBirths data frame.\n\n\nTo examine seasonality in birth rates, look at the daily number of births using `DailyBirths` (don't aggregate) by\n\na. week of the year (1-53)\nb. month of the year (January to December)\nc. Julian day (1-366)\n\nWhen are the most babies born? The fewest?\n\n\n\n\nDay of the Week\n\nTo examine patterns within the week, make a box plot showing the daily number of births by day of the week (use `DailyBirths`). Interpret your results.\n\n\n\n\nHolidays\n\nPick a two-year span of the `DailyBirths` that falls in the 1980s, say, 1980/1981.  Extract out the data just in this interval, calling it `MyTwoYears`.  (Hint: `filter()`, `year()`).  Plot out the births in this two-year span day by day. Color each date according to its day of the week.  Make sure to choose your font size, line widths, and color scheme to make your figure legible. Explain the pattern that you see.\n\n\nThe plot you generate for Exercise @ref(exr:two-year) should be generally consistent with the weekend effect and seasonal patterns we have already seen; however, a few days each year stand out as exceptions. We are going to examine the hypothesis that these are holidays. You can find a data set listing US federal holidays here. Read it in as follows:5\n\n\nError: 'data/US-Holidays.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\nHolidays &lt;- read_csv(\"https://bcheggeseth.github.io/112_fall_2022/data/US-Holidays.csv\") %&gt;%\n  mutate(date = as.POSIXct(lubridate::dmy(date)))\n\n\nNow let's update the plot from Exercise \\@ref(exr:two-year) to include the holidays.   \n\na. Add a variable to `MyTwoYears` called `is_holiday`. It should be `TRUE` when the day is a holiday, and `FALSE` otherwise. One way to do this is with the transformation verb `%in%`, for instance, `is_holiday = date %in% Holidays$date`.   \nb. Add a `geom_point` layer to your plot that sets the color of the points based on the day of the week and the shape of the points based on whether or not the day is a holiday.   \nc. Finally, some holidays seem to have more of an effect than others. It would be helpful to label them. Use `geom_text` with the holiday data to add labels to each of the holidays. Hints: 1. Start by making a new data table called `MyHolidays` that just contains the holidays in your selected two year window. 2. Start your geometry line with `geom_text(data=MyHolidays)`. 3. You'll have to make up a y-coordinate for each label.  4. You can set the orientation of each label with the `angle` argument; e.g., `geom_text(data=MyHolidays, angle=40, ...)`.\n\n\n\n\nGeography\nIn any way you choose, explore the effect of geography on birth patterns using the original Birthdays data. For example, do parents in Minnesota have fewer winter babies than in other states? Which states have the largest increases or decreases in their portion of US births over time? Is the weekend effect less strong for states with a higher percentage of their populations living in rural areas?\nIf you have extra time or want some extra practice, pick any issue (not all of these) that interests you, explore it, and create a graphic to illustrate your findings.\n\n\nSuperstition\nThis article from FiveThirtyEight demonstrates that fewer babies are born on the 13th of each month, and the effect is even stronger when the 13th falls on a Friday. If you have extra time or want some extra practice, you can try to recreate the first graphic in the article.",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#appendix-r-functions",
    "href": "src/07-Six_Main_Verbs.html#appendix-r-functions",
    "title": "(PART) Data Wrangling",
    "section": "Appendix: R Functions",
    "text": "Appendix: R Functions\n\nSix Main Verbs\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nselect()\nProvides a subset of variables\nselect(Birthdays, state, date, year, births)\n\n\nmutate()\nCreates a new variable\nmutate(Birthdays, year_short = year - 1900)\n\n\nfilter()\nProvides a subset of rows\nfilter(Birthdays, state %in% c(\"AK\",\"CA\",\"MA\"))\n\n\narrange()\nSorts the rows of a dataset\narrange(Birthdays, desc(births))\n\n\nsummarize()\nCollapses rows into summaries of variables across rows\nsummarize(Birthdays,total_births = sum(births), average_births = mean(births), nstates = n_distinct(state), ncases = n())\n\n\ngroup_by()\nCollapses rows into summaries of variables across rows\ngroup_by(Birthdays, year, state) %&gt;% summarize(average = mean(births))\n\n\n\n\n\nLogical/Boolean Operators\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\n==\nChecks whether two items are equal\nyear == 2000\n\n\n&gt;\nChecks whether left is greater than the right\nyear &gt; 2000\n\n\n&lt;\nChecks whether left is less than the right\nyear &lt; 2000\n\n\n&gt;=\nChecks whether left is greater than or equal to right\nyear &gt;= 2000\n\n\n&lt;=\nChecks whether left is less than or equal to right\nyear &lt;= 2000\n\n\n!=\nChecks whether left is not equal to right\nyear != 2000\n\n\n%in%\nChecks whether left is in vector on right\nstate %in% c(\"AK\",\"CA\",\"MA\")",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/07-Six_Main_Verbs.html#footnotes",
    "href": "src/07-Six_Main_Verbs.html#footnotes",
    "title": "(PART) Data Wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe fivethirtyeight package has more recent data.↩︎\nImportant note about = vs. ==: A single = is an assignment operator that assigns the value after the equal sign to the variable before the equal sign. We saw an example of this above with year_short = year - 1900. In order to compare whether two values are the same, we need to use the double equal == as in year == 1979.↩︎\nThe label = TRUE argument tells month to return a string abbreviation for the month instead of the month’s number.↩︎\nThe verbs head(n = 5), tail(n = 3) are often used just after an arrange to keep, e.g., only the first 5 entries or last 3 entries, where n specifies the number of entries to keep.↩︎\nThe point of the lubridate::dmy() function is to convert the character-string date stored in the CSV to a POSIX date-number.↩︎",
    "crumbs": [
      "Src",
      "(PART) Data Wrangling"
    ]
  },
  {
    "objectID": "src/09-Join.html",
    "href": "src/09-Join.html",
    "title": "Joining Two Data Frames",
    "section": "",
    "text": "Understand the concept of keys and variables that uniquely identify rows or cases\nUnderstand the different types of joins, different ways of combining two data frames together\nDevelop comfort in using mutating joins: left_join, inner_join and full_join in the dplyr package\nDevelop comfort in using filtering joins: semi_join, anti_join in the dplyr package\n\nYou can download a template .Rmd of this activity here. Put it in a folder Assignment_06 in COMP_STAT_112.\n\n\n\nA join is a verb that means to combine two data tables.\n\nThese tables are often called the left and the right tables.\n\nThere are several kinds of join.\n\nAll involve establishing a correspondence — a match — between each case in the left table and zero or more cases in the right table.\nThe various joins differ in how they handle multiple matches or missing matches.\n\n\n\nA match between a case in the left data table and a case in the right data table is made based on the values in keys, variables that uniquely define observations in a data table.\nAs an example, we’ll examine the following two tables on grades and courses.\nThe Grades file has one case for each class of each student (student-class pair), and includes variables describing the ID of the student (sid), the ID of the session (section of class), and the grade received.\nThe Courses file has one case for each section of a class, and includes variables for the ID of the session (section of class), the department (coded as letters), the level, the semester, the enrollment, and the ID of the instructor (iid). We show a few rows of each table below.\n\n\nError: 'data/grades.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError: 'data/courses.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\n\n\nA primary key uniquely identifies an observation (a row or case) in its own data table.\nsid (student ID) and sessionID (class ID) are the primary keys for Grades as they unique identify each case.\n\n# You can check to make sure that there are no combinations of sid and session ID that have more than 1 row\nGrades %&gt;%\n  count(sid, sessionID) %&gt;%\n  filter(n &gt; 1)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nsessionID (class ID) and dept are the primary keys for Courses as they unique identify each case. You may have guessed that sessionID alone was sufficient as the key; however, if a course is cross-listed, then it may have multiple departments listed.\n\n# check to make sure that there are no combinations \n# of session ID and dept that have more than 1 row\nCourses %&gt;%\n  count(sessionID, dept) %&gt;%\n  filter(n &gt; 1)\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\n\n\n\n\nIn order to establish a match between two data tables,\n\nYou specify which variables (or keys) to use.\nEach match is specify as a pair of variables, where one variable from the left table corresponds to one variable from the right table.\nCases must have exactly equal values in the left variable and right variable for a match to be made.\n\n\n\n\nThe first class of joins are mutating joins, which add new variables (columns) to the left data table from matching observations in the right table.1\nThe main difference in the three mutating join options in this class is how they answer the following questions:\n\nWhat happens when a case in the left table has no matches in the right table?\nWhat happens when a case in the right table has no matches in the left table?\n\nThree mutating join functions:\n\nleft_join(): the output has all cases from the left, regardless if there is a match in the right, but discards any cases in the right that do not have a match in the left.\ninner_join(): the output has only the cases from the left with a match in the right.\nfull_join(): the output has all cases from the left and the right. This is less common than the first two join operators.\n\nWhen there are multiple matches in the right table for a particular case in the left table, all three of these mutating join operators produce a separate case in the new table for each of the matches from the right.\n\nDetermine the average class size from the viewpoint of a student (getting an average size for each student and the classes they take) and the viewpoint of the Provost / Admissions Office (getting an average size across all classes).\n\n\n\n\nSolution\n\nProvost Perspective:\nThe Provost counts each section as one class and takes the average of all classes. We have to be a little careful and cannot simply do mean(Courses$enroll), because some sessionID appear twice on the course list. Why is that?2 We can still do this from the data we have in the Courses table, but we should aggregate by sessionID first:\n\nCourseSizes &lt;- Courses %&gt;%\n  group_by(sessionID) %&gt;%\n  summarise(total_enroll = sum(enroll))\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nmean(CourseSizes$total_enroll)\n\nError in eval(expr, envir, enclos): object 'CourseSizes' not found\n\n\nStudent Perspective:\nTo get the average class size from the student perspective, we can join the enrollment of the section onto each instance of a student section. Here, the left table is Grades, the right table is CourseSizes, we are going to match based on sessionID, and we want to add the variable total_enroll from CoursesSizes.\nWe’ll use a left_join since we aren’t interested in any sections from the CourseSizes table that do not show up in the Grades table; their enrollments should be 0, and they are not actually seen by any students. Note, e.g., if there were 100 extra sections of zero enrollments on the Courses table, this would change the average from the Provost’s perspective, but not at all from the students’ perspective.\nIf the by = is omitted from a join, then R will perform a natural join, which matches the two tables by all variables they have in common. In this case, the only variable in common is the sessionID, so we would get the same results by omitting the second argument. In general, this is not reliable unless we check ahead of time which variables the tables have in common. If two variables to match have different names in the two tables, we can write by = c(\"name1\" = \"name2\").\n\nEnrollmentsWithClassSize &lt;- Grades %&gt;%\n  left_join(CourseSizes,\n    by = c(\"sessionID\" = \"sessionID\")\n  ) %&gt;%\n  select(sid, sessionID, total_enroll)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'EnrollmentsWithClassSize' not found\n\n\n\nAveClassEachStudent &lt;- EnrollmentsWithClassSize %&gt;%\n  group_by(sid) %&gt;%\n  summarise(ave_enroll = mean(total_enroll, na.rm = TRUE))\n\nError in eval(expr, envir, enclos): object 'EnrollmentsWithClassSize' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'AveClassEachStudent' not found\n\n\nThe na.rm = TRUE here says that if the class size is not available for a given class, we do not count that class towards the student’s average class size. What is another way to capture the same objective? We could have used an inner_join instead of a left_join when we joined the tables to eliminate any entries from the left table that did not have a match in the right table.\nNow we can take the average of the AveClassEachStudent table, counting each student once, to find the average class size from the student perspective:\n\nmean(AveClassEachStudent$ave_enroll)\n\nError in eval(expr, envir, enclos): object 'AveClassEachStudent' not found\n\n\nWe see that the average size from the student perspective (24.4) is greater than the average size from the Provost’s perspective (21.5).\n\n\n\n\nThe second class of joins are filtering joins, which select specific cases from the left table based on whether they match an observation in the right table.\n\nsemi_join(): discards any cases in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join(): discards any cases in the left table that have a match in the right table.\n\nA particularly common employment of these joins is to use a filtered summary as a comparison to select a subset of the original cases, as follows.\n\nFind a subset of the `Grades` data that only contains data on the four largest sections in the `Courses` data set.\n\n\n\n\nSolution\n\n\nLargeSections &lt;- Courses %&gt;%\n  group_by(sessionID) %&gt;%\n  summarise(total_enroll = sum(enroll)) %&gt;%\n  arrange(desc(total_enroll)) %&gt;% head(4)\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nGradesFromLargeSections &lt;- Grades %&gt;%\n  semi_join(LargeSections)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nUse `semi_join()` to create a table with a subset of the rows of `Grades` corresponding to all classes taken in department `J`.\n\n\n\n\nSolution\n\nThere are multiple ways to do this. We could do a left join to the Grades table to add on the dept variable, and then filter by department, then select all variables except the additional dept variable we just added. Here is a more direct way with semi_join that does not involve adding and subtracting the extra variable:\n\nJCourses &lt;- Courses %&gt;%\n  filter(dept == \"J\")\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nJGrades &lt;- Grades %&gt;%\n  semi_join(JCourses)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nLet’s double check this worked. Here are the first few entries of our new table:\n\n\nError in eval(expr, envir, enclos): object 'JGrades' not found\n\n\nThe first entry is for session1791. Which department is that course in? What department should it be?\n\n(Courses %&gt;% filter(sessionID == \"session1791\"))\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\nGreat, it worked! But that only checked the first one. What if we want to double check all of the courses included in Table @ref(tab:jtab)? We can add on the department and do a group by to count the number from each department in our table.\n\nJGrades %&gt;%\n  left_join(Courses) %&gt;%\n  count(dept) \n\nError in eval(expr, envir, enclos): object 'JGrades' not found\n\n\n\n\n\n\n\nUse all of your wrangling skills to answer the following questions.\n\nHint 1: start by thinking about what tables you might need to join (if any) and identifying the corresponding variables to match. Hint 2: you’ll need an extra table to convert grades to grade point averages. I’ve given you the code below.\n\nHow many student enrollments in each department?\nWhat’s the grade-point average (GPA) for each student? The average student GPA? Hint: There are some “S” and “AU” grades that we want to exclude from GPA calculations. What is the correct variant of join to accomplish this?\nWhat fraction of grades are below B+?\nWhat’s the grade-point average for each instructor?\nEstimate the grade-point average for each department. We cannot actually compute the correct grade-point average for each department from the information we have. The reason why is due to cross-listed courses. Students for those courses could be enrolled under either department, and we do not know which department to assign the grade to. There are a number of possible workarounds to get an estimate. One would be to assign all grades in a section to the department of the instructor, which we’d have to infer from the data. For this exercise, start by creating a table with all cross-listed courses. Then use an anti_join to eliminate all cross-listed courses. Finally, use an inner_join to compute the grade-point average for each department.\n\n\n(GPAConversion &lt;- tibble(grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\"), gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0)))\n\n# A tibble: 13 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n\n\n\n\n\n\nIn this activity, you’ll examine some factors that may influence the use of bicycles in a bike-renting program. The data come from Washington, DC and cover the last quarter of 2014.\n\n\nError in knitr::include_graphics(\"images/bike_station.jpeg\"): Cannot find the file(s): \"images/bike_station.jpeg\"\n\n\n\n\nError in knitr::include_graphics(\"images/bike_van.jpeg\"): Cannot find the file(s): \"images/bike_van.jpeg\"\n\n\nTwo data tables are available:\n\nTrips contains records of individual rentals here\nStations gives the locations of the bike rental stations here\n\nHere is the code to read in the data:3\n\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data-Small.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\n\nError: 'data/DC-Stations.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\ndata_site &lt;-\n  \"https://bcheggeseth.github.io/112_spring_2023/data/2014-Q4-Trips-History-Data-Small.rds\"\nTrips &lt;- readRDS(gzcon(url(data_site)))\nStations &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/DC-Stations.csv\")\n\nThe Trips data table is a random subset of 10,000 trips from the full quarterly data.\n\n\nIt’s natural to expect that bikes are rented more at some times of day, some days of the week, some months of the year than others. The variable sdate gives the time (including the date) that the rental started.\n\nMake the following plots and interpret them:\n\n\nA density plot of the events versus sdate. Use ggplot() and geom_density().\nA density plot of the events versus time of day. You can use mutate with lubridate::hour(), and lubridate::minute() to extract the hour of the day and minute within the hour from sdate. Hint: A minute is 1/60 of an hour, so create a field where 3:30 is 3.5 and 3:45 is 3.75.\nA bar plot of the events versus day of the week.\nFacet your graph from (b) by day of the week. Is there a pattern?\n\nThe variable client describes whether the renter is a regular user (level Registered) or has not joined the bike-rental organization (Causal). Do you think these two different categories of users show different rental behavior? How might it interact with the patterns you found in Exercise @ref(exr:exr-temp)?\n\nRepeat the graphic from Exercise \\@ref(exr:exr-temp) (d) with the following changes:\n\n\nSet the fill aesthetic for geom_density() to the client variable. You may also want to set the alpha for transparency and color=NA to suppress the outline of the density function.\nNow add the argument position = position_stack() to geom_density(). In your opinion, is this better or worse in terms of telling a story? What are the advantages/disadvantages of each?\nRather than faceting on day of the week, create a new faceting variable like this: mutate(wkday = ifelse(lubridate::wday(sdate) %in% c(1,7), \"weekend\", \"weekday\")). What does the variable wkday represent? Try to understand the code.\nIs it better to facet on wkday and fill with client, or vice versa?\nOf all of the graphics you created so far, which is most effective at telling an interesting story?\n\n\n\n\n\nUse the latitude and longitude variables in `Stations` to make a visualization of the total number of departures from each station in the `Trips` data. To layer your data on top of a  map, start your plotting code as follows:\n\n\nmyMap&lt;-get_stamenmap(c(-77.1,38.87,-76.975,38.95),zoom=14,maptype=\"terrain\") \nggmap(myMap) + ...\nNote: If you want to use Google Maps instead, which do look a bit nicer, you’ll need to get a Google Maps API Key (free but requires credit card to sign up), and then you can use get_map instead of get_stamenmap.\n\nOnly 14.4% of the trips in our data are carried out by casual users.^[We can compute this statistic via `mean(Trips$client==\"Casual\")`.] Create a map that shows which area(s) of the city have stations with a much higher percentage of departures by casual users. Interpret your map.\n\n\n\n\n\n\nConsider the following:\n\n(a) Make a table with the ten station-date combinations (e.g., 14th & V St., 2014-10-14) with the highest number of departures, sorted from most departures to fewest. Hint: `as_date(sdate)` converts `sdate` from date-time format to date format.\n(b) Use a join operation to make a table with only those trips whose departures match those top ten station-date combinations from part (a).\n(c) Group the trips you filtered out in part (b) by client type and `wkday` (weekend/weekday), and count the total number of trips in each of the four groups. Interpret your results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nleft_join()\nJoins two data sets together (adding variables from right to left data sets), keeping all rows of the left or 1st dataset\nGrades %&gt;% left_join(CourseSizes, by = c(\"sessionID\" = \"sessionID\"))\n\n\ninner_join()\nJoins two data sets together (adding variables from right to left data sets), keeping only rows in left that have a match in right\nGrades %&gt;% inner_join(GPAConversion)\n\n\nfull_join()\nJoins two data sets together (adding variables from right to left data sets), keeping all rows of both left and right datasets\nGrades %&gt;% full_join(CourseSizes, by = c(\"sessionID\" = \"sessionID\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nsemi_join()\nKeep only rows in left that have a match in right\nGrades %&gt;% semi_join(LargeSections)\n\n\nanti_join()\nKeep only rows in left that don’t have a match in right\nGrades %&gt;% anti_join(CrossListedSections)",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/09-Join.html#learning-goals",
    "href": "src/09-Join.html#learning-goals",
    "title": "Joining Two Data Frames",
    "section": "",
    "text": "Understand the concept of keys and variables that uniquely identify rows or cases\nUnderstand the different types of joins, different ways of combining two data frames together\nDevelop comfort in using mutating joins: left_join, inner_join and full_join in the dplyr package\nDevelop comfort in using filtering joins: semi_join, anti_join in the dplyr package\n\nYou can download a template .Rmd of this activity here. Put it in a folder Assignment_06 in COMP_STAT_112.",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/09-Join.html#joins",
    "href": "src/09-Join.html#joins",
    "title": "Joining Two Data Frames",
    "section": "",
    "text": "A join is a verb that means to combine two data tables.\n\nThese tables are often called the left and the right tables.\n\nThere are several kinds of join.\n\nAll involve establishing a correspondence — a match — between each case in the left table and zero or more cases in the right table.\nThe various joins differ in how they handle multiple matches or missing matches.\n\n\n\nA match between a case in the left data table and a case in the right data table is made based on the values in keys, variables that uniquely define observations in a data table.\nAs an example, we’ll examine the following two tables on grades and courses.\nThe Grades file has one case for each class of each student (student-class pair), and includes variables describing the ID of the student (sid), the ID of the session (section of class), and the grade received.\nThe Courses file has one case for each section of a class, and includes variables for the ID of the session (section of class), the department (coded as letters), the level, the semester, the enrollment, and the ID of the instructor (iid). We show a few rows of each table below.\n\n\nError: 'data/grades.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError: 'data/courses.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\n\n\nA primary key uniquely identifies an observation (a row or case) in its own data table.\nsid (student ID) and sessionID (class ID) are the primary keys for Grades as they unique identify each case.\n\n# You can check to make sure that there are no combinations of sid and session ID that have more than 1 row\nGrades %&gt;%\n  count(sid, sessionID) %&gt;%\n  filter(n &gt; 1)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nsessionID (class ID) and dept are the primary keys for Courses as they unique identify each case. You may have guessed that sessionID alone was sufficient as the key; however, if a course is cross-listed, then it may have multiple departments listed.\n\n# check to make sure that there are no combinations \n# of session ID and dept that have more than 1 row\nCourses %&gt;%\n  count(sessionID, dept) %&gt;%\n  filter(n &gt; 1)\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\n\n\n\n\nIn order to establish a match between two data tables,\n\nYou specify which variables (or keys) to use.\nEach match is specify as a pair of variables, where one variable from the left table corresponds to one variable from the right table.\nCases must have exactly equal values in the left variable and right variable for a match to be made.\n\n\n\n\nThe first class of joins are mutating joins, which add new variables (columns) to the left data table from matching observations in the right table.1\nThe main difference in the three mutating join options in this class is how they answer the following questions:\n\nWhat happens when a case in the left table has no matches in the right table?\nWhat happens when a case in the right table has no matches in the left table?\n\nThree mutating join functions:\n\nleft_join(): the output has all cases from the left, regardless if there is a match in the right, but discards any cases in the right that do not have a match in the left.\ninner_join(): the output has only the cases from the left with a match in the right.\nfull_join(): the output has all cases from the left and the right. This is less common than the first two join operators.\n\nWhen there are multiple matches in the right table for a particular case in the left table, all three of these mutating join operators produce a separate case in the new table for each of the matches from the right.\n\nDetermine the average class size from the viewpoint of a student (getting an average size for each student and the classes they take) and the viewpoint of the Provost / Admissions Office (getting an average size across all classes).\n\n\n\n\nSolution\n\nProvost Perspective:\nThe Provost counts each section as one class and takes the average of all classes. We have to be a little careful and cannot simply do mean(Courses$enroll), because some sessionID appear twice on the course list. Why is that?2 We can still do this from the data we have in the Courses table, but we should aggregate by sessionID first:\n\nCourseSizes &lt;- Courses %&gt;%\n  group_by(sessionID) %&gt;%\n  summarise(total_enroll = sum(enroll))\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nmean(CourseSizes$total_enroll)\n\nError in eval(expr, envir, enclos): object 'CourseSizes' not found\n\n\nStudent Perspective:\nTo get the average class size from the student perspective, we can join the enrollment of the section onto each instance of a student section. Here, the left table is Grades, the right table is CourseSizes, we are going to match based on sessionID, and we want to add the variable total_enroll from CoursesSizes.\nWe’ll use a left_join since we aren’t interested in any sections from the CourseSizes table that do not show up in the Grades table; their enrollments should be 0, and they are not actually seen by any students. Note, e.g., if there were 100 extra sections of zero enrollments on the Courses table, this would change the average from the Provost’s perspective, but not at all from the students’ perspective.\nIf the by = is omitted from a join, then R will perform a natural join, which matches the two tables by all variables they have in common. In this case, the only variable in common is the sessionID, so we would get the same results by omitting the second argument. In general, this is not reliable unless we check ahead of time which variables the tables have in common. If two variables to match have different names in the two tables, we can write by = c(\"name1\" = \"name2\").\n\nEnrollmentsWithClassSize &lt;- Grades %&gt;%\n  left_join(CourseSizes,\n    by = c(\"sessionID\" = \"sessionID\")\n  ) %&gt;%\n  select(sid, sessionID, total_enroll)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'EnrollmentsWithClassSize' not found\n\n\n\nAveClassEachStudent &lt;- EnrollmentsWithClassSize %&gt;%\n  group_by(sid) %&gt;%\n  summarise(ave_enroll = mean(total_enroll, na.rm = TRUE))\n\nError in eval(expr, envir, enclos): object 'EnrollmentsWithClassSize' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'AveClassEachStudent' not found\n\n\nThe na.rm = TRUE here says that if the class size is not available for a given class, we do not count that class towards the student’s average class size. What is another way to capture the same objective? We could have used an inner_join instead of a left_join when we joined the tables to eliminate any entries from the left table that did not have a match in the right table.\nNow we can take the average of the AveClassEachStudent table, counting each student once, to find the average class size from the student perspective:\n\nmean(AveClassEachStudent$ave_enroll)\n\nError in eval(expr, envir, enclos): object 'AveClassEachStudent' not found\n\n\nWe see that the average size from the student perspective (24.4) is greater than the average size from the Provost’s perspective (21.5).\n\n\n\n\nThe second class of joins are filtering joins, which select specific cases from the left table based on whether they match an observation in the right table.\n\nsemi_join(): discards any cases in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join(): discards any cases in the left table that have a match in the right table.\n\nA particularly common employment of these joins is to use a filtered summary as a comparison to select a subset of the original cases, as follows.\n\nFind a subset of the `Grades` data that only contains data on the four largest sections in the `Courses` data set.\n\n\n\n\nSolution\n\n\nLargeSections &lt;- Courses %&gt;%\n  group_by(sessionID) %&gt;%\n  summarise(total_enroll = sum(enroll)) %&gt;%\n  arrange(desc(total_enroll)) %&gt;% head(4)\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nGradesFromLargeSections &lt;- Grades %&gt;%\n  semi_join(LargeSections)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\n\n\nUse `semi_join()` to create a table with a subset of the rows of `Grades` corresponding to all classes taken in department `J`.\n\n\n\n\nSolution\n\nThere are multiple ways to do this. We could do a left join to the Grades table to add on the dept variable, and then filter by department, then select all variables except the additional dept variable we just added. Here is a more direct way with semi_join that does not involve adding and subtracting the extra variable:\n\nJCourses &lt;- Courses %&gt;%\n  filter(dept == \"J\")\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\nJGrades &lt;- Grades %&gt;%\n  semi_join(JCourses)\n\nError in eval(expr, envir, enclos): object 'Grades' not found\n\n\nLet’s double check this worked. Here are the first few entries of our new table:\n\n\nError in eval(expr, envir, enclos): object 'JGrades' not found\n\n\nThe first entry is for session1791. Which department is that course in? What department should it be?\n\n(Courses %&gt;% filter(sessionID == \"session1791\"))\n\nError in eval(expr, envir, enclos): object 'Courses' not found\n\n\nGreat, it worked! But that only checked the first one. What if we want to double check all of the courses included in Table @ref(tab:jtab)? We can add on the department and do a group by to count the number from each department in our table.\n\nJGrades %&gt;%\n  left_join(Courses) %&gt;%\n  count(dept) \n\nError in eval(expr, envir, enclos): object 'JGrades' not found\n\n\n\n\n\n\n\nUse all of your wrangling skills to answer the following questions.\n\nHint 1: start by thinking about what tables you might need to join (if any) and identifying the corresponding variables to match. Hint 2: you’ll need an extra table to convert grades to grade point averages. I’ve given you the code below.\n\nHow many student enrollments in each department?\nWhat’s the grade-point average (GPA) for each student? The average student GPA? Hint: There are some “S” and “AU” grades that we want to exclude from GPA calculations. What is the correct variant of join to accomplish this?\nWhat fraction of grades are below B+?\nWhat’s the grade-point average for each instructor?\nEstimate the grade-point average for each department. We cannot actually compute the correct grade-point average for each department from the information we have. The reason why is due to cross-listed courses. Students for those courses could be enrolled under either department, and we do not know which department to assign the grade to. There are a number of possible workarounds to get an estimate. One would be to assign all grades in a section to the department of the instructor, which we’d have to infer from the data. For this exercise, start by creating a table with all cross-listed courses. Then use an anti_join to eliminate all cross-listed courses. Finally, use an inner_join to compute the grade-point average for each department.\n\n\n(GPAConversion &lt;- tibble(grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\"), gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0)))\n\n# A tibble: 13 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/09-Join.html#bicycle-use-patterns",
    "href": "src/09-Join.html#bicycle-use-patterns",
    "title": "Joining Two Data Frames",
    "section": "",
    "text": "In this activity, you’ll examine some factors that may influence the use of bicycles in a bike-renting program. The data come from Washington, DC and cover the last quarter of 2014.\n\n\nError in knitr::include_graphics(\"images/bike_station.jpeg\"): Cannot find the file(s): \"images/bike_station.jpeg\"\n\n\n\n\nError in knitr::include_graphics(\"images/bike_van.jpeg\"): Cannot find the file(s): \"images/bike_van.jpeg\"\n\n\nTwo data tables are available:\n\nTrips contains records of individual rentals here\nStations gives the locations of the bike rental stations here\n\nHere is the code to read in the data:3\n\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data-Small.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\n\nError: 'data/DC-Stations.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\ndata_site &lt;-\n  \"https://bcheggeseth.github.io/112_spring_2023/data/2014-Q4-Trips-History-Data-Small.rds\"\nTrips &lt;- readRDS(gzcon(url(data_site)))\nStations &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/DC-Stations.csv\")\n\nThe Trips data table is a random subset of 10,000 trips from the full quarterly data.\n\n\nIt’s natural to expect that bikes are rented more at some times of day, some days of the week, some months of the year than others. The variable sdate gives the time (including the date) that the rental started.\n\nMake the following plots and interpret them:\n\n\nA density plot of the events versus sdate. Use ggplot() and geom_density().\nA density plot of the events versus time of day. You can use mutate with lubridate::hour(), and lubridate::minute() to extract the hour of the day and minute within the hour from sdate. Hint: A minute is 1/60 of an hour, so create a field where 3:30 is 3.5 and 3:45 is 3.75.\nA bar plot of the events versus day of the week.\nFacet your graph from (b) by day of the week. Is there a pattern?\n\nThe variable client describes whether the renter is a regular user (level Registered) or has not joined the bike-rental organization (Causal). Do you think these two different categories of users show different rental behavior? How might it interact with the patterns you found in Exercise @ref(exr:exr-temp)?\n\nRepeat the graphic from Exercise \\@ref(exr:exr-temp) (d) with the following changes:\n\n\nSet the fill aesthetic for geom_density() to the client variable. You may also want to set the alpha for transparency and color=NA to suppress the outline of the density function.\nNow add the argument position = position_stack() to geom_density(). In your opinion, is this better or worse in terms of telling a story? What are the advantages/disadvantages of each?\nRather than faceting on day of the week, create a new faceting variable like this: mutate(wkday = ifelse(lubridate::wday(sdate) %in% c(1,7), \"weekend\", \"weekday\")). What does the variable wkday represent? Try to understand the code.\nIs it better to facet on wkday and fill with client, or vice versa?\nOf all of the graphics you created so far, which is most effective at telling an interesting story?\n\n\n\n\n\nUse the latitude and longitude variables in `Stations` to make a visualization of the total number of departures from each station in the `Trips` data. To layer your data on top of a  map, start your plotting code as follows:\n\n\nmyMap&lt;-get_stamenmap(c(-77.1,38.87,-76.975,38.95),zoom=14,maptype=\"terrain\") \nggmap(myMap) + ...\nNote: If you want to use Google Maps instead, which do look a bit nicer, you’ll need to get a Google Maps API Key (free but requires credit card to sign up), and then you can use get_map instead of get_stamenmap.\n\nOnly 14.4% of the trips in our data are carried out by casual users.^[We can compute this statistic via `mean(Trips$client==\"Casual\")`.] Create a map that shows which area(s) of the city have stations with a much higher percentage of departures by casual users. Interpret your map.\n\n\n\n\n\n\nConsider the following:\n\n(a) Make a table with the ten station-date combinations (e.g., 14th & V St., 2014-10-14) with the highest number of departures, sorted from most departures to fewest. Hint: `as_date(sdate)` converts `sdate` from date-time format to date format.\n(b) Use a join operation to make a table with only those trips whose departures match those top ten station-date combinations from part (a).\n(c) Group the trips you filtered out in part (b) by client type and `wkday` (weekend/weekday), and count the total number of trips in each of the four groups. Interpret your results.",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/09-Join.html#appendix-r-functions",
    "href": "src/09-Join.html#appendix-r-functions",
    "title": "Joining Two Data Frames",
    "section": "",
    "text": "Function/Operator\nAction\nExample\n\n\n\n\nleft_join()\nJoins two data sets together (adding variables from right to left data sets), keeping all rows of the left or 1st dataset\nGrades %&gt;% left_join(CourseSizes, by = c(\"sessionID\" = \"sessionID\"))\n\n\ninner_join()\nJoins two data sets together (adding variables from right to left data sets), keeping only rows in left that have a match in right\nGrades %&gt;% inner_join(GPAConversion)\n\n\nfull_join()\nJoins two data sets together (adding variables from right to left data sets), keeping all rows of both left and right datasets\nGrades %&gt;% full_join(CourseSizes, by = c(\"sessionID\" = \"sessionID\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction/Operator\nAction\nExample\n\n\n\n\nsemi_join()\nKeep only rows in left that have a match in right\nGrades %&gt;% semi_join(LargeSections)\n\n\nanti_join()\nKeep only rows in left that don’t have a match in right\nGrades %&gt;% anti_join(CrossListedSections)",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/09-Join.html#footnotes",
    "href": "src/09-Join.html#footnotes",
    "title": "Joining Two Data Frames",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is also a right_join() that adds variables in the reverse direction from the left table to the right table, but we do not really need it as we can always switch the roles of the two tables.↩︎\nThey are courses that are cross-listed in multiple departments!↩︎\nImportant: To avoid repeatedly re-reading the files, start the data import chunk with {r cache = TRUE} rather than the usual {r}.↩︎",
    "crumbs": [
      "Src",
      "Joining Two Data Frames"
    ]
  },
  {
    "objectID": "src/11-Mini_Project.html",
    "href": "src/11-Mini_Project.html",
    "title": "Mini-Project",
    "section": "",
    "text": "Apply data wrangling and visualization skills to a new data set\n\nCreate a new Rmd file (save it as 11-Mini_Project.Rmd). Put this file in a folder Assignment_07 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!\n\n\n\n\nKiva is a non-profit that allows people from around the world to lend small amounts to others to start or grow a business, go to school, access clean energy, etc. Since its founding in 2005, more than $1.2 billion in loans to over 3 million borrowers have been funded. In this activity, we’ll examine some lending data from 2005-2012.\n\n\nKiva has field partners who generally act as intermediaries between borrowers and Kiva (lenders). They evaluate borrower risk, post loan requests on Kiva, and process payments. The following command loads a table with data on Kiva’s field partners.\n\npartners &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/kiva_partners2.csv\")\n\nExamine the codebook for the partners table.\n\nMake a summary table with only five columns: region (`countries.region`), total number of partners, total number of loans posted, total amount raised, and average loan size per loan posted. \n\nThe four columns after region should all be on a per region basis; for example, the row for Central America should include average loan size per loan posted in Central America. Sort your table by total amount raised.\n\n\n\nDraw a map of all of the partners in Africa, with the size of the dot corresponding to the amount raised.   \n\n\n\n\n\nThe loans table below contains information about individual loans to borrowers.\n\n# a random sample of 10,000\nloans &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/kiva_loans_small.csv\")\n\nExamine the codebook for the loans table. View the loans table and browse through some of the data (e.g., different sectors, uses, countries)\n\nConsider the `loans` data and perform the following preprocessing steps and save the new data as `loans2`.\n\n\nRemove all rows where the funded amount is 0.\n\nNote that the date information about when a loan request was posted is separated out into different fields for year, month, day, hour, etc. It would be more helpful to have a single field for the exact time at which each loan was posted. We’ll do this in three steps. First, create (mutate) a new column by pasting together the year, month, and date, separated by hyphens: post_dt=paste(posted_yr, posted_mo, posted_day, sep='-'). Second, create a new column with the time: post_time=paste(posted_hr,posted_min,posted_sec, sep=':'). Third, using the ymd_hms command from lubridate, add a new column with the full date (including time of day):\npost_date=ymd_hms(paste(post_dt,post_time,sep=' ')).\n\nRepeat the previous part to add a column funded_date with the exact time at which each loan was funded.\n\nCreate a new table called loans2 that only contains the following columns from loans: loan_id,status,funded_amount,paid_amount,sector,location.country,lat,lon,partner_id,post_date,funded_date\n\n\nConsider the `loans2` table. \n\n\nFind the top 5 countries by number of loans.\n\nFind the top 5 countries by total funded loan amount.\n\n\nMake a scatterplot with the number of loans in each sector on the x-axis and the average loan size in each sector on the y-axis. Instead of using points, use text with each sector's name as the glyph.\n\nHint: start by wrangling the data into glyph-ready form, with one row corresponding to one glyph.\n\n\n\n\nJoin the `countries.region` variable from the `partners` table onto the `loans2` table, in order to have a region associated with each loan. Save the output table as `loans3`.  \n\n\n\nConsider the following:\n\n\nStarting with loans3, create a new table DefaultData that only includes those loans whose status is either “defaulted” or “paid”.\n\nMake a density plot of the funded amount, split by loan status (“defaulted” or “paid”). That is, your plot should have two separate density curves on top of each other. Interpret the plot.\nMake a bar chart with 10 bars, such that each bar is a $1000 range of funding amount; that is, the first bar should be for $0-$1000, the second for $1001-$2000, and so forth. The bars should all have height equal to 1, but be filled with two colors: one for the percentage of loans in that interval bin that defaulted and one for the percentage that were paid back in full. Interpret your graphic.\n\nStarting with the data in DefaultData from part (a), make a table with four columns: partner_id, number of defaulted loans through that partner, number of loans completely paid back through that partner, and percentage of loans defaulted (the second column divided by the sum of the second and third columns). Sort your table from highest default percentage to lowest, and print out only those with at least a 10% default percentage. Hint: start by filtering out partners that have not had any defaulted loans.\n\n\nConsider the following:\n\n\nUse the command days_to_fund = difftime(funded_date, post_date, units=\"days\") within mutate() to add a column to the loans3 data that has the number of days between the time a loan was posted and the time it was funded. Save your new table as loans4.\n\nThe days_to_fund should always be positive, but there are some negative values, most likely due to data entry errors. Filter these out of the data table loans4.\n\nMake a bar chart with days to fund by region. Reorder the x-axis so that the regions are in order of ascending days to fund.\n\nMake a bar chart with days to fund by sector. Reorder the x-axis so that the sectors are in order of ascending days to fund.\n\nMake a scatterplot with funded_amount on the x-axis and days_to_fund on the y-axis. Color the points by region and set the transparency to 0.2. Add linear trendlines for each region. What are the trends?\n\n\nNow that you are more familiar with the data, investigate your own research question and address it with a summary table or a data visualization.",
    "crumbs": [
      "Src",
      "Mini-Project"
    ]
  },
  {
    "objectID": "src/11-Mini_Project.html#learning-goals",
    "href": "src/11-Mini_Project.html#learning-goals",
    "title": "Mini-Project",
    "section": "",
    "text": "Apply data wrangling and visualization skills to a new data set\n\nCreate a new Rmd file (save it as 11-Mini_Project.Rmd). Put this file in a folder Assignment_07 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!",
    "crumbs": [
      "Src",
      "Mini-Project"
    ]
  },
  {
    "objectID": "src/11-Mini_Project.html#data-kiva",
    "href": "src/11-Mini_Project.html#data-kiva",
    "title": "Mini-Project",
    "section": "",
    "text": "Kiva is a non-profit that allows people from around the world to lend small amounts to others to start or grow a business, go to school, access clean energy, etc. Since its founding in 2005, more than $1.2 billion in loans to over 3 million borrowers have been funded. In this activity, we’ll examine some lending data from 2005-2012.\n\n\nKiva has field partners who generally act as intermediaries between borrowers and Kiva (lenders). They evaluate borrower risk, post loan requests on Kiva, and process payments. The following command loads a table with data on Kiva’s field partners.\n\npartners &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/kiva_partners2.csv\")\n\nExamine the codebook for the partners table.\n\nMake a summary table with only five columns: region (`countries.region`), total number of partners, total number of loans posted, total amount raised, and average loan size per loan posted. \n\nThe four columns after region should all be on a per region basis; for example, the row for Central America should include average loan size per loan posted in Central America. Sort your table by total amount raised.\n\n\n\nDraw a map of all of the partners in Africa, with the size of the dot corresponding to the amount raised.   \n\n\n\n\n\nThe loans table below contains information about individual loans to borrowers.\n\n# a random sample of 10,000\nloans &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/kiva_loans_small.csv\")\n\nExamine the codebook for the loans table. View the loans table and browse through some of the data (e.g., different sectors, uses, countries)\n\nConsider the `loans` data and perform the following preprocessing steps and save the new data as `loans2`.\n\n\nRemove all rows where the funded amount is 0.\n\nNote that the date information about when a loan request was posted is separated out into different fields for year, month, day, hour, etc. It would be more helpful to have a single field for the exact time at which each loan was posted. We’ll do this in three steps. First, create (mutate) a new column by pasting together the year, month, and date, separated by hyphens: post_dt=paste(posted_yr, posted_mo, posted_day, sep='-'). Second, create a new column with the time: post_time=paste(posted_hr,posted_min,posted_sec, sep=':'). Third, using the ymd_hms command from lubridate, add a new column with the full date (including time of day):\npost_date=ymd_hms(paste(post_dt,post_time,sep=' ')).\n\nRepeat the previous part to add a column funded_date with the exact time at which each loan was funded.\n\nCreate a new table called loans2 that only contains the following columns from loans: loan_id,status,funded_amount,paid_amount,sector,location.country,lat,lon,partner_id,post_date,funded_date\n\n\nConsider the `loans2` table. \n\n\nFind the top 5 countries by number of loans.\n\nFind the top 5 countries by total funded loan amount.\n\n\nMake a scatterplot with the number of loans in each sector on the x-axis and the average loan size in each sector on the y-axis. Instead of using points, use text with each sector's name as the glyph.\n\nHint: start by wrangling the data into glyph-ready form, with one row corresponding to one glyph.\n\n\n\n\nJoin the `countries.region` variable from the `partners` table onto the `loans2` table, in order to have a region associated with each loan. Save the output table as `loans3`.  \n\n\n\nConsider the following:\n\n\nStarting with loans3, create a new table DefaultData that only includes those loans whose status is either “defaulted” or “paid”.\n\nMake a density plot of the funded amount, split by loan status (“defaulted” or “paid”). That is, your plot should have two separate density curves on top of each other. Interpret the plot.\nMake a bar chart with 10 bars, such that each bar is a $1000 range of funding amount; that is, the first bar should be for $0-$1000, the second for $1001-$2000, and so forth. The bars should all have height equal to 1, but be filled with two colors: one for the percentage of loans in that interval bin that defaulted and one for the percentage that were paid back in full. Interpret your graphic.\n\nStarting with the data in DefaultData from part (a), make a table with four columns: partner_id, number of defaulted loans through that partner, number of loans completely paid back through that partner, and percentage of loans defaulted (the second column divided by the sum of the second and third columns). Sort your table from highest default percentage to lowest, and print out only those with at least a 10% default percentage. Hint: start by filtering out partners that have not had any defaulted loans.\n\n\nConsider the following:\n\n\nUse the command days_to_fund = difftime(funded_date, post_date, units=\"days\") within mutate() to add a column to the loans3 data that has the number of days between the time a loan was posted and the time it was funded. Save your new table as loans4.\n\nThe days_to_fund should always be positive, but there are some negative values, most likely due to data entry errors. Filter these out of the data table loans4.\n\nMake a bar chart with days to fund by region. Reorder the x-axis so that the regions are in order of ascending days to fund.\n\nMake a bar chart with days to fund by sector. Reorder the x-axis so that the sectors are in order of ascending days to fund.\n\nMake a scatterplot with funded_amount on the x-axis and days_to_fund on the y-axis. Color the points by region and set the transparency to 0.2. Add linear trendlines for each region. What are the trends?\n\n\nNow that you are more familiar with the data, investigate your own research question and address it with a summary table or a data visualization.",
    "crumbs": [
      "Src",
      "Mini-Project"
    ]
  },
  {
    "objectID": "src/13-EDA.html",
    "href": "src/13-EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Understand the first steps that should be taken when you encounter a new data set\nDevelop comfort in knowing how to explore data to understand it\nDevelop comfort in formulating research questions\n\nCreate a new Rmd file (save it as 13-EDA.Rmd). Put this file in a folder Assignment_08 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!\n\n\n\n\nWhat are the first steps to take when you start a project or get a hold of a new data set?\nIn a data science project, you will typically have a question or idea brought to you: How are people interacting with the new version of my software? How is weather in St. Paul changing in the last decade? What type of people are most likely to enroll in Obamacare?\nYou will sometimes be given a dataset when asked one of these questions, or more often, a general description of the data you could get if you talked to the right people or interacted with the right software systems.\nIn this section, we will talk about Exploratory Data Analysis (EDA), a name given to the process of\n\n“getting to know” a dataset, and\ntrying to identify any meaningful insights within it.\n\nGrolemund and Wickham visualize the place of this “Understand” process with a simple diagram:\n\n\n\n\n\nThe process of EDA, as described by Grolemund and Wickham.\n\n\n\n\nWe view the process similarly:\n\nUnderstand the basic data that is available to you.\nVisualize and describe the variables that seem most interesting or relevant.\nFormulate a research question.\nAnalyze the data related to the research question, starting from simple analyses to more complex ones.\nInterpret your findings, refine your research question, and return to step 4.\n\n\n\n\nStart by understanding the data that is available to you. If you have a codebook, you have struck gold! If not (the more common case), you’ll need to do some detective work that often involves talking to people. At this stage, ask yourself:\n\nWhere does my data come from? How was it collected?1\nIs there a codebook? If not, how can I learn about it?\nAre there people I can reach out to who have experience with this data?\n\nNext, you need to load the data and clean it. Once the data is loaded, ask yourself about each table:\n\nWhat is an observation?\nHow many observations are there?\nWhat is the meaning of each variable?\nWhat is the type of each variable (date, location, string, factor, number, boolean, etc.)?\n\nSome great methods to start with are the functions\n\nstr() to learn about the numbers of variables and observations as well as the classes of variables\nhead() to view the top of the data table (can specify the number of rows with n= )\ntail() to view the bottom of the data table\n\nHere is an example:\n\ncrime &lt;- read_csv(\"http://datasets.flowingdata.com/crimeRatesByState2005.csv\")\nstr(crime)\n\nspc_tbl_ [52 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state              : chr [1:52] \"United States\" \"Alabama\" \"Alaska\" \"Arizona\" ...\n $ murder             : num [1:52] 5.6 8.2 4.8 7.5 6.7 6.9 3.7 2.9 4.4 35.4 ...\n $ forcible_rape      : num [1:52] 31.7 34.3 81.1 33.8 42.9 26 43.4 20 44.7 30.2 ...\n $ robbery            : num [1:52] 140.7 141.4 80.9 144.4 91.1 ...\n $ aggravated_assault : num [1:52] 291 248 465 327 387 ...\n $ burglary           : num [1:52] 727 954 622 948 1085 ...\n $ larceny_theft      : num [1:52] 2286 2650 2599 2965 2711 ...\n $ motor_vehicle_theft: num [1:52] 417 288 391 924 262 ...\n $ population         : num [1:52] 2.96e+08 4.55e+06 6.69e+05 5.97e+06 2.78e+06 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   murder = col_double(),\n  ..   forcible_rape = col_double(),\n  ..   robbery = col_double(),\n  ..   aggravated_assault = col_double(),\n  ..   burglary = col_double(),\n  ..   larceny_theft = col_double(),\n  ..   motor_vehicle_theft = col_double(),\n  ..   population = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nhead(crime)\n\n# A tibble: 6 × 9\n  state   murder forcible_rape robbery aggravated_assault burglary larceny_theft\n  &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 United…    5.6          31.7   141.                291.     727.         2286.\n2 Alabama    8.2          34.3   141.                248.     954.         2650 \n3 Alaska     4.8          81.1    80.9               465.     622.         2599.\n4 Arizona    7.5          33.8   144.                327.     948.         2965.\n5 Arkans…    6.7          42.9    91.1               387.    1085.         2711.\n6 Califo…    6.9          26     176.                317.     693.         1916.\n# ℹ 2 more variables: motor_vehicle_theft &lt;dbl&gt;, population &lt;dbl&gt;\n\ntail(crime)\n\n# A tibble: 6 × 9\n  state   murder forcible_rape robbery aggravated_assault burglary larceny_theft\n  &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Vermont    1.3          23.3    11.7               83.5     492.         1686.\n2 Virgin…    6.1          22.7    99.2              155.      392.         2035 \n3 Washin…    3.3          44.7    92.1              206.      960.         3150.\n4 West V…    4.4          17.7    44.6              206.      621.         1794 \n5 Wiscon…    3.5          20.6    82.2              135.      441.         1993.\n6 Wyoming    2.7          24      15.3              188.      476.         2534.\n# ℹ 2 more variables: motor_vehicle_theft &lt;dbl&gt;, population &lt;dbl&gt;\n\n\nFinally, ask yourself about the relationships between tables:\n\nWhat variables are keys and link the tables (i.e., which variables can you use in join commands)?\n\n\n\n\nOnce you have the data loaded and cleaned, it is usually helpful to do some univariate visualization; e.g., plotting histograms, densities, and box plots of different variables. You might ask questions such as:\n\nWhat do you see that is interesting?\nWhich values are most common or unusual (outliers)?\nIs there a lot of missing data?\nWhat type of variation occurs within the individual variables?\nWhat might be causing the interesting findings?\nHow could you figure out whether your ideas are correct?\n\nOnce you have done some univariate visualization, you might examine the covariation between different variables. One convenient way to do this is with a pairs plot.\nHere are three different versions of such plots based on 2005 crime data (rates per 100,000 population) from Chapter 6 of Visualize This, by Nathan Yau. The main point of such plots is not necessarily to draw any conclusions, but help generate more specific research questions and hypotheses.\n\ncrime2 &lt;- crime %&gt;%\n  filter(state != \"District of Columbia\", state != \"United States\")\npairs(crime2[, 2:9], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\nggpairs(crime2[, 2:9], aes(alpha = 0.4)) + theme_minimal()\n\n\n\n\n\n\n\n\n\nlowerFn &lt;- function(data, mapping, method = \"lm\", ...) {\n  p &lt;- ggplot(data = data, mapping = mapping) +\n    geom_point(colour = \"blue\") +\n    geom_smooth(method = method, color = \"red\", ...)\n  p\n}\n\nggpairs(\n  crime2[, 2:9],\n  lower = list(continuous = wrap(lowerFn, method = \"lm\")),\n  diag = list(continuous = wrap(\"barDiag\", colour = \"blue\")),\n  upper = list(continuous = wrap(\"cor\", size = 5))\n)\n\n\n\n\n\n\n\n\n\n\n\nYou will often end up with a lot of data, and it can be easy to be overwhelmed. How should you get started? One easy idea is to brainstorm ideas for research questions, and pick one that seems promising. This process is much easier with more than one brain! You will often be working off of a broad question posed by your business, organization, or supervisor, and be thinking about how to narrow it down. To do so, you can again revisit questions like “What patterns do you see?” or “Why might they be occurring?”\n\n\n\nHere are some exploratory data analysis examples I like:\n\nCrime mapping\nChanges in fine particle air pollution in the US\nMarried people have more sex2\nTropical storm analysis\n\n\n\n\nLet’s practice these steps using data about flight delays from Kaggle. You have access to the data for airlines, airports, and flights.\nThere are three tables of data:\n\n\nError: 'data/airlines.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: 'data/airports.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: 'data/flights_jan_jul_sample2.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\nairlines &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/airlines.csv\")\nairports &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/airports.csv\")\nflights &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/flights_jan_jul_sample2.csv\")\n\nNote that the full set of flight data has more than 5.8 million flights. To start, I have given you a subset that includes all flights in the first 15 days of January 2015 and in the first 15 days of July 2015. If at the end you wish to try your analysis on the whole data set, you can download the original .csv file from the Kaggle page and substitute a link to it in the code chunk above.\nThere is a ton of data here, and it can be easy to be overwhelmed. We are going to focus our exploration a bit by considering the following broad research question: which flights are most likely to be delayed?\n\nWhere does this data come from? Who collected it?   \n\n\nThis data set comes with a codebook on the Kaggle site. Have a look at the codebook to understand which variables are contained in each of the three tables. \n\nWhat is the unit of observation for each table?\nWhat are the levels of `CANCELLATION_REASON` and what do they mean? \n  \n\n\nWhat variables link the three tables? How could you join data from one table to another?\n  \n\n\nUse some univariate and bivariate visualizations to start to explore the questions mentioned above:\n\n* What do you see that is interesting? \n* Which values are most common or unusual (outliers)?\n* Is there a lot of missing data?\n* What type of variation occurs within the individual variables?\n* What might be causing the interesting findings?\n* How could you figure out whether your ideas are correct?\n\n\n\nBased on your preliminary visualizations and exploration of the date, formulate a more specific research question/hypothesis within this broad area of understanding the causes of flight delays.\n\n\n\nDevelop a single visualization to share with the class that tells a story about your more specific research question/hypothesis. Note: the story may very well be something along the lines of \"we thought variable X would affect flight delays in way Y, but the evidence does not support that.\"",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#learning-goals",
    "href": "src/13-EDA.html#learning-goals",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Understand the first steps that should be taken when you encounter a new data set\nDevelop comfort in knowing how to explore data to understand it\nDevelop comfort in formulating research questions\n\nCreate a new Rmd file (save it as 13-EDA.Rmd). Put this file in a folder Assignment_08 in your COMP_STAT_112 folder.\n\nMake sure to add alt text using fig.alt!",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#first-steps-of-a-data-analysis",
    "href": "src/13-EDA.html#first-steps-of-a-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "What are the first steps to take when you start a project or get a hold of a new data set?\nIn a data science project, you will typically have a question or idea brought to you: How are people interacting with the new version of my software? How is weather in St. Paul changing in the last decade? What type of people are most likely to enroll in Obamacare?\nYou will sometimes be given a dataset when asked one of these questions, or more often, a general description of the data you could get if you talked to the right people or interacted with the right software systems.\nIn this section, we will talk about Exploratory Data Analysis (EDA), a name given to the process of\n\n“getting to know” a dataset, and\ntrying to identify any meaningful insights within it.\n\nGrolemund and Wickham visualize the place of this “Understand” process with a simple diagram:\n\n\n\n\n\nThe process of EDA, as described by Grolemund and Wickham.\n\n\n\n\nWe view the process similarly:\n\nUnderstand the basic data that is available to you.\nVisualize and describe the variables that seem most interesting or relevant.\nFormulate a research question.\nAnalyze the data related to the research question, starting from simple analyses to more complex ones.\nInterpret your findings, refine your research question, and return to step 4.",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#understand-the-basic-data",
    "href": "src/13-EDA.html#understand-the-basic-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Start by understanding the data that is available to you. If you have a codebook, you have struck gold! If not (the more common case), you’ll need to do some detective work that often involves talking to people. At this stage, ask yourself:\n\nWhere does my data come from? How was it collected?1\nIs there a codebook? If not, how can I learn about it?\nAre there people I can reach out to who have experience with this data?\n\nNext, you need to load the data and clean it. Once the data is loaded, ask yourself about each table:\n\nWhat is an observation?\nHow many observations are there?\nWhat is the meaning of each variable?\nWhat is the type of each variable (date, location, string, factor, number, boolean, etc.)?\n\nSome great methods to start with are the functions\n\nstr() to learn about the numbers of variables and observations as well as the classes of variables\nhead() to view the top of the data table (can specify the number of rows with n= )\ntail() to view the bottom of the data table\n\nHere is an example:\n\ncrime &lt;- read_csv(\"http://datasets.flowingdata.com/crimeRatesByState2005.csv\")\nstr(crime)\n\nspc_tbl_ [52 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state              : chr [1:52] \"United States\" \"Alabama\" \"Alaska\" \"Arizona\" ...\n $ murder             : num [1:52] 5.6 8.2 4.8 7.5 6.7 6.9 3.7 2.9 4.4 35.4 ...\n $ forcible_rape      : num [1:52] 31.7 34.3 81.1 33.8 42.9 26 43.4 20 44.7 30.2 ...\n $ robbery            : num [1:52] 140.7 141.4 80.9 144.4 91.1 ...\n $ aggravated_assault : num [1:52] 291 248 465 327 387 ...\n $ burglary           : num [1:52] 727 954 622 948 1085 ...\n $ larceny_theft      : num [1:52] 2286 2650 2599 2965 2711 ...\n $ motor_vehicle_theft: num [1:52] 417 288 391 924 262 ...\n $ population         : num [1:52] 2.96e+08 4.55e+06 6.69e+05 5.97e+06 2.78e+06 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   murder = col_double(),\n  ..   forcible_rape = col_double(),\n  ..   robbery = col_double(),\n  ..   aggravated_assault = col_double(),\n  ..   burglary = col_double(),\n  ..   larceny_theft = col_double(),\n  ..   motor_vehicle_theft = col_double(),\n  ..   population = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nhead(crime)\n\n# A tibble: 6 × 9\n  state   murder forcible_rape robbery aggravated_assault burglary larceny_theft\n  &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 United…    5.6          31.7   141.                291.     727.         2286.\n2 Alabama    8.2          34.3   141.                248.     954.         2650 \n3 Alaska     4.8          81.1    80.9               465.     622.         2599.\n4 Arizona    7.5          33.8   144.                327.     948.         2965.\n5 Arkans…    6.7          42.9    91.1               387.    1085.         2711.\n6 Califo…    6.9          26     176.                317.     693.         1916.\n# ℹ 2 more variables: motor_vehicle_theft &lt;dbl&gt;, population &lt;dbl&gt;\n\ntail(crime)\n\n# A tibble: 6 × 9\n  state   murder forcible_rape robbery aggravated_assault burglary larceny_theft\n  &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Vermont    1.3          23.3    11.7               83.5     492.         1686.\n2 Virgin…    6.1          22.7    99.2              155.      392.         2035 \n3 Washin…    3.3          44.7    92.1              206.      960.         3150.\n4 West V…    4.4          17.7    44.6              206.      621.         1794 \n5 Wiscon…    3.5          20.6    82.2              135.      441.         1993.\n6 Wyoming    2.7          24      15.3              188.      476.         2534.\n# ℹ 2 more variables: motor_vehicle_theft &lt;dbl&gt;, population &lt;dbl&gt;\n\n\nFinally, ask yourself about the relationships between tables:\n\nWhat variables are keys and link the tables (i.e., which variables can you use in join commands)?",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#visualize-and-describe-the-data",
    "href": "src/13-EDA.html#visualize-and-describe-the-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Once you have the data loaded and cleaned, it is usually helpful to do some univariate visualization; e.g., plotting histograms, densities, and box plots of different variables. You might ask questions such as:\n\nWhat do you see that is interesting?\nWhich values are most common or unusual (outliers)?\nIs there a lot of missing data?\nWhat type of variation occurs within the individual variables?\nWhat might be causing the interesting findings?\nHow could you figure out whether your ideas are correct?\n\nOnce you have done some univariate visualization, you might examine the covariation between different variables. One convenient way to do this is with a pairs plot.\nHere are three different versions of such plots based on 2005 crime data (rates per 100,000 population) from Chapter 6 of Visualize This, by Nathan Yau. The main point of such plots is not necessarily to draw any conclusions, but help generate more specific research questions and hypotheses.\n\ncrime2 &lt;- crime %&gt;%\n  filter(state != \"District of Columbia\", state != \"United States\")\npairs(crime2[, 2:9], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\nggpairs(crime2[, 2:9], aes(alpha = 0.4)) + theme_minimal()\n\n\n\n\n\n\n\n\n\nlowerFn &lt;- function(data, mapping, method = \"lm\", ...) {\n  p &lt;- ggplot(data = data, mapping = mapping) +\n    geom_point(colour = \"blue\") +\n    geom_smooth(method = method, color = \"red\", ...)\n  p\n}\n\nggpairs(\n  crime2[, 2:9],\n  lower = list(continuous = wrap(lowerFn, method = \"lm\")),\n  diag = list(continuous = wrap(\"barDiag\", colour = \"blue\")),\n  upper = list(continuous = wrap(\"cor\", size = 5))\n)",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#formulate-a-research-question",
    "href": "src/13-EDA.html#formulate-a-research-question",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "You will often end up with a lot of data, and it can be easy to be overwhelmed. How should you get started? One easy idea is to brainstorm ideas for research questions, and pick one that seems promising. This process is much easier with more than one brain! You will often be working off of a broad question posed by your business, organization, or supervisor, and be thinking about how to narrow it down. To do so, you can again revisit questions like “What patterns do you see?” or “Why might they be occurring?”",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#examples",
    "href": "src/13-EDA.html#examples",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here are some exploratory data analysis examples I like:\n\nCrime mapping\nChanges in fine particle air pollution in the US\nMarried people have more sex2\nTropical storm analysis",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#practice-flight-data",
    "href": "src/13-EDA.html#practice-flight-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Let’s practice these steps using data about flight delays from Kaggle. You have access to the data for airlines, airports, and flights.\nThere are three tables of data:\n\n\nError: 'data/airlines.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: 'data/airports.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\nError: 'data/flights_jan_jul_sample2.csv' does not exist in current working directory ('C:/Users/Amin Alhashim/Documents/GitHub/alhashimphd/mac-comp112website-f24/src').\n\n\n\nairlines &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/airlines.csv\")\nairports &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/airports.csv\")\nflights &lt;- read_csv(\"https://bcheggeseth.github.io/112_spring_2023/data/flights_jan_jul_sample2.csv\")\n\nNote that the full set of flight data has more than 5.8 million flights. To start, I have given you a subset that includes all flights in the first 15 days of January 2015 and in the first 15 days of July 2015. If at the end you wish to try your analysis on the whole data set, you can download the original .csv file from the Kaggle page and substitute a link to it in the code chunk above.\nThere is a ton of data here, and it can be easy to be overwhelmed. We are going to focus our exploration a bit by considering the following broad research question: which flights are most likely to be delayed?\n\nWhere does this data come from? Who collected it?   \n\n\nThis data set comes with a codebook on the Kaggle site. Have a look at the codebook to understand which variables are contained in each of the three tables. \n\nWhat is the unit of observation for each table?\nWhat are the levels of `CANCELLATION_REASON` and what do they mean? \n  \n\n\nWhat variables link the three tables? How could you join data from one table to another?\n  \n\n\nUse some univariate and bivariate visualizations to start to explore the questions mentioned above:\n\n* What do you see that is interesting? \n* Which values are most common or unusual (outliers)?\n* Is there a lot of missing data?\n* What type of variation occurs within the individual variables?\n* What might be causing the interesting findings?\n* How could you figure out whether your ideas are correct?\n\n\n\nBased on your preliminary visualizations and exploration of the date, formulate a more specific research question/hypothesis within this broad area of understanding the causes of flight delays.\n\n\n\nDevelop a single visualization to share with the class that tells a story about your more specific research question/hypothesis. Note: the story may very well be something along the lines of \"we thought variable X would affect flight delays in way Y, but the evidence does not support that.\"",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/13-EDA.html#footnotes",
    "href": "src/13-EDA.html#footnotes",
    "title": "Exploratory Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nParticularly important questions about how it was collected include WHO (whether it is a sample of a larger data set, and, if so, how the sampling was done? Randomly? All cases during a specific time frame? All data for a selected set of users?), WHEN (is this current data or historical? what events may have had an impact?), WHAT (what variables were measured? how was it measured, self-reported through a questionnaire or measured directly?), WHY (who funded the data collection? for what purposes what the data collected? to whose benefit was the data collected?) Answers to such questions strongly impact the conclusions you will be able to draw from the data.↩︎\nThe graphics in this one are a bit more developed than you would really see in an exploratory analysis, but I think the progression of visualizations is interesting and follows an exploratory story.↩︎",
    "crumbs": [
      "Src",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "src/15-Data_Scraping.html#learning-goals",
    "href": "src/15-Data_Scraping.html#learning-goals",
    "title": "(PART) Additional Topics",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nDevelop comfort in using CSS Selectors and the Selector Gadget to identify data of interest within a website\nUse html_nodes and html_text within rvest packages to scrape data from websites using CSS selectors\nDevelop initial comfort in cleaning text data\n\nYou can download a template .Rmd of this activity here.",
    "crumbs": [
      "Src",
      "(PART) Additional Topics"
    ]
  },
  {
    "objectID": "src/15-Data_Scraping.html#scraping-data",
    "href": "src/15-Data_Scraping.html#scraping-data",
    "title": "(PART) Additional Topics",
    "section": "Scraping Data",
    "text": "Scraping Data\nAdditional readings:\n\nrvest\n\nCSS selectors\n\nWhile a great deal of data is available via Web APIs, data download link, and data warehouses, not all of it is. Programs can use a process called web scraping to collect data that is available to humans (via web browsers) but not computer programs.\n\nFinding CSS Selectors\nIn order to gather information from a webpage, we must learn the language used to identify patterns of specific information. For example, on the Macalester Registrar’s Fall 2021 Class Schedule you can visually see that the data is represented in a table. The first column shows the course number, the second the title, etc.\n\n\nError in knitr::include_graphics(\"images/registrar.png\"): Cannot find the file(s): \"images/registrar.png\"\n\n\nWe will identify data in a webpage using a pattern matching language called CSS Selectors that can refer to specific patterns in HTML, the language used to write web pages.\nFor example, the CSS selector “a” selects all hyperlinks in a webpage (“a” represents “anchor” links in HTML), “table &gt; tr &gt; td:nth-child(2)” would find the second column of an HTML table.\nWarning: Websites change often! So if you are going to scrape a lot of data, it is probably worthwhile to save and date a copy of the website. Otherwise, you may return after some time and your scraping code will include all of the wrong CSS selectors.\nAlthough you can learn how to use CSS Selectors by hand, we will use a shortcut by installing the Selector Gadget for Chrome.\nYes, you must have the Chrome web browser installed to do this! You “teach” the Selector Gadget which data you are interested in on a web page, and it will show you the CSS Selector for this data. We will eventually use this selector in R.\nFirst watch the Selector Gadget video and install the Chrome Extension.\nHead over to the Macalester Registrar’s Fall 2021 class schedule. Click the selector gadget icon in the top right corner of Chrome (you may need to click on the puzzle piece and then the pin icon next to the Selector). As you mouse over the webpage, different parts will be highlighted in orange. Click on the first course number, AMST 194-01. You’ll notice that the Selector Gadget information in the lower right describes what you clicked on:\n\n\nError in knitr::include_graphics(\"images/SelectorGadgetActionShot2.png\"): Cannot find the file(s): \"images/SelectorGadgetActionShot2.png\"\n\n\nScroll through the page to verify that only the information you intend (the course number) is selected. The selector panel shows the CSS selector (.class-schedule-course-number) and the number of matches for that CSS selector (763).\nNow that we have the selector for the course number, let’s find the selector for the days of the week. Clear the selector by clicking the “Clear” button on the result pane, and then click the MWF under days for AMST 194-01. You will notice that the selector was too broad and highlighted information we don’t want. You need to teach Selector Gadget a correct selector by clicking the information you don’t want to turn it red. Once this is done, you should have 763 matches and a CSS selector of .class-schedule-course-title+ .class-schedule-label.\n\n\nError in knitr::include_graphics(\"images/SelectorGadgetDay2.png\"): Cannot find the file(s): \"images/SelectorGadgetDay2.png\"\n\n\n\nRepeat the process above to find the correct selectors for the following fields. Make sure that each matches 763 results:\n\n\nCourse Number\nCourse Name\nDay\nTime\nRoom\nInstructor\nAvail. / Max\n\n\n\nRetrieving Data Using rvest and CSS Selector\nNow that we have identified CSS selectors for the information we need, let’s fetch the data in R. We will be using the rvest package, which retrieves information from a webpage and turns it into R data tables:\n\nfall2021 &lt;- read_html(\"https://www.macalester.edu/registrar/schedules/2021fall/class-schedule\")\n\nOnce the webpage is loaded, we can retrieve data using the CSS selectors we specified earlier. The following code retrieves the course numbers and names as a vector:\n\n# Retrieve and inspect course numbers\ncourse_nums &lt;-\n  fall2021 %&gt;%\n  html_nodes(\".class-schedule-course-number\") %&gt;%\n  html_text()\nhead(course_nums)\n\ncharacter(0)\n\n# Retrieve and inspect course names\ncourse_names &lt;-\n  fall2021 %&gt;%\n  html_nodes(\".class-schedule-course-title\") %&gt;%\n  html_text()\nhead(course_names)\n\ncharacter(0)\n\n\nAt the end of each course number entry is a 5 digit number, which is CRN number used internally for the registrar’s office. Let’s put that in its own variable.\nTo clean up the course number data, we can chop off the last 6 characters of each entry using the command sub_str from the stringr package:1\n\ncourse_nums_clean &lt;- stringr::str_sub(course_nums, end = nchar(course_nums) - 6)\nhead(course_nums_clean)\n\ncharacter(0)\n\ncrn &lt;- stringr::str_sub(course_nums, start = nchar(course_nums) - 4)\nhead(crn)\n\ncharacter(0)\n\ncourse_df &lt;- tibble(number = course_nums_clean,crn = crn, name = course_names)\nhead(course_df)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: number &lt;chr&gt;, crn &lt;chr&gt;, name &lt;chr&gt;\n\n\nWhat happens when we try to grab the instructors in the same manner?\n\ncourse_instructors &lt;-\n  fall2021 %&gt;%\n  html_nodes(\".class-schedule-label:nth-child(6)\") %&gt;%\n  html_text()\nhead(course_instructors)\n\ncharacter(0)\n\n\nIn front of each entry is a bunch of spaces and “Instructor:”, which we don’t really need stored in every entry of our data table. This is because the website is set up in a responsive manner to change when the browser window is narrowed or you are on a mobile device.\n\n\nError in knitr::include_graphics(\"images/registrar-narrow.png\"): Cannot find the file(s): \"images/registrar-narrow.png\"\n\n\nTo clean up the data, we can chop off the white space with trimws and then first 12 characters of each entry using the command sub_str from the stringr package:2\n\ncourse_instructors_short &lt;- stringr::str_sub(trimws(course_instructors), start = 13)\nhead(course_instructors_short)\n\ncharacter(0)\n\n\n\nCreate a data table that contains all the information about courses you found selectors for earlier (7 columns). Do not include any extraneous information like \"Instructor: \".\n\n\n\nCreate a chart that shows the number of sections offered per department. *Hint: The department is a substring of the course number*.^[Yes, COMP, STAT, and MATH are the same department, but for this exercise you can just show the results by four letter department code, e.g., with COMP, STAT, and MATH separate.] \n\n\n\nAnalyze the typical length of course names by department. To do so, create a `dplyr` pipeline that creates a new data table based on your courses data table, with the following changes:\n\n\nNew columns for the length of the title of a course. Hint: nchar.\nRemove departments that have fewer than 10 sections of courses. To do so, group by department, then remove observations in groups with fewer than 10 sections (Hint: use filter with n()). Then ungroup the data so it flattens it back into a regular data table. This is one of the rare cases when we will use group_by without summarize.\nCreate a visualization of the differences across groups in lengths of course names. Think carefully about the visualization you should be using!\n\nNote: If you’d like to see the html code for a website, in Chrome go to View &gt; Developer &gt; Developer Tools. Through this, we can see that all of the details about the course such as gen. ed. requirements, distributional requirements, and course description are contained on external links the correspond to the CRN numbers (#####): https://webapps.macalester.edu/registrardata/classdata/Fall2021/##### If you’d like a challenge, use that CRN variable we created above to pull out the description and gen ed. requirements for each class and join that data with the course information data table.\n\n\nWeb Scraping in Python\nIf you prefer to use Python rather than R, there are also great web scraping tools in Python. The equivalent to rvest in Python is called BeautifulSoup. Here is a reference for learning web scraping in Python:\n\nWeb Scraping with Python, by Ryan Mitchell, O’Reilly, 2015 (the second version is here)",
    "crumbs": [
      "Src",
      "(PART) Additional Topics"
    ]
  },
  {
    "objectID": "src/15-Data_Scraping.html#additional-practice-analyze-wnba-players",
    "href": "src/15-Data_Scraping.html#additional-practice-analyze-wnba-players",
    "title": "(PART) Additional Topics",
    "section": "Additional Practice: Analyze WNBA Players",
    "text": "Additional Practice: Analyze WNBA Players\n\n\nThis is an open-ended activity that asks you to analyze data from [Basketball-Reference.com](https://www.basketball-reference.com/wnba/years/2021_per_game.html). For this task, you will need to scrape at Women's NBA players statistics from 2021. You are welcome to pursue an analysis that interests you about these players. Here are some examples you might choose:\n\n* Which players are \"the most valuable\" players?\n\n* How do teams compare in terms of their players?",
    "crumbs": [
      "Src",
      "(PART) Additional Topics"
    ]
  },
  {
    "objectID": "src/15-Data_Scraping.html#footnotes",
    "href": "src/15-Data_Scraping.html#footnotes",
    "title": "(PART) Additional Topics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a simple example of text processing with regular expressions. We’ll learn how to deal with more complicated situations in the text processing unit.↩︎\nThis is a simple example of text processing with regular expressions. We’ll learn how to deal with more complicated situations in the text processing unit.↩︎",
    "crumbs": [
      "Src",
      "(PART) Additional Topics"
    ]
  },
  {
    "objectID": "src/17-SQL.html",
    "href": "src/17-SQL.html",
    "title": "SQL",
    "section": "",
    "text": "Develop comfort in composing SQL queries\nSee the connections between tidyverse verbs and SQL commands\n\nYou can download a template .Rmd of this activity here.\n\n\n\nAdditional readings and video tutorial:\n1. SQL Tutorial for Data Analysis\n2. Data Explorer Tutorial\n3. Database querying using SQL, Baumer, Kaplan, and Horton 4. More Fun Practice - Solve a Mystery\nIf you find yourself analyzing data within a medium or large organization, you will probably draw on data stored within a centralized data warehouse.\nData warehouses contain vast collections of information; far more than a desktop computer can easily analyze and they typically rely on structured data repositories called SQL databases.\nData scientists interacting with data warehouses often follow a pattern that balances the scalability of SQL databases and the expressivity of data science langauges like R and Python. After finding and understanding data relevant to a project1, a data scientist writes SQL code that creates a filtered, simplified or aggregated version of the data within the warehouse. This smaller dataset is then exported as a CSV and analyzed in R.2\nSQL databases can be conceptually thought of as a collection of data tables, where each table resembles a data frame. While there is a core subset of SQL supported by all databases, different databases (Hive, Postgres, MySQL, Oracle, RedShift, etc.) use slightly different variants of SQL. Even though SQL is a complex language, the basic data wrangling techniques we learned earlier (filtering, joining and summarizing) follow easily replicable patterns and cover the majority of needs.\n\n\n\n\nError in knitr::include_graphics(\"images/stack_exchange_explorer.jpeg\"): Cannot find the file(s): \"images/stack_exchange_explorer.jpeg\"\n\n\nWe will experiment with the Stack Exchange Data Explorer, a website that provides a SQL interface for all the data in StackExchange. StackExchange powers the StackOverflow programming question and answer site, but it also powers question and answer sites related to 126 topics including English, Travel, Bicycles, and Parenting.\nStackExchange provides an in-depth Data Explorer Tutorial. We will take a quick walk through the basics of SQL using the data explorer. I chose to analyze the Travel Data Explorer, but you could perform the steps below on any one of StackExchange data explorer sites\nHead to the Stack Exchange Data Explorer for Travel. You see a list of queries other users have created in the past. These queries are for all Stack Exchange sites, so some may not be relevant. Queries about your activity (for example, “How many upvotes do I have for each tag?”) will not be useful either if you do not have activity for the particular site.\nClick on one of them and you see the SQL code for the query. Then click the “Run Query” button to get results. For example, you might look at the number of up vs down votes for questions and answers by weekday and notice that for questions, Tuesday has the highest up vs. down vote ratio and Saturday has the lowest. You can contemplate hypotheses for this difference!\n\n\n\nLet’s experiment with our own queries. Click on “Compose Query” in the upper right, and notice the tables are shown in the right. As a reminder, a table is similar to a data frame. Each table lists the columns stored within the table and the data types for the columns. Look through the tables for Posts, Users, and Comments. Do the columns generally make sense, and correspond to the StackOverflow website? There’s a description of the tables and columns (called a schema) available on StackExchange’s Meta Q&A Site.\nNow enter your first query in the text box and click the “Run Query” button:\n\nSELECT TOP(100) Id, Title, Score, Body, Tags\nFROM Posts\n\nIn this query we already see several important features of SQL:\n\nSELECT tells SQL that a query is coming.\nTOP(100) only returns the first 100 rows.3\nId, Title, Score, Body, Tags determines what columns are included in the result\nFROM Posts determines the source dataset.\n\nFrom glancing at the results, it appears that this table contains both questions and answers. Let’s try to focus on answers. Looking again at the Schema Description, notice that there is a PostTypeId column in Posts, and a value of 1 corresponds to questions. Let’s update our query to only include questions:\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1\n\nThe SQL command WHERE is like the filter command we have been using in dplyr.4\n\nFind the title and score of Posts that have a score of at least 110. *Hint: TOP is not necessary here because you want all result rows.*\n\n\nFind posts whose title contains some place you are interested in (you pick!). *Hint: use SQL's [LIKE operator](http://www.sqltutorial.org/sql-like/).*\n\n\nNote that you can look up the actual webpage for any question using its Id. For example, if the Id is 19591, the webpage URL would be https://travel.stackexchange.com/questions/19591/. Look up a few of the questions by their Id.\nIt’s unclear how the 100 questions we saw were selected from among the over 43,000 total questions.5 Let’s try to arrange the Posts by score. The following query surfaces the top scoring question: OK we’re all adults here, so really, how on earth should I use a squat toilet?\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1\nORDER BY Score DESC\n\nThe ORDER BY ??? DESC syntax is similar to R’s arrange(). You can leave off the DESC if you want the results ordered smallest to largest.\nWe could also find the highest rated questions tagged “italy” (the top question is Does Venice Smell?):\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1 AND Tags LIKE '%italy%'\nORDER BY Score DESC\n\n\nPick two tags that interest you and you think will occur together and find the top voted posts that contain both.\n\n\n\n\n\nSo far, we have covered the equivalent of R’s selecting, filtering, and arranging. Let’s take a look at grouping and summarizing now, which has similar structures in both R and SQL. Imagine we want to see how many posts of each type there are. This query shows us that there are 44K questions and 71K answers.\n\nSELECT \nPostTypeId, COUNT(Id) numPosts\nFROM posts\nGROUP BY PostTypeId \nORDER BY PostTypeId\n\nNote two characteristics of SQL summarization here:\n\nThe GROUP BYclause indicates the table column for grouping, much like R’s group_by.\nThere is no explicit summarize. Instead, all columns that appear in the SELECT except for those listed in GROUP BY must make use of an aggregate function. COUNT(*) is one of these, and is the equivalent of R’s n(). Many other aggregate functions exist, including MAX, SUM, AVG, and many others. Every aggregate function requires a column as an argument (even COUNT() which doesn’t logically need one).\nThe aggregate column (in this case COUNT(Id)) must immediately be followed by a name that will be used for it in the results (in this case numPosts). This can be particularly useful if you want to order by the aggregated value.\n\n\nChange the previous query so it orders the result rows by the number of posts of that type. *Hint: Reuse the name you assigned to the aggregate function.*\n\n\n\nFind the most commonly used tagsets applied to posts. Note that I am not asking you to count the most common individual tags --- this would be more complex because multiple tags are squashed into the Tags field.\n\n\n\n\n\nFinally, as with R, we often want to join data from two or more tables. The types of joins in SQL are the same as we saw with R (inner, outer, left, right). Most commonly we want to perform an INNER join, which is the default if you just say JOIN.\nLet’s say we wanted to enhance the earlier query to find the highest scoring answers with some information about each user.\n\nSELECT TOP(100)\nTitle, Score, DisplayName, Reputation\nFROM posts p\nJOIN users u\nON p.OwnerUserId = u.Id\nWHERE PostTypeId =1\nORDER BY Score Desc\n\nWe see a few notable items here:\n\nThe JOIN keyword must go in between the two tables we want to join.\nEach table must be named. In this case we named posts p and users u.\nWe need to specify the relationship that joins the two tables. In this case, a posts OwnerUserId column refers to the Id column in the users table.\n\n\nCreate a query similar to the one above that identifies the authors of the top rated comments instead of posts.\n\n\n\n\n\n\nThe first few exercises will ask you to analyze Stack Exchange badges. Start at https://data.stackexchange.com/stackoverflow/query/new. For each exercise, record the query you used.\n\nCount the number of total badges that have been given out. Hint: count the number of rows in the relevant table.\n\n\n\nFind how many times each badge has been awarded, sorted from most awarded to least awarded.\n\n\n\nFind a badge that looks interesting to you. Find all the user **DisplayNames** that have received the badge, along with the date at which they received it.\n\n\n\nShow the users who have received the most badges, along with how many they have received.\n\n\nThe next few activities analyze user activity. These activities mimic the common workflow of creating datasets in SQL that you analyze in R.\n\nExport a CSV file containing information about each user: `DisplayName, Id, Reputation, and CreationDate`. Name your file `users.csv`\n\n\n\nMake a table that has each user's total number of posts and total number of upvotes, and export this file as a CSV named `posts.csv`. *Hint: Start with the posts table, join information from users, and perform some grouped summaries.*\n\n\n\nCalculate the number of comments per user, and the total number of upvotes across all comments per user (this is the sum of the `Score` variable under the Comments table) and export this as a CSV file named `comments.csv`.\n\n\n\nImport these three datasets into `R`. Visualize the relationship between the three datasets. Include at least one visualization comparing each of: \n\na) information from the user CSV and the post CSV, and \nb) information from the user CSV and comment CSV\n\nTo receive full credit your visualizations must tell a compelling story.\n\n\n\n\n\nWhile we will not cover the details of how to do so, you can indeed access SQL databases directly from RStudio. Chapters 15 and 16 of Modern Data Science with R have detailed instructions and examples on how to do so. The basic gist is to first set up a connection with the database (slightly trickier part covered in Chapter 16) and then write queries, either in SQL or – even cooler – using the common dplyr data verbs we have already learned like select, group_by, filter and inner_join. R can then automatically translate a piping sequence with many of these data verbs into an SQL query and execute that query. The reason this method is so powerful is that the bulk of the data still lives in the original database (i.e., it is not on your computer). With each query, you can access a small and customized amount of data tailored to your specific analysis needs.",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/17-SQL.html#learning-goals",
    "href": "src/17-SQL.html#learning-goals",
    "title": "SQL",
    "section": "",
    "text": "Develop comfort in composing SQL queries\nSee the connections between tidyverse verbs and SQL commands\n\nYou can download a template .Rmd of this activity here.",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/17-SQL.html#introduction-to-sql",
    "href": "src/17-SQL.html#introduction-to-sql",
    "title": "SQL",
    "section": "",
    "text": "Additional readings and video tutorial:\n1. SQL Tutorial for Data Analysis\n2. Data Explorer Tutorial\n3. Database querying using SQL, Baumer, Kaplan, and Horton 4. More Fun Practice - Solve a Mystery\nIf you find yourself analyzing data within a medium or large organization, you will probably draw on data stored within a centralized data warehouse.\nData warehouses contain vast collections of information; far more than a desktop computer can easily analyze and they typically rely on structured data repositories called SQL databases.\nData scientists interacting with data warehouses often follow a pattern that balances the scalability of SQL databases and the expressivity of data science langauges like R and Python. After finding and understanding data relevant to a project1, a data scientist writes SQL code that creates a filtered, simplified or aggregated version of the data within the warehouse. This smaller dataset is then exported as a CSV and analyzed in R.2\nSQL databases can be conceptually thought of as a collection of data tables, where each table resembles a data frame. While there is a core subset of SQL supported by all databases, different databases (Hive, Postgres, MySQL, Oracle, RedShift, etc.) use slightly different variants of SQL. Even though SQL is a complex language, the basic data wrangling techniques we learned earlier (filtering, joining and summarizing) follow easily replicable patterns and cover the majority of needs.\n\n\n\n\nError in knitr::include_graphics(\"images/stack_exchange_explorer.jpeg\"): Cannot find the file(s): \"images/stack_exchange_explorer.jpeg\"\n\n\nWe will experiment with the Stack Exchange Data Explorer, a website that provides a SQL interface for all the data in StackExchange. StackExchange powers the StackOverflow programming question and answer site, but it also powers question and answer sites related to 126 topics including English, Travel, Bicycles, and Parenting.\nStackExchange provides an in-depth Data Explorer Tutorial. We will take a quick walk through the basics of SQL using the data explorer. I chose to analyze the Travel Data Explorer, but you could perform the steps below on any one of StackExchange data explorer sites\nHead to the Stack Exchange Data Explorer for Travel. You see a list of queries other users have created in the past. These queries are for all Stack Exchange sites, so some may not be relevant. Queries about your activity (for example, “How many upvotes do I have for each tag?”) will not be useful either if you do not have activity for the particular site.\nClick on one of them and you see the SQL code for the query. Then click the “Run Query” button to get results. For example, you might look at the number of up vs down votes for questions and answers by weekday and notice that for questions, Tuesday has the highest up vs. down vote ratio and Saturday has the lowest. You can contemplate hypotheses for this difference!\n\n\n\nLet’s experiment with our own queries. Click on “Compose Query” in the upper right, and notice the tables are shown in the right. As a reminder, a table is similar to a data frame. Each table lists the columns stored within the table and the data types for the columns. Look through the tables for Posts, Users, and Comments. Do the columns generally make sense, and correspond to the StackOverflow website? There’s a description of the tables and columns (called a schema) available on StackExchange’s Meta Q&A Site.\nNow enter your first query in the text box and click the “Run Query” button:\n\nSELECT TOP(100) Id, Title, Score, Body, Tags\nFROM Posts\n\nIn this query we already see several important features of SQL:\n\nSELECT tells SQL that a query is coming.\nTOP(100) only returns the first 100 rows.3\nId, Title, Score, Body, Tags determines what columns are included in the result\nFROM Posts determines the source dataset.\n\nFrom glancing at the results, it appears that this table contains both questions and answers. Let’s try to focus on answers. Looking again at the Schema Description, notice that there is a PostTypeId column in Posts, and a value of 1 corresponds to questions. Let’s update our query to only include questions:\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1\n\nThe SQL command WHERE is like the filter command we have been using in dplyr.4\n\nFind the title and score of Posts that have a score of at least 110. *Hint: TOP is not necessary here because you want all result rows.*\n\n\nFind posts whose title contains some place you are interested in (you pick!). *Hint: use SQL's [LIKE operator](http://www.sqltutorial.org/sql-like/).*\n\n\nNote that you can look up the actual webpage for any question using its Id. For example, if the Id is 19591, the webpage URL would be https://travel.stackexchange.com/questions/19591/. Look up a few of the questions by their Id.\nIt’s unclear how the 100 questions we saw were selected from among the over 43,000 total questions.5 Let’s try to arrange the Posts by score. The following query surfaces the top scoring question: OK we’re all adults here, so really, how on earth should I use a squat toilet?\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1\nORDER BY Score DESC\n\nThe ORDER BY ??? DESC syntax is similar to R’s arrange(). You can leave off the DESC if you want the results ordered smallest to largest.\nWe could also find the highest rated questions tagged “italy” (the top question is Does Venice Smell?):\n\nSELECT TOP(100)\nId, Title, Score, Body, Tags\nFROM Posts\nWHERE PostTypeId = 1 AND Tags LIKE '%italy%'\nORDER BY Score DESC\n\n\nPick two tags that interest you and you think will occur together and find the top voted posts that contain both.\n\n\n\n\n\nSo far, we have covered the equivalent of R’s selecting, filtering, and arranging. Let’s take a look at grouping and summarizing now, which has similar structures in both R and SQL. Imagine we want to see how many posts of each type there are. This query shows us that there are 44K questions and 71K answers.\n\nSELECT \nPostTypeId, COUNT(Id) numPosts\nFROM posts\nGROUP BY PostTypeId \nORDER BY PostTypeId\n\nNote two characteristics of SQL summarization here:\n\nThe GROUP BYclause indicates the table column for grouping, much like R’s group_by.\nThere is no explicit summarize. Instead, all columns that appear in the SELECT except for those listed in GROUP BY must make use of an aggregate function. COUNT(*) is one of these, and is the equivalent of R’s n(). Many other aggregate functions exist, including MAX, SUM, AVG, and many others. Every aggregate function requires a column as an argument (even COUNT() which doesn’t logically need one).\nThe aggregate column (in this case COUNT(Id)) must immediately be followed by a name that will be used for it in the results (in this case numPosts). This can be particularly useful if you want to order by the aggregated value.\n\n\nChange the previous query so it orders the result rows by the number of posts of that type. *Hint: Reuse the name you assigned to the aggregate function.*\n\n\n\nFind the most commonly used tagsets applied to posts. Note that I am not asking you to count the most common individual tags --- this would be more complex because multiple tags are squashed into the Tags field.\n\n\n\n\n\nFinally, as with R, we often want to join data from two or more tables. The types of joins in SQL are the same as we saw with R (inner, outer, left, right). Most commonly we want to perform an INNER join, which is the default if you just say JOIN.\nLet’s say we wanted to enhance the earlier query to find the highest scoring answers with some information about each user.\n\nSELECT TOP(100)\nTitle, Score, DisplayName, Reputation\nFROM posts p\nJOIN users u\nON p.OwnerUserId = u.Id\nWHERE PostTypeId =1\nORDER BY Score Desc\n\nWe see a few notable items here:\n\nThe JOIN keyword must go in between the two tables we want to join.\nEach table must be named. In this case we named posts p and users u.\nWe need to specify the relationship that joins the two tables. In this case, a posts OwnerUserId column refers to the Id column in the users table.\n\n\nCreate a query similar to the one above that identifies the authors of the top rated comments instead of posts.",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/17-SQL.html#additional-exercises",
    "href": "src/17-SQL.html#additional-exercises",
    "title": "SQL",
    "section": "",
    "text": "The first few exercises will ask you to analyze Stack Exchange badges. Start at https://data.stackexchange.com/stackoverflow/query/new. For each exercise, record the query you used.\n\nCount the number of total badges that have been given out. Hint: count the number of rows in the relevant table.\n\n\n\nFind how many times each badge has been awarded, sorted from most awarded to least awarded.\n\n\n\nFind a badge that looks interesting to you. Find all the user **DisplayNames** that have received the badge, along with the date at which they received it.\n\n\n\nShow the users who have received the most badges, along with how many they have received.\n\n\nThe next few activities analyze user activity. These activities mimic the common workflow of creating datasets in SQL that you analyze in R.\n\nExport a CSV file containing information about each user: `DisplayName, Id, Reputation, and CreationDate`. Name your file `users.csv`\n\n\n\nMake a table that has each user's total number of posts and total number of upvotes, and export this file as a CSV named `posts.csv`. *Hint: Start with the posts table, join information from users, and perform some grouped summaries.*\n\n\n\nCalculate the number of comments per user, and the total number of upvotes across all comments per user (this is the sum of the `Score` variable under the Comments table) and export this as a CSV file named `comments.csv`.\n\n\n\nImport these three datasets into `R`. Visualize the relationship between the three datasets. Include at least one visualization comparing each of: \n\na) information from the user CSV and the post CSV, and \nb) information from the user CSV and comment CSV\n\nTo receive full credit your visualizations must tell a compelling story.",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/17-SQL.html#accessing-sql-databases-from-rstudio",
    "href": "src/17-SQL.html#accessing-sql-databases-from-rstudio",
    "title": "SQL",
    "section": "",
    "text": "While we will not cover the details of how to do so, you can indeed access SQL databases directly from RStudio. Chapters 15 and 16 of Modern Data Science with R have detailed instructions and examples on how to do so. The basic gist is to first set up a connection with the database (slightly trickier part covered in Chapter 16) and then write queries, either in SQL or – even cooler – using the common dplyr data verbs we have already learned like select, group_by, filter and inner_join. R can then automatically translate a piping sequence with many of these data verbs into an SQL query and execute that query. The reason this method is so powerful is that the bulk of the data still lives in the original database (i.e., it is not on your computer). With each query, you can access a small and customized amount of data tailored to your specific analysis needs.",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/17-SQL.html#footnotes",
    "href": "src/17-SQL.html#footnotes",
    "title": "SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA word of caution: It can be difficult to discover and understand information in a data warehouse that is relevant to a project. Data is often produced across different arms of a large organization, and documentation describing the data can be scattered, missing, or out-of-date. The best way to decode information is by talking to the people who produced it!↩︎\nAn alternative is to use an R package like DBI to generate the SQL calls directly in R (similar to what we did with API wrapper packages); however, this often results in more trouble than it is worth.↩︎\nThe StackExchange data explorer uses a variant of SQL called Transact SQL that is supported by Microsoft databases. TOP(100) is a non-standard SQL feature supported by T-SQL. For most databases you would accomplish the same goal by adding LIMIT 100 to the end of the query.↩︎\nNote that whereas we used the double equals == for comparison in R, the SQL WHERE command takes just a single =.↩︎\nTo count the number of posts, run {SQL} SELECT COUNT(Id) FROM Posts Where PostTypeId = 1.↩︎",
    "crumbs": [
      "Src",
      "SQL"
    ]
  },
  {
    "objectID": "src/19-Adv_Viz.html",
    "href": "src/19-Adv_Viz.html",
    "title": "Advanced Visualization",
    "section": "",
    "text": "Read in the Capital Bikeshare data from the last quarter of 2014:\n\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\n\nWarning in file(file, \"rt\"): cannot open file 'data/DC-Stations.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\ndata_site &lt;-\n  \"https://bcheggeseth.github.io/112_fall_2022/data/2014-Q4-Trips-History-Data-Small.rds\"\nTrips &lt;- readRDS(gzcon(url(data_site)))\nStations &lt;- read_csv(\"https://bcheggeseth.github.io/112_fall_2022/data/DC-Stations.csv\")\n\nOne way to plot networks is to just use the geom_segment function in ggplot. Here is an example where we compute the bike ride flows between each pair of stations, keeping the data faceted by client and is_weekend, and filtering out low traffic links:\n\nTrafficFlow &lt;- Trips %&gt;%\n  mutate(is_weekend = ifelse(lubridate::wday(sdate) %in% c(1, 7), \"weekend\", \"weekday\")) %&gt;%\n  group_by(sstation, estation, client, is_weekend) %&gt;%\n  summarise(flow = n()) %&gt;%\n  left_join(Stations %&gt;% select(name, lat, long), by = c(\"sstation\" = \"name\")) %&gt;%\n  rename(slat = lat) %&gt;%\n  rename(slong = long) %&gt;%\n  left_join(Stations %&gt;% select(name, lat, long), by = c(\"estation\" = \"name\")) %&gt;%\n  rename(elat = lat) %&gt;%\n  rename(elong = long) %&gt;%\n  filter(!is.na(slat) & !is.na(slong) & !is.na(elat) & !is.na(elong))\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\n\n\nmyMap &lt;- get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\") # centered at Logan Circle\n\nError in get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\"): Stamen map tiles are now hosted by Stadia Maps, use `get_stadiamap()`.\n\n# myMap&lt;-get_map(location=\"Logan Circle\",source=\"google\",maptype=\"roadmap\",zoom=13)\n\nPlot data on the whole network:\n\nthresh &lt;- .04\nmax_flow &lt;- max(TrafficFlow$flow)\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\nTrafficFlow &lt;- TrafficFlow %&gt;%\n  mutate(weight = flow / max_flow) %&gt;%\n  filter(weight &gt; thresh)\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\nggmap(myMap) +\n  geom_point(data = Stations, size = 2, color = \"red\", aes(x = long, y = lat)) +\n  geom_segment(data = TrafficFlow, aes(x = slong, xend = elong, y = slat, yend = elat, alpha = weight / 2), arrow = arrow(length = unit(0.03, \"npc\")), color = \"red\") +\n  facet_grid(client ~ is_weekend)\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\n\n\n\n\nThe gganimate package animates a series of plots. Here are some resources:\n\ngganimate intro slides by Katherine Good\ngganimate cheat sheet\ngganimate by Thomas Pedersen\nPedersen introductory vignette\ngganimate wiki page\nropensci examples\n\nLet’s do one example here. First we create a static plot of a single bike moving around town.\nIdentify a busy bike:\n\nbusyBikes &lt;- Trips %&gt;%\n  group_by(bikeno) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(3)\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nGather and tidy all data for that bike:\n\nsingleBike &lt;- Trips %&gt;%\n  filter(bikeno == busyBikes$bikeno[1]) %&gt;%\n  arrange(sdate) %&gt;%\n  select(sdate, sstation, edate, estation)\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nsingleTidy &lt;- bind_rows(\n  singleBike %&gt;%\n    select(date = sdate, station = sstation) %&gt;%\n    mutate(key = \"start\"),\n  singleBike %&gt;%\n    select(date = edate, station = estation) %&gt;%\n    mutate(key = \"end\")\n) %&gt;%\n  arrange(date) %&gt;%\n  left_join(Stations, by = c(\"station\" = \"name\"))\n\nError in eval(expr, envir, enclos): object 'singleBike' not found\n\n\nPlot the movements of the bike over the first week:\n\nstops &lt;- singleTidy %&gt;%\n  select(station, lat, long, date) %&gt;%\n  head(102) %&gt;%\n  mutate(elapsed_hours = as.numeric(difftime(date, date[1], units = \"hours\"))) %&gt;%\n  mutate(order = 1:102)\n\nError in eval(expr, envir, enclos): object 'singleTidy' not found\n\nggmap(myMap) +\n  geom_path(data = stops, aes(x = long, y = lat, color = elapsed_hours), size = 1.3) +\n  scale_color_distiller(palette = \"Reds\") +\n  labs(color = \"Elapsed Hours\")\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\n\nNow let’s animate the plot with gganimate:\n\nlibrary(gganimate)\nlibrary(av)\n\npp_anim &lt;- ggmap(myMap) +\n  geom_path(data = stops, aes(x = long, y = lat, color = elapsed_hours), size = 1.3) +\n  scale_color_distiller(palette = \"Reds\") +\n  labs(color = \"Elapsed Hours\", title = \"Date and Time: {frame_along}\") +\n  transition_reveal(date)\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\nanimate(pp_anim, fps = 1, start_pause = 2, end_pause = 15, renderer = av_renderer())\n\nError in eval(expr, envir, enclos): object 'pp_anim' not found\n\n\nThe animations above do not allow for interactivity. We’ll explore different methods to include interactivity in the following sections.\n\n\n\nAdditional reading:\n\nInteractivity in R for Data Science by Grolemund and Wickham.\n\nhttp://www.htmlwidgets.org/\n\n\n\nDifferent htmlwidgets allow you to take advantage of the interactivity of html when generating graphics. Different types of widgets have been designed for different types of visualizations. In general, I found all of these easy to learn and use (i.e., I could get them up and running on an example I had in mind in under an hour).\n\n\nThe leaflet htmlwidget allows you to easily create interactive maps. Just like ggplot, you add different layers to the visualiation (a “Tiles”” layer for a background map, different types of “Markers”, points lines, etc.). I found it super easy to learn and use. Here is an example:\n\nlibrary(leaflet)\npal &lt;- colorNumeric(\n  palette = \"Greys\",\n  domain = stops$order, reverse = TRUE\n)\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\nleaflet(stops) %&gt;%\n  setView(-77.0296, 38.9096, zoom = 13) %&gt;% # Logan Circle coords\n  addProviderTiles(\"OpenStreetMap.Mapnik\") %&gt;% # this fixes a bug in addTiles() %&gt;%\n  addCircleMarkers(\n    lat = ~lat, lng = ~long, color = ~ pal(order),\n    popup = ~ paste(as.character(order), \": \", station, sep = \"\")\n  ) %&gt;%\n  addPolylines(lat = ~lat, lng = ~long)\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\n\n\n\nThe dygraph pacakge allows us to generate interactive time series charts.\nI am interested in how often the van needs to come by and pick up or drop off bicycles at different stations. So I want to look at the net daily departures at each station; that is, the number of departures minus the number of arrivals.\n\nnum_daily_departures &lt;- Trips %&gt;%\n  mutate(month = lubridate::month(sdate)) %&gt;%\n  mutate(day = lubridate::day(sdate)) %&gt;%\n  group_by(month, day, sstation) %&gt;%\n  summarise(num_departures = n())\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nnum_daily_arrivals &lt;- Trips %&gt;%\n  mutate(month = lubridate::month(edate)) %&gt;%\n  mutate(day = lubridate::day(edate)) %&gt;%\n  group_by(month, day, estation) %&gt;%\n  filter(month &gt; 9) %&gt;%\n  summarise(num_arrivals = n())\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nNetTraffic &lt;- num_daily_departures %&gt;%\n  full_join(num_daily_arrivals, by = c(\"sstation\" = \"estation\", \"month\" = \"month\", \"day\" = \"day\"))\n\nError in eval(expr, envir, enclos): object 'num_daily_departures' not found\n\nNetTraffic[is.na(NetTraffic)] &lt;- 0\n\nError: object 'NetTraffic' not found\n\nNetTraffic &lt;- NetTraffic %&gt;%\n  mutate(total_events = num_departures + num_arrivals) %&gt;%\n  mutate(net_departures = num_departures - num_arrivals) %&gt;%\n  rename(station = sstation) %&gt;%\n  group_by(station) %&gt;%\n  mutate(tot = sum(total_events)) %&gt;%\n  filter(tot &gt; 6000) %&gt;%\n  ungroup() %&gt;%\n  mutate(date = ymd(paste(\"2014\", as.character(month), as.character(day), sep = \"\"))) %&gt;%\n  mutate(wday = wday(date, label = TRUE))\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\nLet’s plot the net daily departures for four different stations.\n\nNetTrafficSelect &lt;- NetTraffic %&gt;%\n  filter(station %in% c(\"Massachusetts Ave & Dupont Circle NW\", \"16th & Harvard St NW\", \"Lincoln Memorial\", \"Columbus Circle / Union Station\")) %&gt;%\n  select(date, station, net_departures)\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\nNote that dygraphs wants each time series in a separate column, as opposed to the tidy format in which you would want it for ggplot. It also wants it in the xts format. We can fix this with a spread command:\n\nlibrary(xts)\nNetTrafficSelectWide &lt;- NetTrafficSelect %&gt;%\n  spread(key = station, value = net_departures)\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\nNetTrafficSelectXTS &lt;- xts(NetTrafficSelectWide[, 2:5], order.by = NetTrafficSelectWide$date)\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectWide' not found\n\n\nAnd now we are ready to create the visualization. Note how you can hover over points to see the values or use the range selector to adjust the domain on the x-axis.\n\nlibrary(dygraphs)\ndygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four Select Stations\") %&gt;%\n  dyRangeSelector() %&gt;%\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize = 5,\n    strokeWidth = 3,\n    colors = RColorBrewer::brewer.pal(4, \"Set2\")\n  ) %&gt;%\n  dyLegend(width = 1200)\n\nWarning in dygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four\nSelect Stations\"): restarting interrupted promise evaluation\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectXTS' not found\n\n\n\n\n\nThe plotly package is a super convenient way to incorporate many of the cool features of d3 into your graphics without having to learn anything about d3 programming. This might be my favorite widget so far, because all you have to do is make your regular graphic with ggplot and then pass it to the function ggplotly.\n\nlibrary(plotly)\np &lt;- ggplot(\n  NetTrafficSelect,\n  aes(x = date, y = net_departures, fill = station)\n) +\n  geom_col(position = \"dodge\")\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\nggplotly(p)\n\nError in eval(expr, envir, enclos): object 'p' not found\n\n\nNote all of the extra functionality we get:\n\nYou can turn individual time series on and off.\nYou can pan and zoom in and out on select areas.\nYou can hover on specific points to see either individual values, or (really cool) compare all values at that date.\n\n\n\n\nHere is a list of other cool htmlwidgets, along with demos: http://www.htmlwidgets.org/.\n\n\n\n\nWith the flexdashboard package, you can create dashboards with different configurations to display information visually. Each of these panels can include standard ggplot figures, htmlwidgets, text, tables, etc. The resulting dashboard is output as an html file that can be opened in a browswer.\nYou can check out the source code for each of these demo examples:\n\nhtmlwidgets showcase storyboard\nhighcharter dashboard\n\nThis page gives detailed instructions on using this package.\n\n\n\nAs opposed to htmlwidgets, which leverage JavaScript code to create the interactivity, Shiny Web Apps use R code to directly build the interactivity. This interactivity is built on the server side, so a Shiny App needs to be hosted on a server, as opposed to an htmlwidget, which can be embedded into the html page.1\nWhile this can be more complicated, it also opens the door to more possibilities. For example, if data is continuously being collected by the server, users can access up to date information. Shiny can also be used in conjunction with dashboards. Here are a couple examples:\n\nBus dashboard that is continuously updated\nCRAN downloads\nDiamond explorer\n\nThe programming paradigm is slightly different than we are used to, because it is reactive. Here is another article on understanding reactivity. It points out that when the user changes the input in a Shiny app (e.g., checking a box, moving a slider, filtering out certain variables), “Shiny is re-running your R expressions in a carefully scheduled way.”",
    "crumbs": [
      "Src",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "src/19-Adv_Viz.html#network-visualization",
    "href": "src/19-Adv_Viz.html#network-visualization",
    "title": "Advanced Visualization",
    "section": "",
    "text": "Read in the Capital Bikeshare data from the last quarter of 2014:\n\n\nWarning in gzfile(file, \"rb\"): cannot open compressed file\n'data/2014-Q4-Trips-History-Data.rds', probable reason 'No such file or\ndirectory'\n\n\nError in gzfile(file, \"rb\"): cannot open the connection\n\n\nWarning in file(file, \"rt\"): cannot open file 'data/DC-Stations.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\ndata_site &lt;-\n  \"https://bcheggeseth.github.io/112_fall_2022/data/2014-Q4-Trips-History-Data-Small.rds\"\nTrips &lt;- readRDS(gzcon(url(data_site)))\nStations &lt;- read_csv(\"https://bcheggeseth.github.io/112_fall_2022/data/DC-Stations.csv\")\n\nOne way to plot networks is to just use the geom_segment function in ggplot. Here is an example where we compute the bike ride flows between each pair of stations, keeping the data faceted by client and is_weekend, and filtering out low traffic links:\n\nTrafficFlow &lt;- Trips %&gt;%\n  mutate(is_weekend = ifelse(lubridate::wday(sdate) %in% c(1, 7), \"weekend\", \"weekday\")) %&gt;%\n  group_by(sstation, estation, client, is_weekend) %&gt;%\n  summarise(flow = n()) %&gt;%\n  left_join(Stations %&gt;% select(name, lat, long), by = c(\"sstation\" = \"name\")) %&gt;%\n  rename(slat = lat) %&gt;%\n  rename(slong = long) %&gt;%\n  left_join(Stations %&gt;% select(name, lat, long), by = c(\"estation\" = \"name\")) %&gt;%\n  rename(elat = lat) %&gt;%\n  rename(elong = long) %&gt;%\n  filter(!is.na(slat) & !is.na(slong) & !is.na(elat) & !is.na(elong))\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\n\n\nmyMap &lt;- get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\") # centered at Logan Circle\n\nError in get_stamenmap(c(-77.1, 38.87, -76.975, 38.95), zoom = 14, maptype = \"terrain\"): Stamen map tiles are now hosted by Stadia Maps, use `get_stadiamap()`.\n\n# myMap&lt;-get_map(location=\"Logan Circle\",source=\"google\",maptype=\"roadmap\",zoom=13)\n\nPlot data on the whole network:\n\nthresh &lt;- .04\nmax_flow &lt;- max(TrafficFlow$flow)\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\nTrafficFlow &lt;- TrafficFlow %&gt;%\n  mutate(weight = flow / max_flow) %&gt;%\n  filter(weight &gt; thresh)\n\nError in eval(expr, envir, enclos): object 'TrafficFlow' not found\n\nggmap(myMap) +\n  geom_point(data = Stations, size = 2, color = \"red\", aes(x = long, y = lat)) +\n  geom_segment(data = TrafficFlow, aes(x = slong, xend = elong, y = slat, yend = elat, alpha = weight / 2), arrow = arrow(length = unit(0.03, \"npc\")), color = \"red\") +\n  facet_grid(client ~ is_weekend)\n\nError in eval(expr, envir, enclos): object 'myMap' not found",
    "crumbs": [
      "Src",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "src/19-Adv_Viz.html#animations-with-gganimate",
    "href": "src/19-Adv_Viz.html#animations-with-gganimate",
    "title": "Advanced Visualization",
    "section": "",
    "text": "The gganimate package animates a series of plots. Here are some resources:\n\ngganimate intro slides by Katherine Good\ngganimate cheat sheet\ngganimate by Thomas Pedersen\nPedersen introductory vignette\ngganimate wiki page\nropensci examples\n\nLet’s do one example here. First we create a static plot of a single bike moving around town.\nIdentify a busy bike:\n\nbusyBikes &lt;- Trips %&gt;%\n  group_by(bikeno) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(3)\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\n\nGather and tidy all data for that bike:\n\nsingleBike &lt;- Trips %&gt;%\n  filter(bikeno == busyBikes$bikeno[1]) %&gt;%\n  arrange(sdate) %&gt;%\n  select(sdate, sstation, edate, estation)\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nsingleTidy &lt;- bind_rows(\n  singleBike %&gt;%\n    select(date = sdate, station = sstation) %&gt;%\n    mutate(key = \"start\"),\n  singleBike %&gt;%\n    select(date = edate, station = estation) %&gt;%\n    mutate(key = \"end\")\n) %&gt;%\n  arrange(date) %&gt;%\n  left_join(Stations, by = c(\"station\" = \"name\"))\n\nError in eval(expr, envir, enclos): object 'singleBike' not found\n\n\nPlot the movements of the bike over the first week:\n\nstops &lt;- singleTidy %&gt;%\n  select(station, lat, long, date) %&gt;%\n  head(102) %&gt;%\n  mutate(elapsed_hours = as.numeric(difftime(date, date[1], units = \"hours\"))) %&gt;%\n  mutate(order = 1:102)\n\nError in eval(expr, envir, enclos): object 'singleTidy' not found\n\nggmap(myMap) +\n  geom_path(data = stops, aes(x = long, y = lat, color = elapsed_hours), size = 1.3) +\n  scale_color_distiller(palette = \"Reds\") +\n  labs(color = \"Elapsed Hours\")\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\n\nNow let’s animate the plot with gganimate:\n\nlibrary(gganimate)\nlibrary(av)\n\npp_anim &lt;- ggmap(myMap) +\n  geom_path(data = stops, aes(x = long, y = lat, color = elapsed_hours), size = 1.3) +\n  scale_color_distiller(palette = \"Reds\") +\n  labs(color = \"Elapsed Hours\", title = \"Date and Time: {frame_along}\") +\n  transition_reveal(date)\n\nError in eval(expr, envir, enclos): object 'myMap' not found\n\nanimate(pp_anim, fps = 1, start_pause = 2, end_pause = 15, renderer = av_renderer())\n\nError in eval(expr, envir, enclos): object 'pp_anim' not found\n\n\nThe animations above do not allow for interactivity. We’ll explore different methods to include interactivity in the following sections.",
    "crumbs": [
      "Src",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "src/19-Adv_Viz.html#interactive-visualizations",
    "href": "src/19-Adv_Viz.html#interactive-visualizations",
    "title": "Advanced Visualization",
    "section": "",
    "text": "Additional reading:\n\nInteractivity in R for Data Science by Grolemund and Wickham.\n\nhttp://www.htmlwidgets.org/\n\n\n\nDifferent htmlwidgets allow you to take advantage of the interactivity of html when generating graphics. Different types of widgets have been designed for different types of visualizations. In general, I found all of these easy to learn and use (i.e., I could get them up and running on an example I had in mind in under an hour).\n\n\nThe leaflet htmlwidget allows you to easily create interactive maps. Just like ggplot, you add different layers to the visualiation (a “Tiles”” layer for a background map, different types of “Markers”, points lines, etc.). I found it super easy to learn and use. Here is an example:\n\nlibrary(leaflet)\npal &lt;- colorNumeric(\n  palette = \"Greys\",\n  domain = stops$order, reverse = TRUE\n)\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\nleaflet(stops) %&gt;%\n  setView(-77.0296, 38.9096, zoom = 13) %&gt;% # Logan Circle coords\n  addProviderTiles(\"OpenStreetMap.Mapnik\") %&gt;% # this fixes a bug in addTiles() %&gt;%\n  addCircleMarkers(\n    lat = ~lat, lng = ~long, color = ~ pal(order),\n    popup = ~ paste(as.character(order), \": \", station, sep = \"\")\n  ) %&gt;%\n  addPolylines(lat = ~lat, lng = ~long)\n\nError in eval(expr, envir, enclos): object 'stops' not found\n\n\n\n\n\nThe dygraph pacakge allows us to generate interactive time series charts.\nI am interested in how often the van needs to come by and pick up or drop off bicycles at different stations. So I want to look at the net daily departures at each station; that is, the number of departures minus the number of arrivals.\n\nnum_daily_departures &lt;- Trips %&gt;%\n  mutate(month = lubridate::month(sdate)) %&gt;%\n  mutate(day = lubridate::day(sdate)) %&gt;%\n  group_by(month, day, sstation) %&gt;%\n  summarise(num_departures = n())\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nnum_daily_arrivals &lt;- Trips %&gt;%\n  mutate(month = lubridate::month(edate)) %&gt;%\n  mutate(day = lubridate::day(edate)) %&gt;%\n  group_by(month, day, estation) %&gt;%\n  filter(month &gt; 9) %&gt;%\n  summarise(num_arrivals = n())\n\nError in eval(expr, envir, enclos): object 'Trips' not found\n\nNetTraffic &lt;- num_daily_departures %&gt;%\n  full_join(num_daily_arrivals, by = c(\"sstation\" = \"estation\", \"month\" = \"month\", \"day\" = \"day\"))\n\nError in eval(expr, envir, enclos): object 'num_daily_departures' not found\n\nNetTraffic[is.na(NetTraffic)] &lt;- 0\n\nError: object 'NetTraffic' not found\n\nNetTraffic &lt;- NetTraffic %&gt;%\n  mutate(total_events = num_departures + num_arrivals) %&gt;%\n  mutate(net_departures = num_departures - num_arrivals) %&gt;%\n  rename(station = sstation) %&gt;%\n  group_by(station) %&gt;%\n  mutate(tot = sum(total_events)) %&gt;%\n  filter(tot &gt; 6000) %&gt;%\n  ungroup() %&gt;%\n  mutate(date = ymd(paste(\"2014\", as.character(month), as.character(day), sep = \"\"))) %&gt;%\n  mutate(wday = wday(date, label = TRUE))\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\n\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\nLet’s plot the net daily departures for four different stations.\n\nNetTrafficSelect &lt;- NetTraffic %&gt;%\n  filter(station %in% c(\"Massachusetts Ave & Dupont Circle NW\", \"16th & Harvard St NW\", \"Lincoln Memorial\", \"Columbus Circle / Union Station\")) %&gt;%\n  select(date, station, net_departures)\n\nError in eval(expr, envir, enclos): object 'NetTraffic' not found\n\n\nNote that dygraphs wants each time series in a separate column, as opposed to the tidy format in which you would want it for ggplot. It also wants it in the xts format. We can fix this with a spread command:\n\nlibrary(xts)\nNetTrafficSelectWide &lt;- NetTrafficSelect %&gt;%\n  spread(key = station, value = net_departures)\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\nNetTrafficSelectXTS &lt;- xts(NetTrafficSelectWide[, 2:5], order.by = NetTrafficSelectWide$date)\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectWide' not found\n\n\nAnd now we are ready to create the visualization. Note how you can hover over points to see the values or use the range selector to adjust the domain on the x-axis.\n\nlibrary(dygraphs)\ndygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four Select Stations\") %&gt;%\n  dyRangeSelector() %&gt;%\n  dyOptions(\n    drawPoints = TRUE,\n    pointSize = 5,\n    strokeWidth = 3,\n    colors = RColorBrewer::brewer.pal(4, \"Set2\")\n  ) %&gt;%\n  dyLegend(width = 1200)\n\nWarning in dygraph(NetTrafficSelectXTS, main = \"Daily Net Departures at Four\nSelect Stations\"): restarting interrupted promise evaluation\n\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelectXTS' not found\n\n\n\n\n\nThe plotly package is a super convenient way to incorporate many of the cool features of d3 into your graphics without having to learn anything about d3 programming. This might be my favorite widget so far, because all you have to do is make your regular graphic with ggplot and then pass it to the function ggplotly.\n\nlibrary(plotly)\np &lt;- ggplot(\n  NetTrafficSelect,\n  aes(x = date, y = net_departures, fill = station)\n) +\n  geom_col(position = \"dodge\")\n\nError in eval(expr, envir, enclos): object 'NetTrafficSelect' not found\n\nggplotly(p)\n\nError in eval(expr, envir, enclos): object 'p' not found\n\n\nNote all of the extra functionality we get:\n\nYou can turn individual time series on and off.\nYou can pan and zoom in and out on select areas.\nYou can hover on specific points to see either individual values, or (really cool) compare all values at that date.\n\n\n\n\nHere is a list of other cool htmlwidgets, along with demos: http://www.htmlwidgets.org/.\n\n\n\n\nWith the flexdashboard package, you can create dashboards with different configurations to display information visually. Each of these panels can include standard ggplot figures, htmlwidgets, text, tables, etc. The resulting dashboard is output as an html file that can be opened in a browswer.\nYou can check out the source code for each of these demo examples:\n\nhtmlwidgets showcase storyboard\nhighcharter dashboard\n\nThis page gives detailed instructions on using this package.\n\n\n\nAs opposed to htmlwidgets, which leverage JavaScript code to create the interactivity, Shiny Web Apps use R code to directly build the interactivity. This interactivity is built on the server side, so a Shiny App needs to be hosted on a server, as opposed to an htmlwidget, which can be embedded into the html page.1\nWhile this can be more complicated, it also opens the door to more possibilities. For example, if data is continuously being collected by the server, users can access up to date information. Shiny can also be used in conjunction with dashboards. Here are a couple examples:\n\nBus dashboard that is continuously updated\nCRAN downloads\nDiamond explorer\n\nThe programming paradigm is slightly different than we are used to, because it is reactive. Here is another article on understanding reactivity. It points out that when the user changes the input in a Shiny app (e.g., checking a box, moving a slider, filtering out certain variables), “Shiny is re-running your R expressions in a carefully scheduled way.”",
    "crumbs": [
      "Src",
      "Advanced Visualization"
    ]
  },
  {
    "objectID": "src/19-Adv_Viz.html#footnotes",
    "href": "src/19-Adv_Viz.html#footnotes",
    "title": "Advanced Visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShiny still utilizes JavaScript libraries like d3 and Leaflet.↩︎",
    "crumbs": [
      "Src",
      "Advanced Visualization"
    ]
  }
]