{
  "hash": "f2f5ba5a7a10200c7cf4f75f397b1e87",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n# Text Analysis  {-}\n\n## Learning Goals {-}\n\n- Understand the analysis process of decomposing text into tokens and considering word/token frequency\n- Develop comfort in comparing the text across multiple documents in terms of tf-idf and log odds ratio\n- Develop comfort in using lexicons to perform sentiment analysis on a document of text\n\n\n## Introduction to Text Analysis in R {-}\n\nWe have seen how to manipulate strings with regular expressions. Here, we examine how to analyze longer text documents. Text refers to information composed primarily of words: song lyrics, Tweets, news articles, novels, Wikipedia articles, online forums, and countless other resources.\n\nIn `R` and most other programming languages, text is stored in strings of characters.\n\nThere are a variety of common ways to get strings containing the text we want to analyze.\n\n### Text Acquisition {-}\n\n**String Literals**\n\nIt may be natural to start by declaring an `R` variable that holds a string. Let's consider the [U.S. Declaration of Independence](https://en.wikipedia.org/wiki/United_States_Declaration_of_Independence).\nHere's an `R` variable that contains one of the most memorable sentences in the Declaration of Independence:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_dec_sentence <- \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n# Show the number of characters in the sentence.\nnchar(us_dec_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 209\n```\n\n\n:::\n\n```{.r .cell-code}\n# Show the sentence itself.\nus_dec_sentence\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n```\n\n\n:::\n:::\n\n\nUnfortunately, creating literal string variables like this can become unwieldy for larger texts, or collections of multiple texts.\n\nUsing this technique, your `R` program would be narrowly written to analyze [*hard-coded*](https://en.wikipedia.org/wiki/Hard_coding) string variables, and defining those string variables may take up the vast majority of our program's source code, making it difficult to read.\n\nWe will discuss two more flexible ways of getting textual data: reading a `.TXT` file and accessing a web API.\n\n#### Reading `.txt` Files {-}\n\nThe simplest file format for text is a `.TXT` (or `.txt`) file. A `.txt` file contains raw textual data. You can find `.txt` files by using Google's `filetype:` search filter.\nGo to http://google.com and type `filetype:txt declaration of independence` in the search box.\n\nIn the results you should see many `.txt` files containing the U.S. Declaration of Independence.\n\nFor example, https://infamous.net/documents/declaration-of-independence.txt. We can read this `.txt` file into `R` as a string using the `readr` package.^[Instead of reading the file directly from the internet, it is often a good idea to first save it to your working directory through your browser, and then read it locally. The benefits of this include having the data backed-up in case the website changes, and being able to run your code if you are offline. The drawback is that if the website is updated and you actually want those changes to be reflected in your text analysis, you'll have to download new versions periodically.] \n\nBecause the text is so large, we use the `strtrim` function to only show the first 500 characters of the text.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nus_dec <- read_file(\"https://infamous.net/documents/declaration-of-independence.txt\")\nnchar(us_dec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9841\n```\n\n\n:::\n\n```{.r .cell-code}\nstrtrim(us_dec, 500)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Congress, July 4, 1776,\\nTHE UNANIMOUS DECLARATION OF THE THIRTEEN UNITED STATES OF AMERICA\\n\\nWhen in the Course of human events, it becomes necessary for one \\npeople to dissolve the political bands which have connected them \\nwith another, and to assume among the Powers of the earth, the \\nseparate and equal station to which the Laws of Nature and of \\nNature's God entitle them, a decent respect to the opinions of \\nmankind requires that they should declare the causes which\"\n```\n\n\n:::\n:::\n\nNotice all those `\\n` sequences that appear in the string. \n\nThese are *newline* characters that denote the end of a line.\n\nThere are a few other [special characters](https://en.wikipedia.org/wiki/Escape_character) that you may see. For example, `'\\t'` is a tab.\n\n#### Using Web APIs {-}\n\n\nWhen we want to analyze textual data created on websites or mobile apps such as Facebook and Twitter, we can use web APIs to gather the text. Here is one example from the largest single collection of written knowledge in human history: Wikipedia! \n\nThe function below retrieves the text content of a Wikipedia article with a particular title. It uses Wikipedia's *Public API* to do so, which enables any computer program to interact with Wikipedia. Wikipedia's API is convenient for us because it is vast, open, and free. Don't worry if you don't follow the details of the code below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGetArticleText <- function(langCode, titles) {\n  # Returns the text of the specified article in the specified language\n\n  # An accumulator variable that will hold the text of each article\n\n  # Create\n  texts <- sapply(titles, function(t) {\n    print(t)\n    article_info <- getForm(\n      paste(\"https://\", langCode, \".wikipedia.org/w/api.php\", sep = \"\"),\n      action  = \"query\",\n      prop = \"extracts\",\n      format  = \"json\",\n      explaintext = \"\",\n      titles  = t\n    )\n\n    js <- fromJSON(article_info)\n    return(js$query$pages[[1]]$extract)\n  })\n  return(texts)\n}\n\n# Get the text for https://en.wikipedia.org/wiki/Macalester_College,\n# https://en.wikipedia.org/wiki/Carleton_College, and https://en.wikipedia.org/wiki/University_of_Minnesota in English (\"en\").\n# We could also get the text for the Spanish article (\"es\"), or German article (\"de\")\n\nschool_wiki_titles <- c(\"Macalester College\", \"Carleton College\", \"University of Minnesota\")\nschool_wiki_text <- GetArticleText(\"en\", school_wiki_titles)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Macalester College\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in function (type, msg, asError = TRUE) : schannel: CertGetCertificateChain trust error CERT_TRUST_IS_NOT_TIME_VALID\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print out the first 500 characters of the text\nstrtrim(school_wiki_text, 500)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'school_wiki_text' not found\n```\n\n\n:::\n:::\n\n\nWe'll analyze these documents further below.\n\n### Analyzing Single Documents {-}\n\nIf we tried to make a data frame directly out of the text, it would look odd. It contains the text as a single row in a column named \"text.\" This doesn't seem any more useful than the original string itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_dec_df <- tibble(title = \"Declaration of Independence\", text = us_dec)\nus_dec_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  title                       text                                              \n  <chr>                       <chr>                                             \n1 Declaration of Independence \"\\n\\nTHE DECLARATION OF INDEPENDENCE:\\n\\n\\nIn Con…\n```\n\n\n:::\n:::\n\n\n#### Unnesting Tokens {-}\n\nWe need to restructure the text into components that can be easily analyzed.\n\nWe will use two units of data:\n\n- A **token** is the smallest textual information unit we wish to measure, typically a word.\n- A **document** is a collection of tokens. \n\nFor our example here, the Declaration of Independence is the document, and a word is the token. However, a document could be a tweet, a novel chapter, a Wikipedia article, or anything else that seems interesting. Other possibilities for tokens include sentences, lines, paragraphs, characters, [ngrams](https://en.wikipedia.org/wiki/N-gram), and more.^[See the help for `unnest_tokens` to learn more about options for the token.]\n\nLater on, we will also give an example of how to perform textual analyses comparing two or more documents.\n\nWe will be using the tidy text format, which has one row for each unit of analysis. Our work will focus on word-level analysis within each document, so each row will contain a document and word.\n\nTidyText's `unnest_tokens` function takes a data frame containing one row per document and breaks it into a data frame containing one row per token.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_us_dec <- us_dec_df %>%\n  unnest_tokens(word, text)\n\ntidy_us_dec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,539 × 2\n   title                       word        \n   <chr>                       <chr>       \n 1 Declaration of Independence the         \n 2 Declaration of Independence declaration \n 3 Declaration of Independence of          \n 4 Declaration of Independence independence\n 5 Declaration of Independence in          \n 6 Declaration of Independence congress    \n 7 Declaration of Independence july        \n 8 Declaration of Independence 4           \n 9 Declaration of Independence 1776        \n10 Declaration of Independence the         \n# ℹ 1,529 more rows\n```\n\n\n:::\n:::\n\n\nNote that because we only have one document, the initial data frame (`us_dec_df`) is just one row and the tidy text data frame (`tidy_us_dec`) has the same `title` for each row. \n\nLater on we will analyze more than one document and these columns can change.\n\nWe can now analyze this tidy text data frame. For example, we can determine the total number of words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(tidy_us_dec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1539\n```\n\n\n:::\n:::\n\n\nWe can also find the most frequently used words by using dplyr's `count` function, which creates a frequency table for (in our case) words: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create and display frequency count table\nall_us_dec_counts <- tidy_us_dec %>%\n  count(word, sort = TRUE)\nall_us_dec_counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 674 × 2\n   word      n\n   <chr> <int>\n 1 the      84\n 2 of       83\n 3 to       67\n 4 and      58\n 5 for      28\n 6 our      26\n 7 has      20\n 8 in       20\n 9 their    20\n10 he       19\n# ℹ 664 more rows\n```\n\n\n:::\n:::\n\nWe can count the rows in this data frame to determine how many different unique words appear in the document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(all_us_dec_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 674\n```\n\n\n:::\n:::\n\n\n#### Stop Words {-}\n\nNotice that the most frequent words are common words that are present in any document and not particularly descriptive of the topic of the document.\nThese common words are called **stop words**, and they are typically removed from textual analysis.\n\nTidyText provides a built in set of 1,149 different stop words.\nWe can load the dataset and use `anti_join` to remove rows associated with words in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load stop words dataset and display it\ndata(stop_words)\nstop_words\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,149 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create and display frequency count table after removing stop words from the dataset\nus_dec_counts <- tidy_us_dec %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\nus_dec_counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 525 × 2\n   word           n\n   <chr>      <int>\n 1 people        10\n 2 laws           9\n 3 government     6\n 4 john           6\n 5 thomas         6\n 6 william        6\n 7 free           5\n 8 george         5\n 9 powers         5\n10 time           5\n# ℹ 515 more rows\n```\n\n\n:::\n:::\n\n\n**Word Clouds**\n\nA [word cloud](https://georeferenced.wordpress.com/2013/01/15/rwordcloud/) is a visualization of the most frequent words in the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wordcloud)\n\n# Show a word cloud with some customized options\n\nwordcloud(us_dec_counts$word, # column of words\n  us_dec_counts$n, # column of frequencies\n  scale = c(5, 0.2), # range of font sizes of words\n  min.freq = 2, # minimum word frequency to show\n  max.words = 200, # show the 200 most frequent words\n  random.order = FALSE, # position the most popular words first\n  colors = brewer.pal(8, \"Dark2\") # color palette\n) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in wordcloud(us_dec_counts$word, us_dec_counts$n, scale = c(5, 0.2), :\ntelecomputing could not be fit on page. It will not be plotted.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](18-Text_Analysis_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\n### Comparing the text in two (or more) documents {-}\n\nLet's now create a TidyText data frame with the three Wikipedia documents we collected above via the API. Remember that the TidyText data frame has one row for each word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the three-row original data frame\ntext_df <- tibble(article = school_wiki_titles, text = school_wiki_text)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'school_wiki_text' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ntext_df\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'text_df' not found\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unnest the data frame so each row corresponds to a single word in a single document.\ntidy_df <- text_df %>%\n  unnest_tokens(word, text)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'text_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_df\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n:::\n\n\n#### Side-by-Side Word Clouds {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmacalester_counts <- tidy_df %>%\n  filter(article == \"Macalester College\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nmacalester_counts\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n```\n\n\n:::\n\n```{.r .cell-code}\numn_counts <- tidy_df %>%\n  filter(article == \"University of Minnesota\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\numn_counts\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ncarleton_counts <- tidy_df %>%\n  filter(article == \"Carleton College\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ncarleton_counts\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n```\n\n\n:::\n:::\n\n::: {.cell fig.fullwidth='true'}\n\n```{.r .cell-code}\nwordcloud(macalester_counts$word, macalester_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nwordcloud(umn_counts$word, umn_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'umn_counts' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nwordcloud(carleton_counts$word, carleton_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'carleton_counts' not found\n```\n\n\n:::\n:::\n\n\n**Brainstorm**\n\nHow do we compare multiple documents quantitatively?\n\n\n::: {.cell}\n\n```{.exercise .cell-code}\nBrainstorm a metric for comparing the relative frequency/importance of different words in two or more documents. What factors might you account for?\n\n```\n:::\n\n\n#### Term Frequency - Inverse Document Frequency {-}\n\nTo compare the prevalence of certain words in one document relative to another document, we could just count the occurrences. However, the documents may be different lengths, meaning that many more words might occur more often in the longer document. There are different ways to account for this, but one of the most common is [term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). \n\n- The *term frequency* aims to capture how frequently a word appears in each document. There are different ways to measure this, including a raw count, logarithmically scaled (1 + log of the raw count), or Boolean (either 1 or 0 depending on whether the word occurs). \n- The *inverse document frequency* aims to capture how common the word is across documents. It is \n$$\\log\\left(\\frac{N}{|\\{doc: word \\in doc\\}|}\\right),$$\nwhere $N$ is the number of documents, and the denominator of the fraction is the number of documents in which the selected word appears. Thus, if the word appears in all documents under consideration, the idf score is equal to log(1)=0. \n- The *td-idf score* is then the product of the term frequency and the inverse document frequency.\n\nWe'll use the `bind_tf_idf` command from the `tidytext` library. Its default measure for term frequency is the raw count of a given word divided by the total number of words in the document. Let's start by computing the thirty-five document-word pairs with the highest tf-idf scores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntfidf_analysis <- tidy_df %>%\n  count(article, word) %>%\n  bind_tf_idf(word, article, n) %>%\n  arrange(desc(tf_idf))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n```\n\n\n:::\n:::\n\n\nHere is a graphic with the same data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntfidf_analysis %>%\n  mutate(word = factor(word, levels = rev(unique(word)))) %>%\n  top_n(35) %>%\n  ggplot(aes(word, tf_idf, fill = article)) +\n  geom_col() +\n  labs(x = NULL, y = \"tf-idf\") +\n  coord_flip()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n```\n\n\n:::\n:::\n\n\nNext, let's say we want to determine which school is the most relevant to the query \"internationalism, multiculturalism, and service to society.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_words <- c(\"internationalism\", \"multiculturalism\", \"service\", \"society\")\nmission <- tfidf_analysis %>%\n  filter(word %in% target_words)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tfidf_analysis' not found\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'mission' not found\n```\n\n\n:::\n:::\n\n\n#### Log Odds Ratio {-}\n\nAnother metric for comparing the frequency of different words in two documents is the log odds ratio:\n\n$$\\log\\left(\\frac{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc1}}}{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc2}}} \\right),$$\nwhere $n$ is the number of times the word appears and $total$ is the total number of words in the document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal.mac <- nrow(filter(tidy_df, article == \"Macalester College\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ntotal.carleton <- nrow(filter(tidy_df, article == \"Carleton College\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'tidy_df' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nlogratios <- macalester_counts %>%\n  full_join(carleton_counts, by = \"word\", suffix = c(\".mac\", \".carleton\")) %>%\n  replace_na(list(n.mac = 0, n.carleton = 0)) %>%\n  mutate(n.total = n.mac + n.carleton) %>%\n  filter(n.total >= 5) %>%\n  mutate(logodds.mac = log(((n.mac + 1) / (total.mac + 1)) / ((n.carleton + 1) / (total.carleton + 1))))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'macalester_counts' not found\n```\n\n\n:::\n:::\n\n\nWhich words appear at roughly equal frequencies?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogratios %>%\n  arrange(abs(logodds.mac)) %>%\n  head(n = 20)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'logratios' not found\n```\n\n\n:::\n:::\n\n\nWhat are the most distinctive words?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogratios %>%\n  group_by(logodds.mac < 0) %>%\n  top_n(15, abs(logodds.mac)) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, logodds.mac)) %>%\n  ggplot(aes(word, logodds.mac, fill = logodds.mac < 0)) +\n  geom_col() +\n  coord_flip() +\n  ylab(\"log odds ratio (Mac/Carleton)\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Macalester\", \"Carleton\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'logratios' not found\n```\n\n\n:::\n:::\n\n\n### Sentiment Analysis {-}\n\nWe often want to understand whether text conveys certain characteristics. For example, is Macalester's mission statement more happy, sad, or angry than that of the University of Minnesota?\n\nA common way of doing this is by using a word dictionary that contains a list of words with the characteristics we are seeking (e.g., a list of words that are happy, sad, or angry). We can then measure how often words with each characteristic appear in the text. These word dictionaries are also called *lexicons*, and dictionaries related to emotive feelings are often called *sentiment lexicons*.\n\nTidy Text's `sentiments` dataset contains built-in sentiment lexicons. We can look at the structure of some of these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- get_sentiments(\"afinn\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: The textdata package is required to download the AFINN lexicon.\nInstall the textdata package to access this dataset.\n```\n\n\n:::\n\n```{.r .cell-code}\nnrc <- get_sentiments(\"nrc\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: The textdata package is required to download the NRC word-emotion association lexicon.\nInstall the textdata package to access this dataset.\n```\n\n\n:::\n\n```{.r .cell-code}\nbing <- get_sentiments(\"bing\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'afinn' not found\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'nrc' not found\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|word       |sentiment |\n|:----------|:---------|\n|2-faces    |negative  |\n|abnormal   |negative  |\n|abolish    |negative  |\n|abominable |negative  |\n|abominably |negative  |\n|abominate  |negative  |\n\n\n:::\n:::\n\n\nLet's take a look at the sentiments described within each lexicon:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show the number of words and unique sentiments in each lexicon\nafinn %>%\n  summarize(num_words = n(), values = paste(sort(unique(value)), collapse = \", \"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'afinn' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nnrc %>%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'nrc' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nbing %>%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  num_words sentiments        \n      <int> <chr>             \n1      6786 negative, positive\n```\n\n\n:::\n:::\n\n\n\nThe Tidy Text book has some great background on these data sets:\n\n> \n> The three general-purpose lexicons are\n> \n> * `AFINN` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),\n> * `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and\n> * `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).\n> \n> All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The `nrc` lexicon categorizes words in a binary fashion (\"yes\"/\"no\") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the `sentiments` dataset, and tidytext provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon.\n\nTo apply these dictionaries, we need to create a Tidy Text data frame with words for each row and join it to the dictionary scores. Let's give this a try using the [Macalester Statement of Purpose and Belief](https://www.macalester.edu/about/mission/). We start by creating the Tidy Text data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Declare a string containing the Macalester Statement of Purpose & Belief\nstatement <- \"At Macalester College we believe that education is a fundamentally transforming experience. As a community of learners, the possibilities for this personal, social, and intellectual transformation extend to us all. We affirm the importance of the intellectual growth of the students, staff and faculty through individual and collaborative endeavor. We believe that this can best be achieved through an environment that values the diverse cultures of our world and recognizes our responsibility to provide a supportive and respectful environment for students, staff and faculty of all cultures and backgrounds.\n\nWe expect students to develop a broad understanding of the liberal arts while they are at Macalester. Students should follow a primary course of study in order to acquire an understanding of disciplinary theory and methodology; they should be able to apply their understanding of theories to address problems in the larger community. Students should develop the ability to use information and communication resources effectively, be adept at critical, analytical and logical thinking, and express themselves well in both oral and written forms. Finally, students should be prepared to take responsibility for their personal, social and intellectual choices.\n\nWe believe that the benefit of the educational experience at Macalester is the development of individuals who make informed judgments and interpretations of the broader world around them and choose actions or beliefs for which they are willing to be held accountable. We expect them to develop the ability to seek and use knowledge and experience in contexts that challenge and inform their suppositions about the world. We are committed to helping students grow intellectually and personally within an environment that models and promotes academic excellence and ethical behavior. The education a student begins at Macalester provides the basis for continuous transformation through learning and service.\"\n\n# Expand this into a tidy data frame, with one row per word\ntidy_df <- tibble(college = c(\"Macalester College\"), text = statement) %>%\n  unnest_tokens(word, text)\n\n# Display the data frame and the most popular words\ntidy_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 293 × 2\n   college            word         \n   <chr>              <chr>        \n 1 Macalester College at           \n 2 Macalester College macalester   \n 3 Macalester College college      \n 4 Macalester College we           \n 5 Macalester College believe      \n 6 Macalester College that         \n 7 Macalester College education    \n 8 Macalester College is           \n 9 Macalester College a            \n10 Macalester College fundamentally\n# ℹ 283 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_df %>%\n  anti_join(stop_words) %>%\n  count(word)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 105 × 2\n   word            n\n   <chr>       <int>\n 1 ability         2\n 2 academic        1\n 3 accountable     1\n 4 achieved        1\n 5 acquire         1\n 6 actions         1\n 7 address         1\n 8 adept           1\n 9 affirm          1\n10 analytical      1\n# ℹ 95 more rows\n```\n\n\n:::\n:::\n\n\nNext, we join this data frame with the lexicon. Let's use nrc. Since we don't care about words not in the lexicon, we will use an inner join.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_df %>%\n  inner_join(nrc) %>%\n  count(sentiment)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'nrc' not found\n```\n\n\n:::\n:::\n\n\nThere are some odd sentiments for a mission statement (anger, disgust, fear, and negative). Let's take a look at what words are contributing to them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_df %>%\n  inner_join(nrc) %>%\n  filter(sentiment %in% c(\"anger\", \"disgust\", \"fear\", \"negative\")) %>%\n  select(word, sentiment)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'nrc' not found\n```\n\n\n:::\n:::\n\n\nAs you can see, word dictionaries are not perfect tools. When using them, make sure you look at the individual words contributing to the overall patterns to ensure they make sense.\n\n\n### Other Interesting Questions {-}\n\nThere are all sorts of other interesting questions we can ask when analyzing texts. These include:\n\n- How do word frequencies change over time (e.g., Twitter) or over the course of a text?\n- What is the correlation between different words (or names of characters in novels)? For example, how frequently do they appear in the same section of a text, or within $K$ number of words of each other?^[Check out the `widyr` package and its `pairwise_count()` function if interested in these and similar questions.]\n- How can we visualize such co-occurrences with networks?\n- What \"topics\" are found in different documents? What word collections comprise these topics? This area is called [topic modeling](http://tidytextmining.com/topicmodeling.html). \n- Can you guess who wrote a document by analyzing its text?\n- How does the structure of different languages (e.g., sentence structure, sentence length, parts-of-speech) compare? These and many other interesting questions are asked by [computational linguists](https://en.wikipedia.org/wiki/Computational_linguistics).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}