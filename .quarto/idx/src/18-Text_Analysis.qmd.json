{"title":"Text Analysis","markdown":{"headingText":"Text Analysis ","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"```{r 18_setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, error = TRUE, tidy = FALSE, message = FALSE, cache.extra = packageVersion(\"tufte\"))\nlibrary(tidyverse)\nlibrary(ggmap)\nlibrary(lubridate)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(jsonlite)\nlibrary(RCurl)\nlibrary(XML)\nlibrary(RJSONIO)\n```\n\n\n## Learning Goals {-}\n\n- Understand the analysis process of decomposing text into tokens and considering word/token frequency\n- Develop comfort in comparing the text across multiple documents in terms of tf-idf and log odds ratio\n- Develop comfort in using lexicons to perform sentiment analysis on a document of text\n\n\n## Introduction to Text Analysis in R {-}\n\nWe have seen how to manipulate strings with regular expressions. Here, we examine how to analyze longer text documents. Text refers to information composed primarily of words: song lyrics, Tweets, news articles, novels, Wikipedia articles, online forums, and countless other resources.\n\nIn `R` and most other programming languages, text is stored in strings of characters.\n\nThere are a variety of common ways to get strings containing the text we want to analyze.\n\n### Text Acquisition {-}\n\n**String Literals**\n\nIt may be natural to start by declaring an `R` variable that holds a string. Let's consider the [U.S. Declaration of Independence](https://en.wikipedia.org/wiki/United_States_Declaration_of_Independence).\nHere's an `R` variable that contains one of the most memorable sentences in the Declaration of Independence:\n\n```{r}\nus_dec_sentence <- \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\"\n\n# Show the number of characters in the sentence.\nnchar(us_dec_sentence)\n\n# Show the sentence itself.\nus_dec_sentence\n```\n\nUnfortunately, creating literal string variables like this can become unwieldy for larger texts, or collections of multiple texts.\n\nUsing this technique, your `R` program would be narrowly written to analyze [*hard-coded*](https://en.wikipedia.org/wiki/Hard_coding) string variables, and defining those string variables may take up the vast majority of our program's source code, making it difficult to read.\n\nWe will discuss two more flexible ways of getting textual data: reading a `.TXT` file and accessing a web API.\n\n#### Reading `.txt` Files {-}\n\nThe simplest file format for text is a `.TXT` (or `.txt`) file. A `.txt` file contains raw textual data. You can find `.txt` files by using Google's `filetype:` search filter.\nGo to http://google.com and type `filetype:txt declaration of independence` in the search box.\n\nIn the results you should see many `.txt` files containing the U.S. Declaration of Independence.\n\nFor example, https://infamous.net/documents/declaration-of-independence.txt. We can read this `.txt` file into `R` as a string using the `readr` package.^[Instead of reading the file directly from the internet, it is often a good idea to first save it to your working directory through your browser, and then read it locally. The benefits of this include having the data backed-up in case the website changes, and being able to run your code if you are offline. The drawback is that if the website is updated and you actually want those changes to be reflected in your text analysis, you'll have to download new versions periodically.] \n\nBecause the text is so large, we use the `strtrim` function to only show the first 500 characters of the text.\n\n```{r}\nlibrary(readr)\nus_dec <- read_file(\"https://infamous.net/documents/declaration-of-independence.txt\")\nnchar(us_dec)\nstrtrim(us_dec, 500)\n```\nNotice all those `\\n` sequences that appear in the string. \n\nThese are *newline* characters that denote the end of a line.\n\nThere are a few other [special characters](https://en.wikipedia.org/wiki/Escape_character) that you may see. For example, `'\\t'` is a tab.\n\n#### Using Web APIs {-}\n\n\nWhen we want to analyze textual data created on websites or mobile apps such as Facebook and Twitter, we can use web APIs to gather the text. Here is one example from the largest single collection of written knowledge in human history: Wikipedia! \n\nThe function below retrieves the text content of a Wikipedia article with a particular title. It uses Wikipedia's *Public API* to do so, which enables any computer program to interact with Wikipedia. Wikipedia's API is convenient for us because it is vast, open, and free. Don't worry if you don't follow the details of the code below.\n\n```{r}\nGetArticleText <- function(langCode, titles) {\n  # Returns the text of the specified article in the specified language\n\n  # An accumulator variable that will hold the text of each article\n\n  # Create\n  texts <- sapply(titles, function(t) {\n    print(t)\n    article_info <- getForm(\n      paste(\"https://\", langCode, \".wikipedia.org/w/api.php\", sep = \"\"),\n      action  = \"query\",\n      prop = \"extracts\",\n      format  = \"json\",\n      explaintext = \"\",\n      titles  = t\n    )\n\n    js <- fromJSON(article_info)\n    return(js$query$pages[[1]]$extract)\n  })\n  return(texts)\n}\n\n# Get the text for https://en.wikipedia.org/wiki/Macalester_College,\n# https://en.wikipedia.org/wiki/Carleton_College, and https://en.wikipedia.org/wiki/University_of_Minnesota in English (\"en\").\n# We could also get the text for the Spanish article (\"es\"), or German article (\"de\")\n\nschool_wiki_titles <- c(\"Macalester College\", \"Carleton College\", \"University of Minnesota\")\nschool_wiki_text <- GetArticleText(\"en\", school_wiki_titles)\n\n# Print out the first 500 characters of the text\nstrtrim(school_wiki_text, 500)\n```\n\nWe'll analyze these documents further below.\n\n### Analyzing Single Documents {-}\n\nIf we tried to make a data frame directly out of the text, it would look odd. It contains the text as a single row in a column named \"text.\" This doesn't seem any more useful than the original string itself.\n\n```{r}\nus_dec_df <- tibble(title = \"Declaration of Independence\", text = us_dec)\nus_dec_df\n```\n\n#### Unnesting Tokens {-}\n\nWe need to restructure the text into components that can be easily analyzed.\n\nWe will use two units of data:\n\n- A **token** is the smallest textual information unit we wish to measure, typically a word.\n- A **document** is a collection of tokens. \n\nFor our example here, the Declaration of Independence is the document, and a word is the token. However, a document could be a tweet, a novel chapter, a Wikipedia article, or anything else that seems interesting. Other possibilities for tokens include sentences, lines, paragraphs, characters, [ngrams](https://en.wikipedia.org/wiki/N-gram), and more.^[See the help for `unnest_tokens` to learn more about options for the token.]\n\nLater on, we will also give an example of how to perform textual analyses comparing two or more documents.\n\nWe will be using the tidy text format, which has one row for each unit of analysis. Our work will focus on word-level analysis within each document, so each row will contain a document and word.\n\nTidyText's `unnest_tokens` function takes a data frame containing one row per document and breaks it into a data frame containing one row per token.\n\n```{r}\n\ntidy_us_dec <- us_dec_df %>%\n  unnest_tokens(word, text)\n\ntidy_us_dec\n```\n\nNote that because we only have one document, the initial data frame (`us_dec_df`) is just one row and the tidy text data frame (`tidy_us_dec`) has the same `title` for each row. \n\nLater on we will analyze more than one document and these columns can change.\n\nWe can now analyze this tidy text data frame. For example, we can determine the total number of words.\n\n```{r}\nnrow(tidy_us_dec)\n```\n\nWe can also find the most frequently used words by using dplyr's `count` function, which creates a frequency table for (in our case) words: \n\n```{r}\n# Create and display frequency count table\nall_us_dec_counts <- tidy_us_dec %>%\n  count(word, sort = TRUE)\nall_us_dec_counts\n```\nWe can count the rows in this data frame to determine how many different unique words appear in the document.\n\n```{r}\nnrow(all_us_dec_counts)\n```\n\n#### Stop Words {-}\n\nNotice that the most frequent words are common words that are present in any document and not particularly descriptive of the topic of the document.\nThese common words are called **stop words**, and they are typically removed from textual analysis.\n\nTidyText provides a built in set of 1,149 different stop words.\nWe can load the dataset and use `anti_join` to remove rows associated with words in the dataset.\n\n```{r}\n# Load stop words dataset and display it\ndata(stop_words)\nstop_words\n\n# Create and display frequency count table after removing stop words from the dataset\nus_dec_counts <- tidy_us_dec %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\nus_dec_counts\n```\n\n**Word Clouds**\n\nA [word cloud](https://georeferenced.wordpress.com/2013/01/15/rwordcloud/) is a visualization of the most frequent words in the dataset:\n\n```{r, fig.width=6, fig.height=6}\nlibrary(wordcloud)\n\n# Show a word cloud with some customized options\n\nwordcloud(us_dec_counts$word, # column of words\n  us_dec_counts$n, # column of frequencies\n  scale = c(5, 0.2), # range of font sizes of words\n  min.freq = 2, # minimum word frequency to show\n  max.words = 200, # show the 200 most frequent words\n  random.order = FALSE, # position the most popular words first\n  colors = brewer.pal(8, \"Dark2\") # color palette\n) \n```\n\n### Comparing the text in two (or more) documents {-}\n\nLet's now create a TidyText data frame with the three Wikipedia documents we collected above via the API. Remember that the TidyText data frame has one row for each word.\n\n```{r}\n# Create the three-row original data frame\ntext_df <- tibble(article = school_wiki_titles, text = school_wiki_text)\ntext_df\n```\n\n```{r,warning=FALSE}\n# Unnest the data frame so each row corresponds to a single word in a single document.\ntidy_df <- text_df %>%\n  unnest_tokens(word, text)\ntidy_df\n```\n\n#### Side-by-Side Word Clouds {-}\n\n```{r}\nmacalester_counts <- tidy_df %>%\n  filter(article == \"Macalester College\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\nmacalester_counts\n\numn_counts <- tidy_df %>%\n  filter(article == \"University of Minnesota\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\numn_counts\n\ncarleton_counts <- tidy_df %>%\n  filter(article == \"Carleton College\") %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\ncarleton_counts\n```\n\n```{r,fig.show='hold',fig.fullwidth=TRUE,fig.width=4,warning=FALSE}\nwordcloud(macalester_counts$word, macalester_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\nwordcloud(umn_counts$word, umn_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\nwordcloud(carleton_counts$word, carleton_counts$n,\n  max.words = 200, random.order = FALSE, colors = brewer.pal(8, \"Dark2\")\n)\n```\n\n**Brainstorm**\n\nHow do we compare multiple documents quantitatively?\n\n```{exercise}\nBrainstorm a metric for comparing the relative frequency/importance of different words in two or more documents. What factors might you account for?\n\n```\n\n#### Term Frequency - Inverse Document Frequency {-}\n\nTo compare the prevalence of certain words in one document relative to another document, we could just count the occurrences. However, the documents may be different lengths, meaning that many more words might occur more often in the longer document. There are different ways to account for this, but one of the most common is [term frequency - inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). \n\n- The *term frequency* aims to capture how frequently a word appears in each document. There are different ways to measure this, including a raw count, logarithmically scaled (1 + log of the raw count), or Boolean (either 1 or 0 depending on whether the word occurs). \n- The *inverse document frequency* aims to capture how common the word is across documents. It is \n$$\\log\\left(\\frac{N}{|\\{doc: word \\in doc\\}|}\\right),$$\nwhere $N$ is the number of documents, and the denominator of the fraction is the number of documents in which the selected word appears. Thus, if the word appears in all documents under consideration, the idf score is equal to log(1)=0. \n- The *td-idf score* is then the product of the term frequency and the inverse document frequency.\n\nWe'll use the `bind_tf_idf` command from the `tidytext` library. Its default measure for term frequency is the raw count of a given word divided by the total number of words in the document. Let's start by computing the thirty-five document-word pairs with the highest tf-idf scores:\n\n```{r}\ntfidf_analysis <- tidy_df %>%\n  count(article, word) %>%\n  bind_tf_idf(word, article, n) %>%\n  arrange(desc(tf_idf))\n```\n\n```{r,echo=FALSE}\nknitr::kable(tfidf_analysis[1:35, ], caption = \"The thirty-five document-word pairs with the highest tf-idf scores.\")\n```\n\nHere is a graphic with the same data:\n\n```{r}\ntfidf_analysis %>%\n  mutate(word = factor(word, levels = rev(unique(word)))) %>%\n  top_n(35) %>%\n  ggplot(aes(word, tf_idf, fill = article)) +\n  geom_col() +\n  labs(x = NULL, y = \"tf-idf\") +\n  coord_flip()\n```  \n\nNext, let's say we want to determine which school is the most relevant to the query \"internationalism, multiculturalism, and service to society.\"\n\n```{r}\ntarget_words <- c(\"internationalism\", \"multiculturalism\", \"service\", \"society\")\nmission <- tfidf_analysis %>%\n  filter(word %in% target_words)\n```\n\n```{r,echo=FALSE}\nknitr::kable(mission)\n```\n\n#### Log Odds Ratio {-}\n\nAnother metric for comparing the frequency of different words in two documents is the log odds ratio:\n\n$$\\log\\left(\\frac{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc1}}}{\\left(\\frac{n+1}{total+1}\\right)_{\\hbox{doc2}}} \\right),$$\nwhere $n$ is the number of times the word appears and $total$ is the total number of words in the document.\n\n```{r}\ntotal.mac <- nrow(filter(tidy_df, article == \"Macalester College\"))\ntotal.carleton <- nrow(filter(tidy_df, article == \"Carleton College\"))\nlogratios <- macalester_counts %>%\n  full_join(carleton_counts, by = \"word\", suffix = c(\".mac\", \".carleton\")) %>%\n  replace_na(list(n.mac = 0, n.carleton = 0)) %>%\n  mutate(n.total = n.mac + n.carleton) %>%\n  filter(n.total >= 5) %>%\n  mutate(logodds.mac = log(((n.mac + 1) / (total.mac + 1)) / ((n.carleton + 1) / (total.carleton + 1))))\n```\n\nWhich words appear at roughly equal frequencies?\n\n```{r}\nlogratios %>%\n  arrange(abs(logodds.mac)) %>%\n  head(n = 20)\n```\n\nWhat are the most distinctive words?\n\n```{r,fig.width=10,fig.height=10}\nlogratios %>%\n  group_by(logodds.mac < 0) %>%\n  top_n(15, abs(logodds.mac)) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, logodds.mac)) %>%\n  ggplot(aes(word, logodds.mac, fill = logodds.mac < 0)) +\n  geom_col() +\n  coord_flip() +\n  ylab(\"log odds ratio (Mac/Carleton)\") +\n  scale_fill_discrete(name = \"\", labels = c(\"Macalester\", \"Carleton\"))\n```\n\n### Sentiment Analysis {-}\n\nWe often want to understand whether text conveys certain characteristics. For example, is Macalester's mission statement more happy, sad, or angry than that of the University of Minnesota?\n\nA common way of doing this is by using a word dictionary that contains a list of words with the characteristics we are seeking (e.g., a list of words that are happy, sad, or angry). We can then measure how often words with each characteristic appear in the text. These word dictionaries are also called *lexicons*, and dictionaries related to emotive feelings are often called *sentiment lexicons*.\n\nTidy Text's `sentiments` dataset contains built-in sentiment lexicons. We can look at the structure of some of these:\n\n```{r}\nafinn <- get_sentiments(\"afinn\")\nnrc <- get_sentiments(\"nrc\")\nbing <- get_sentiments(\"bing\")\n```\n\n```{r,echo=FALSE, fig.width=8}\nknitr::kable(afinn[1:6, ], align = \"ll\")\nknitr::kable(nrc[1:6, ], align = \"ll\")\nknitr::kable(bing[1:6, ], align = \"ll\")\n```\n\nLet's take a look at the sentiments described within each lexicon:\n\n```{r}\n# Show the number of words and unique sentiments in each lexicon\nafinn %>%\n  summarize(num_words = n(), values = paste(sort(unique(value)), collapse = \", \"))\n\nnrc %>%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n\nbing %>%\n  summarize(num_words = n(), sentiments = paste(sort(unique(sentiment)), collapse = \", \"))\n```\n\n\nThe Tidy Text book has some great background on these data sets:\n\n> \n> The three general-purpose lexicons are\n> \n> * `AFINN` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),\n> * `bing` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and\n> * `nrc` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).\n> \n> All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The `nrc` lexicon categorizes words in a binary fashion (\"yes\"/\"no\") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the `sentiments` dataset, and tidytext provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon.\n\nTo apply these dictionaries, we need to create a Tidy Text data frame with words for each row and join it to the dictionary scores. Let's give this a try using the [Macalester Statement of Purpose and Belief](https://www.macalester.edu/about/mission/). We start by creating the Tidy Text data frame:\n\n```{r}\n# Declare a string containing the Macalester Statement of Purpose & Belief\nstatement <- \"At Macalester College we believe that education is a fundamentally transforming experience. As a community of learners, the possibilities for this personal, social, and intellectual transformation extend to us all. We affirm the importance of the intellectual growth of the students, staff and faculty through individual and collaborative endeavor. We believe that this can best be achieved through an environment that values the diverse cultures of our world and recognizes our responsibility to provide a supportive and respectful environment for students, staff and faculty of all cultures and backgrounds.\n\nWe expect students to develop a broad understanding of the liberal arts while they are at Macalester. Students should follow a primary course of study in order to acquire an understanding of disciplinary theory and methodology; they should be able to apply their understanding of theories to address problems in the larger community. Students should develop the ability to use information and communication resources effectively, be adept at critical, analytical and logical thinking, and express themselves well in both oral and written forms. Finally, students should be prepared to take responsibility for their personal, social and intellectual choices.\n\nWe believe that the benefit of the educational experience at Macalester is the development of individuals who make informed judgments and interpretations of the broader world around them and choose actions or beliefs for which they are willing to be held accountable. We expect them to develop the ability to seek and use knowledge and experience in contexts that challenge and inform their suppositions about the world. We are committed to helping students grow intellectually and personally within an environment that models and promotes academic excellence and ethical behavior. The education a student begins at Macalester provides the basis for continuous transformation through learning and service.\"\n\n# Expand this into a tidy data frame, with one row per word\ntidy_df <- tibble(college = c(\"Macalester College\"), text = statement) %>%\n  unnest_tokens(word, text)\n\n# Display the data frame and the most popular words\ntidy_df\n\ntidy_df %>%\n  anti_join(stop_words) %>%\n  count(word)\n```\n\nNext, we join this data frame with the lexicon. Let's use nrc. Since we don't care about words not in the lexicon, we will use an inner join.\n\n```{r}\ntidy_df %>%\n  inner_join(nrc) %>%\n  count(sentiment)\n```\n\nThere are some odd sentiments for a mission statement (anger, disgust, fear, and negative). Let's take a look at what words are contributing to them.\n\n```{r}\ntidy_df %>%\n  inner_join(nrc) %>%\n  filter(sentiment %in% c(\"anger\", \"disgust\", \"fear\", \"negative\")) %>%\n  select(word, sentiment)\n```\n\nAs you can see, word dictionaries are not perfect tools. When using them, make sure you look at the individual words contributing to the overall patterns to ensure they make sense.\n\n\n### Other Interesting Questions {-}\n\nThere are all sorts of other interesting questions we can ask when analyzing texts. These include:\n\n- How do word frequencies change over time (e.g., Twitter) or over the course of a text?\n- What is the correlation between different words (or names of characters in novels)? For example, how frequently do they appear in the same section of a text, or within $K$ number of words of each other?^[Check out the `widyr` package and its `pairwise_count()` function if interested in these and similar questions.]\n- How can we visualize such co-occurrences with networks?\n- What \"topics\" are found in different documents? What word collections comprise these topics? This area is called [topic modeling](http://tidytextmining.com/topicmodeling.html). \n- Can you guess who wrote a document by analyzing its text?\n- How does the structure of different languages (e.g., sentence structure, sentence length, parts-of-speech) compare? These and many other interesting questions are asked by [computational linguists](https://en.wikipedia.org/wiki/Computational_linguistics).\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../sty/styles.css"],"toc":true,"output-file":"18-Text_Analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","code-copy":true,"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}